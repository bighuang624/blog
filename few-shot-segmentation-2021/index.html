<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/huang_full.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/huang_full.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/huang_full.ico">
  <link rel="mask-icon" href="/blog/huang_full.ico" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;kyonhuang.top&quot;,&quot;root&quot;:&quot;&#x2F;blog&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;blog&#x2F;images&quot;,&quot;scheme&quot;:&quot;Pisces&quot;,&quot;version&quot;:&quot;8.5.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:false,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;blog&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script><script src="/blog/js/config.js"></script>
<meta name="description" content="根据手头想法的需要，读一读 2021 年顶会顶刊的小样本分割相关论文并做笔记于此。有开源代码的论文优先，持续更新。  Prior Guided Feature Enrichment Network for Few-Shot Segmentation (TPAMI 2020) Few-Shot Segmentation Via Cycle-Consistent Transformer (NeurIP">
<meta property="og:type" content="article">
<meta property="og:title" content="【paper reading】2021 小样本分割论文选读">
<meta property="og:url" content="https://kyonhuang.top/blog/few-shot-segmentation-2021/">
<meta property="og:site_name" content="Kyon Huang 的博客">
<meta property="og:description" content="根据手头想法的需要，读一读 2021 年顶会顶刊的小样本分割相关论文并做笔记于此。有开源代码的论文优先，持续更新。  Prior Guided Feature Enrichment Network for Few-Shot Segmentation (TPAMI 2020) Few-Shot Segmentation Via Cycle-Consistent Transformer (NeurIP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure4.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure5.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-table1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-table3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-figure1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-figure3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table5.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table1.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table5.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-figure3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table23.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table4.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table6.PNG">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-figure1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-table1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-table2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure5.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-table1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-table2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure3.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure5.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure4.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-table5.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-table7.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-figure1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-figure2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-table.png">
<meta property="article:published_time" content="2021-12-14T08:39:35.000Z">
<meta property="article:modified_time" content="2022-01-22T10:15:35.244Z">
<meta property="article:author" content="Kyon Huang">
<meta property="article:tag" content="few-shot segmentation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure3.png">


<link rel="canonical" href="https://kyonhuang.top/blog/few-shot-segmentation-2021/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;kyonhuang.top&#x2F;blog&#x2F;few-shot-segmentation-2021&#x2F;&quot;,&quot;path&quot;:&quot;few-shot-segmentation-2021&#x2F;&quot;,&quot;title&quot;:&quot;【paper reading】2021 小样本分割论文选读&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>【paper reading】2021 小样本分割论文选读 | Kyon Huang 的博客</title>
  




  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
<link rel="alternate" href="/blog/atom.xml" title="Kyon Huang 的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Kyon Huang 的博客</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Github 连接不稳定，请科学上网以获取最佳访问效果</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-开源"><a href="/blog/opensource/" rel="section"><i class="fa fa-code fa-fw"></i>开源</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prior-Guided-Feature-Enrichment-Network-for-Few-Shot-Segmentation"><span class="nav-number">1.</span> <span class="nav-text">Prior Guided Feature Enrichment Network for Few-Shot Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-Shot-Segmentation-Via-Cycle-Consistent-Transformer"><span class="nav-number">2.</span> <span class="nav-text">Few-Shot Segmentation Via Cycle-Consistent Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer"><span class="nav-number">3.</span> <span class="nav-text">Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-Shot-Segmentation-Without-Meta-Learning-A-Good-Transductive-Inference-Is-All-You-Need"><span class="nav-number">4.</span> <span class="nav-text">Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-Guided-and-Cross-Guided-Learning-for-Few-Shot-Segmentation"><span class="nav-number">5.</span> <span class="nav-text">Self-Guided and Cross-Guided Learning for Few-Shot Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Prototype-Learning-and-Allocation-for-Few-Shot-Segmentation"><span class="nav-number">6.</span> <span class="nav-text">Adaptive Prototype Learning and Allocation for Few-Shot Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-Shot-3D-Point-Cloud-Semantic-Segmentation"><span class="nav-number">7.</span> <span class="nav-text">Few-Shot 3D Point Cloud Semantic Segmentation</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Kyon Huang"
      src="/blog/images/profile.png">
  <p class="site-author-name" itemprop="name">Kyon Huang</p>
  <div class="site-description" itemprop="description">CS 土博在读</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">64</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">130</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://kyonhuang.top/blog/atom.xml" title="RSS → https:&#x2F;&#x2F;kyonhuang.top&#x2F;blog&#x2F;atom.xml"><i class="fa fa-rss fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://kyonhuang.top/" title="学术主页 → https:&#x2F;&#x2F;kyonhuang.top&#x2F;"><i class="fa fa-user-graduate fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/bighuang624" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;bighuang624" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/huang-si-teng-67" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;huang-si-teng-67" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kyonhuang@qq.com" title="E-Mail → mailto:kyonhuang@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/bighuang624" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://kyonhuang.top/blog/few-shot-segmentation-2021/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/profile.png">
      <meta itemprop="name" content="Kyon Huang">
      <meta itemprop="description" content="CS 土博在读">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Kyon Huang 的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【paper reading】2021 小样本分割论文选读
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-12-14 16:39:35" itemprop="dateCreated datePublished" datetime="2021-12-14T16:39:35+08:00">2021-12-14</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-01-22 18:15:35" itemprop="dateModified" datetime="2022-01-22T18:15:35+08:00">2022-01-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文阅读笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>根据手头想法的需要，读一读 2021 年顶会顶刊的小样本分割相关论文并做笔记于此。有开源代码的论文优先，持续更新。</p>
<ul>
<li>Prior Guided Feature Enrichment Network for Few-Shot Segmentation (TPAMI 2020)</li>
<li>Few-Shot Segmentation Via Cycle-Consistent Transformer (NeurIPS 2021)</li>
<li>Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer (ICCV 2021)</li>
<li>Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need? (CVPR 2021)</li>
<li>Self-Guided and Cross-Guided Learning for Few-Shot Segmentation (CVPR 2021)</li>
<li>Adaptive Prototype Learning and Allocation for Few-Shot Segmentation (CVPR 2021)</li>
<li>Few-Shot 3D Point Cloud Semantic Segmentation (CVPR 2021)</li>
</ul>
<!-- [参考](https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers) -->

<span id="more"></span>

<h3 id="Prior-Guided-Feature-Enrichment-Network-for-Few-Shot-Segmentation"><a href="#Prior-Guided-Feature-Enrichment-Network-for-Few-Shot-Segmentation" class="headerlink" title="Prior Guided Feature Enrichment Network for Few-Shot Segmentation"></a>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</h3><p>TPAMI 2020 | <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2008.01449">arxiv</a> | <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/PFENet">github</a></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure3.png"></p>
<p>这篇工作被很多 2021 年的相关论文作为 baseline 并且效果仍然有竞争力，因此先介绍一下。该论文提出了 <strong>Prior Guided Feature Enrichment Network (PFENet)</strong> 来解决两个问题：(1) 很多分割方法都依赖于 high-level 特征，然而 <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1903.02351">CANet, CVPR 2019</a> 的实验结果表明<strong>小样本模型仅仅使用 high-level 特征会导致表现下降</strong>；(2) 样本数量过少会导致 support 物体的尺寸和姿态都和 query target 有很大不同，在该文中被称为 <strong>spatial inconsistency</strong>，并且 ablation study 的实验结果表明仅仅集成 multi-scale 结构对于该问题只是次优解。因此，本文提出了以下两点贡献：</p>
<p><strong>prior generation</strong>：使用 <strong>high-level 特征</strong>（实践中是 conv5_x 的最后一层的输出）来生成 prior mask。在用 support mask 遮盖提取的 support 特征后，计算 support 和 query 特征的 pixel-wise cosine 相似度。对于 query 中的每个像素，取其和所有 support 像素的相似度中的最大值来得到 prior mask。之后 prior mask 经过 min-max normalization 来将所有值放缩到 [0, 1] 范围。如果 shot 数大于 1，将同一 query 特征和不同 support 特征得到的多张 prior masks 求平均来得到最终的 prior mask 输入到 FEM。<!-- 源码显示这里是先单独做 normalization 再总体求平均 --></p>
<!-- 按照 CyCTR 的意思，这里只用 support 的前景信息来计算这个 prior mask 就不合适了。正好 OSAD 任务没有完全的前景信息，如何在这一块上做一些改变，让它不像现在的样子？ -->

<!-- 和 OSADNet_v2 的最后一个模块有点像，不过那个是 query 和其他 query 算的一个 transductive 方法 -->

<!-- 另外在 5-shot 设置能不能不要单纯求均值？感觉会有信息损失 -->

<p><strong>Feature Enrichment Module (FEM)<strong>：这一块看下图比较好理解，就是用多个 average pooling 来将 query 特征、support prototype 和 prior mask 的拼接产物放缩到不同大小，然后做 multi-scale 的信息交互。注意 query 特征和 support 特征是通过</strong>拼接 middle-level 特征</strong>（实践中是 conv3_x、conv_4x 的最后一层的输出）得到，另外如果 shot 数大于 1，直接取所有处理后的 support 特征的均值作为新的 support 特征。inter-scale interaction 的每个 scale 的产物也会过一个 3x3 + 1x1 的卷积层组成的分类头得到损失 $\mathcal{L}^i_1$，然后所有 scale 产物拼接后经过 1x1 卷积也用分类头得到损失 $\mathcal{L}_{2}$。最终的总损失为 $\mathcal{L} = \frac{\sigma}{n}\sum^n_{i=1} \mathcal{L}^i_1 + \mathcal{L}_{2}$，$\sigma$ 在所有实验中设为 1.0。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure4.png"></p>
<p>一个稍微需要注意的点是图中带圈的 M 的结构如下图所示，其中 auxiliary 特征指 finer 特征，main 特征指 coarse 特征。对于没有 auxiliary 特征的 main 特征（如上图最上面的一个 scale），省略下图的第一步拼接操作。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure5.png"></p>
<p>实验结果：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-table1.png"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-table3.png"></p>
<h3 id="Few-Shot-Segmentation-Via-Cycle-Consistent-Transformer"><a href="#Few-Shot-Segmentation-Via-Cycle-Consistent-Transformer" class="headerlink" title="Few-Shot Segmentation Via Cycle-Consistent Transformer"></a>Few-Shot Segmentation Via Cycle-Consistent Transformer</h3><p>NeurIPS 2021 | <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2021/hash/b8b12f949378552c21f28deff8ba8eb6-Abstract.html">link</a> | <a target="_blank" rel="noopener" href="https://github.com/GengDavid/CyCTR">github（尚未包含代码）</a></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-figure1.png"></p>
<p>已有的小样本分割工作通常从 support 的图像特征提取 semantic-level prototypes，其中根据可以分为两类：(1) 如上图 (a) 所示的 class-wise mean pooling (<a target="_blank" rel="noopener" href="http://arxiv.org/abs/1908.06391">PANet, ICCV 2019</a>; <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1810.09091">SG-One, IEEE Trans. Cybern., 2020</a>; <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1903.02351">CANet, CVPR 2019</a>)，将属于不同类别的区域的特征算均值得到 prototypes；(2) 如上图 (b) 所示的 clustering，使用 EM 算法 (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.03898">RPMM, ECCV 2020</a>) 或者 K-means (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.06309">PPNet, ECCV 2020</a>) 来生成多个 prototypes 。然而，基于 prototypes 的方法会导致不同程度的 support information 的损失。因此，一些工作 (<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.html">PGNet, ICCV 2019</a>; <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580715.pdf">DAN, ECCV 2020</a>) 用 attention 机制来从 support 前景像素提取信息。然而，<strong>这些方法忽略了存在对分割有帮助的信息的 support 背景像素</strong>；同时，support 和 query 图像间的包含尺度、颜色、场景等差别使得只有一小部分 support 像素是对 query 图像的分割有帮助，而<strong>另一些 support 像素会和 query 的前景像素有很大差别，导致 attention 无法准确表示语义上的对齐</strong>。</p>
<p>本文提出了 <strong>Cycle-Consistent Transformer (CyCTR)</strong> 模块，包含两种 Transformer 部件：(1) <strong>self-alignment block</strong>：通过聚合相关的上下文信息来编码 query 的图像特征，Query、Key 和 Value 都来自同个 embedding；(2) <strong>cross-alignment block</strong>：将 support 图像的 pixel-wise 特征聚合到 query 图像的 pixel-wise 特征中，query 图像的特征是 Query，support 图像的特征作为 Key 和 Value。另外，一种新的 <strong>cycle-consistent attention</strong> 被部署在 cross-alignment block 上。从一个 support 像素的特征出发，我们找到它在 query 特征上的最近邻，然后在找到与这个最近邻最相似的 support 特征。如果起点和终点的 support 特征属于同一类别，就称作建立起了 <strong>cycle-consistency relationship</strong>。这个操作被集成到 attention 中来让 query 特征只去注意 cycle-consistent support 特征。因此，我们可以在避免将 bias 引入到 query 特征的前提下，利用 support 的背景像素。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-figure3.png"></p>
<!-- 从 Figure 3 可以看出，CyCTR 还是在增强 query 特征 -->

<p>具体实现上，support 和 query 特征都被拉伸成 1D 序列输入到 transformer 中，序列的长度分别为 $H_qW_q$ 和 $N_s$。如上图所示，$L$ 个编码器堆叠起来，每个编码器输出的 query 特征被输入到下一个编码器中的 self-alignment block，直到最终输出的 query 特征进行 pixel-wise 的分类。在 <strong>cross-alignment block</strong> 中，首先计算一个 affinity map $A = \frac{QK^T}{\sqrt{d}}, A \in \mathbb{R}^{H_qW_q \times N_s}$ 来度量所有 query 和 support 像素的关联度。然后，对于第 $j$ 个 support 像素，可以找到与其最相似的 query 像素 $i^{\star}=\text{argmax}_i A_(i,j)$。然后再返回找到与 query 像素 $i^{\star}$ 最相似的 support 像素 $j^{\star}=\text{argmax}_j A_(i^{\star},j)$。给定 support 像素标签 $M_s \in \mathbb{R}^{N_s}$，获得一个 additive bias $B \in \mathbb{R}^{N_s}$：</p>
<p>$$B_{j}= \begin{cases}0, &amp; \text { if } M_{s(j)}=M_{s\left(j^{\star}\right)} \\<br>-\infty, &amp; \text { if } M_{s(j)} \neq M_{s\left(j^{\star}\right)} \end{cases}$$</p>
<p>则对于位置为 $i$ 的单个 query token（即像素）$Z_{q(i)} \in \mathbb{R}^d$，通过下式来聚合 support 信息：</p>
<p>$$\text{CyCAtten}(Q_i, K_i, V_i) = \text{softmax}(A_{i} + B)V$$</p>
<p>当在 self-alignment block 中执行 self-attention 时，也可能存在 query token 聚合到无关甚至是有害的特征（尤其当背景较为复杂时）。然而，由于没有 query 像素的标签，无法在 query 的像素上执行 cycle-consistent attention。受到 <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=gZ9hCDWe6ke">DeformableAttention (ICLR 2021)</a>，可以通过学习的方式获得 consistent 像素 $\triangle = f(Q + \text{Coord}), \triangle \in \mathbb{R}^{H_pW_p \times P}$ 以及对应的 attention weights $A^{<code>&#125; = g(Q + \text&#123;Coord&#125;), A^&#123;</code>} \in \mathbb{R}^{H_qW_q \times P}$，$P$ 代表被聚合的像素的数量。$\text{Coord} \in \mathbb{R}^{H_qW_q \times N_s}$ 是位置编码，$f(\cdot)$ 和 $g(\cdot)$ 是用于预测 offsets 和 attention weights 的全连接层。因此，self-alignment block 中的 self-attention 可以被表示为：</p>
<p>$$\text{PredAtten}(Q_r, V_r) = \sum^P_g\text{softmax}(A^{`})_{(r,g)}V_{r+\triangle_{r,g}}$$</p>
<p>其中，$r \in {0,1, \dots, H_qW_q}$ 是拉伸的 query 特征的 index，$Q$ 和 $V$ 通过对拉伸的 query 特征用可学习的参数进行线性变换得到。</p>
<p>以上讨论的是 1-shot 的情况。当 shot 数大于 1 时，可以将所有 support 特征拉伸并拼接作为输入。然而这样会导致计算量过大，因此可以采取一个非常简单的 mask-guided sampling 策略来减小计算复杂度。给定 $k$-shot support 特征 $Z_s \in \mathbb{R}^{kH_sW_s \times d}$，support tokens（即像素）通过从所有 support 图像的前景区域均匀随机采样 $N_{fg}$ 个（$N_{fg} \leq \frac{N_s}{2}$，其中 $N_s \leq kH_sW_s$）和从背景区域采样 $N_s - N_{fg}$ 个得到。通过选择一个合适的 $N_s$，这个策略既能有效减少计算量，也能够帮助平衡前景和后景的比例。</p>
<p>方法的整体框架如下图所示。值得注意的是，该方法还是先用 <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2008.01449">PFENet (TPAMI 2020)</a> 对 support 和 query 的特征进行了处理再输入到 cycle-consistent transformer 中。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table5.png"></p>
<p>实验结果：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table1.png"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table2.png"></p>
<!-- 对于 OSAD 的参考/思考：

1. 在 FSAD 中，support 和 query 中与 affordance 不相关的像素更多，如何解决？能否参考 weak-shot segmentation 先把边界弄出来？ 
2. 用一个 CyCTR 模块来代替 FSAD 的中间处理部分呢？

-->

<h3 id="Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer"><a href="#Simpler-is-Better-Few-shot-Semantic-Segmentation-with-Classifier-Weight-Transformer" class="headerlink" title="Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer"></a>Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</h3><p>ICCV 2021 | <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2108.03032">arxiv</a> | <a target="_blank" rel="noopener" href="https://github.com/zhiheLu/CWT-for-FSS">github</a></p>
<p>分割模型通常由编码器、解码器和一个简单的分类器（通常是一个 1x1 卷积层，参数 $\mathbf{w} \in \mathbb{R}^{2 \times d}$ 用于将 $d$ 个 channel 映射来二分类以区分前景和背景像素）组成。现有的一些小样本分割方法对三部分的参数都进行元学习。而本文提出考虑到样本量极少，三部分的参数都快速适应到新类仍然很难。因此，本文提出<strong>元学习阶段只关注分类器</strong>，而<strong>采用预训练的方式来处理编码器和解码器</strong>（本文采用 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.01105">PSPNet (CVPR 2017)</a> 作为骨干分割网络），认为在经过大量基类的数据训练后它们能够泛化到新类上。之后，编、解码器的参数被冻结。在元学习阶段，首先使用 support 样本来训练分类器参数 $\mathbf{w}$。本文提出这样得到的总体模型的表现已经能够超越（当时的）SOTA <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.06309">PPNet (ECCV 2020)</a>，如下表 1、5 所示：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table1.PNG"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table5.PNG"></p>
<p>然而，本文继续提出，由于较大的类内变化（或者说类内差异）的存在，在 support 样本上学到的 $\mathbf{w}$ 无法很好地适应到每个 query 样本上。因此，本文提出 <b>Classifier Weight Transformer (CWT)</b>，在元学习阶段去学习每个 query 样本特定的分类器权重。模型流程图如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-figure3.png"></p>
<p>具体地，CWT 中的 self-attention 可以被表示为：</p>
<p>$$\mathbf{w}^{*}=\mathbf{w}+\psi\left(\text{softmax}\left(\frac{\boldsymbol{w} \mathbf{W}_{q}\left(\boldsymbol{F} \mathbf{W}_{k}\right)^{\top}}{\sqrt{d_{a}}}\right)\left(\boldsymbol{F} \mathbf{W}_{v}\right)\right)$$</p>
<p>其中，$\boldsymbol{F} \in \mathbb{R}^{n \times d}$ 代表 query 样本 $n$ 个像素提取出的特征，$\mathbf{W}_{q}/\mathbf{W}_{k}/\mathbf{W}_{v} \in \mathbb{R}^{n \times d_{a}}$ 是可训练参数，而 $\mathbf{w}^{*}$ 即是将用在该 query 样本上的参数。CWT 的直觉是，相比背景像素，通常情况下 query 中的前景像素特征会和 $\mathbf{w}$ 计算得到更大的相似度，因此可以据此调整 $\mathbf{w}$。在（元）测试阶段，CWT 的参数也是冻结的。</p>
<p>除开表 1、5 外的其他实验结果：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table23.PNG"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table4.PNG"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICCV2021-FSSS-with-clssifier-weight-transformer-table6.PNG"></p>
<!-- 这种 Transformer 结构能否用到 few-shot 3D 点云分割上？或者说这种 encoder 和 decoder 一起用所有 base data pre-train 然后 freeze 的范式 -->

<h3 id="Few-Shot-Segmentation-Without-Meta-Learning-A-Good-Transductive-Inference-Is-All-You-Need"><a href="#Few-Shot-Segmentation-Without-Meta-Learning-A-Good-Transductive-Inference-Is-All-You-Need" class="headerlink" title="Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?"></a>Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?</h3><p>CVPR 2021 | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html">link</a> | <a target="_blank" rel="noopener" href="https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation">github</a></p>
<p>本文提出现有的小样本分割工作所遵循的两个假设在现实场景中通常是不可用的：(1) episodic training 假设 testing tasks 的 support shots 数量和 meta-training 阶段使用的任务保持一致；(2) base 和 novel 类别通常被假设从同一数据集中采样得到。针对假设 (1)，本文的思想与 <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rylXBkrYDS">Transductive Fine-Tuning, ICLR 2020</a> 等小样本识别方法有共通之处，提出精心设计的 transductive fine-tuning 的方法 <b>RePRI (Region Proportion Regularized Inference)</b>，能够用线性的分类器取得 SOTA。针对假设 (2)，本文也引入了 cross-domain 的设置，即 base 和 novel 类别来自不同的数据集，并在 COCO-&gt;PASCAL 上取得 SOTA。</p>
<p>在 meta-training 阶段，RePRI 不采用 episodic training，而是用整个 base set 和标准交叉熵来训练特征提取器。在每个 testing task 进行 inference 时，RePRI 在优化时采用的损失包含三项：</p>
<ol>
<li>$\mathrm{CE}=-\frac{1}{K|\Psi|} \sum_{k=1}^{K} \sum_{j \in \Psi} \widetilde{y}_{k}(j)^{\top} \log \left(p_{k}(j)\right)$：标准的交叉熵 (cross-entropy, CE)，作用于 support 图像的有标签像素上。注意只使用这一项来进行优化通常导致过拟合于 support set，尤其是 1-shot 设置时；</li>
<li>$\mathcal{H}=-\frac{1}{|\Psi|} \sum_{j \in \Psi} p_{\mathcal{Q}}(j)^{\top} \log \left(p_{\mathcal{Q}}(j)\right)$：香农熵 (Shannon entropy)，作用于 query 图像像素的预测上，来使模型对 query 图像的预测更加有信心。直观来说，这一项让线性分类器的决策边界推向 query 特征空间的低密度区域。在对最初置信度较低的区域的预测有帮助的同时，仅仅将这项加入到损失中并不能解决 CE 导致的问题，甚至可能使表现进一步恶化，如下图 Figure 1 所示；</li>
<li>$\mathcal{D}_{\mathrm{KL}}=\widehat{p}_{\mathcal{Q}}^{\top} \log \left(\frac{\widehat{p}_{\mathcal{Q}}}{\pi}\right), \widehat{p}_{\mathcal{Q}}=\frac{1}{|\Psi|} \sum_{j \in \Psi} p_{\mathcal{Q}}(j)$：KL 散度，鼓励模型预测的背景/前景 (B/F) 比例接近于一个参数 $\pi \in [0, 1]^2$。论文指出这一项在损失中占关键位置，首先当参数 $\pi$ 与 query 图像的精确 B/F 比例不匹配时，该项有助于避免因 $\mathrm{CE}$ 和 $\mathcal{H}$ 最小化而导致的退化解；而如果能够准确估计 query 图像中的 B/F 比例（即有这个先验知识可供使用时），该项可以大幅提高方法整体的性能。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-figure1.png"></p>
<p><strong>分类器的选择</strong>：RePRI 的线性分类器和 <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1904.04232">Baseline++, ICLR 2019</a> 相似：$s^{(t)}(j)=\text{sigmoid}\left(\tau\left[\cos \left(z(j), w^{(t)}\right)-b^{(t)}\right]\right)$。其中原型 $w^{(0)}$ 是 support 前景特征的均值：$w^{(0)}=\frac{1}{K|\Psi|} \sum_{k=1}^{K} \sum_{j \in \Psi} \widetilde{y}_{k}(j)_{1} z_{k}(j)$；$b^{(0)}$ 是对 query 前景的 soft predictions的均值 ：$b^{(0)}=\frac{1}{|\Psi|} \sum_{j \in \Psi} p_{\mathcal{Q}}(j)_{1}$。</p>
<p><strong>B/F 比例 $\pi$ 的联合估计</strong>：当没有先验知识时，RePRI 使用 $\widehat{p}_{\mathcal{Q}}$ 来联合学习 $\pi$ 和分类器的参数，这时 $\mathcal{D}_{\mathrm{KL}}$ 可以被视为 self-regularization 来防止模型的 marginal distribution 发生偏移。具体实现中，只在初始化后在之后的某一轮迭代 $t_{\pi}$ 更新一次 $\pi$ 即可，即</p>
<p>$$\pi^{(t)}= \begin{cases}\widehat{p}_{\mathcal{Q}}^{(0)} &amp; 0 \leq t \leq t_{\pi} \\<br>\hat{p}_{\mathcal{Q}}^{\left(t_{\pi}\right)} &amp; t&gt;t_{\pi}\end{cases}$$</p>
<p>实验结果：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-table1.png"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-table2.png"></p>
<p>可以看到在 5-shot 上 RePRI 取得了全面的 SOTA。本文同样做了包含 (1) 训练时采用 1-shot task、测试时用 1-\5-\10-shot task；(2) cross-domain 等实验。具体请看原论文。</p>
<h3 id="Self-Guided-and-Cross-Guided-Learning-for-Few-Shot-Segmentation"><a href="#Self-Guided-and-Cross-Guided-Learning-for-Few-Shot-Segmentation" class="headerlink" title="Self-Guided and Cross-Guided Learning for Few-Shot Segmentation"></a>Self-Guided and Cross-Guided Learning for Few-Shot Segmentation</h3><p>CVPR 2021 | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Self-Guided_and_Cross-Guided_Learning_for_Few-Shot_Segmentation_CVPR_2021_paper.html">link</a> | <a target="_blank" rel="noopener" href="https://github.com/zbf1991/SCL">github</a></p>
<p>本文同样提出常用的 masked Global Average Pooling (GAP) 来将 support 图像转换为特征向量的方式会导致有区分性的信息因为求均值操作而损失。另外，当 shot 数量不为 1 时，常用的对所有 support 向量求均值会强制使得所有的 support 图像贡献相同，而不同的 support 图像的对于表示类别的贡献实际上是不同的。</p>
<p>为了解决这两个问题，本文提出 <b>Self-Guided and Cross-Guided Learning (SCL)</b>，首先用初始原型来对 support 图像做初始预测，预测覆盖和没有覆盖到的前景区域被用 masked GAP 编码成 primary 和 auxiliary support 向量来在 query 图像的分割上取得更好表现。同时，针对 shot 数量不为 1 的场景，本文提出 <strong>Cross-Guided Module (CGM)</strong> 来使用其他有标注 support 图像评估每张 support 图像的预测质量，使高质量的 support 图像能够对最终的融合做出更大的贡献。相比 attention 等复杂的方法，CGM 无需重新训练模型，可以直接在 inference 时被应用来提升最终的表现。SCL 遵循 episodic training 的方式，总损失为 $\mathcal{L}=\mathcal{L}_{c e}^{s 1}+\mathcal{L}_{c e}^{s 2}+\mathcal{L}_{c e}^{q}$，前两项来自 support set 的 Self-Guided Learning，后一项来自 query set。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure3.png"></p>
<p><strong>Self-Guided Learning on Support Set</strong>：这里首先以 1-shot 为例。用 masked GAP 获得的初始 support 向量 $\mathcal{v}_s$ 被扩展成和 support 特征图 $F_s$ 同样大小的 $V_s$，然后拼接得到新的特征图 $F_{sv} = \text{Concat}([F_s, V_s, V_s])$。通过将 $F_{sv}$ 输入到 <strong>support FPM</strong> 和 decoder，可以得到support 图像的概率图 $P_{s1} = \text{softmax}(\mathcal{D}(FPM_s(F_{sv})))$，其中 $\mathcal{D}(\cdot)$ 指 decoder。由于 SCL 是一个即插即用的模块，因此 support FPM 和 decoder 的设计遵循所选择的小样本分割 baseline。本文的实验部分选择插入到两种 baseline 中：(1) <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1903.02351">CANet (CVPR 2019)</a>，其 decoder 是 single-scale 结构的，因此 SCL 也采用 single-scale support FPM；(2) <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2008.01449">PFENet (TPAMI 2020)</a>，其 decoder 是 multi-scale 结构的，因此 SCL 也采用 multi-scale support FPM。single-scale 和 multi-scale 的 support FPM 结构如上图右边所示。</p>
<p>使用预测的 mask $\hat{M_s} = \text{argmax}(P_{s1})$ 和 ground-truth mask $M_s$，可以得到 primary support 向量 $\mathcal{v}_{pri}$ 和 auxiliary support 向量 $\mathcal{v}_{aux}$，前者代表预测正确的实际前景信息，后者代表预测错误的实际前景信息。换句话说，$\mathcal{v}_{pri}$ 保持着主要的 support 信息，而 $\mathcal{v}_{aux}$ 包含用 $\mathcal{v}_s$ 无法预测的、损失的重要信息。为了保证 $\mathcal{v}_{pri}$ 能够从 support 的特征图收集到大多数的信息，对 $P_{s1}$ 施加交叉熵得到 $\mathcal{L}_{c e}^{s 1}$：</p>
<p>$$\mathcal{L}_{c e}^{s 1}=-\frac{1}{h w} \sum_{i=1}^{h w} \sum_{c_{j} \in{0,1}}\left[M_{s}(i)=c_{j}\right] \log \left(P_{s 1}^{c_{j}}(i)\right)$$</p>
<p>接下来，$\mathcal{v}_{pri}$ 和 $\mathcal{v}_{aux}$ 也被扩展并与 $F_s$ 拼接得到 $F^A_{s} = \text{Concat}([F_s, V^{pri}_s, V^{aux}_s])$，然后得到 $P_{s2} = \text{softmax}(\mathcal{D}(FPM_s(F^A_{s})))$。为了确保集成 $\mathcal{v}_{pri}$ 和 $\mathcal{v}_{aux}$ 能够得到精准的分割 mask，同样施加交叉熵得到 $\mathcal{L}_{c e}^{s 2}$：</p>
<p>$$\mathcal{L}_{c e}^{s 2}=-\frac{1}{h w} \sum_{i=1}^{h w} \sum_{c_{j} \in{0,1}}\left[M_{s}(i)=c_{j}\right] \log \left(P_{s 2}^{c_{j}}(i)\right)$$</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure2.png"></p>
<p><strong>Training on Query Set</strong>：对于 query 的特征图 $F_q$，流程为 $F^A_{q} = \text{Concat}([F_q, V^{pri}_q, V^{aux}_q])$，其中 $V^{pri}_q$ 和 $V^{aux}_q$ 是 $\mathcal{v}_{pri}$ 和 $\mathcal{v}_{aux}$ 扩展大小得到；$P_{q} = \text{softmax}(\mathcal{D}(FPM_q(F^A_{q})))$，注意这里有一个单独的 <strong>query FPM</strong> $FPM_q$ 而非 $FPM_s$。同样计算交叉熵得到 $\mathcal{L}_{c e}^{q}$：</p>
<p>$$\mathcal{L}_{c e}^{q}=-\frac{1}{h w} \sum_{i=1}^{h w} \sum_{c_{j} \in{0,1}}\left[M_{q}(i)=c_{j}\right] \log \left(P_{q}^{c_{j}}(i)\right)$$</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure5.png"></p>
<p><strong>Cross-Guided Multiple Shot Learning</strong>：当 shot 数 &gt; 1 时，对于第 k 张 support 图像，首先将其作为 support 图像，将所有 K 张 support 图像作为 query 图像来输入到所提出的面向 1-shot 的模型中。对于第 i 张 support 图像，得到在第 k 张图像的支持下的预测 mask $\hat{M}_{s}^{i \mid k}$。因为第 i 张 support 图像的 ground-truth mask $M_s^i$ 是可得的，因此可以用预测和 ground-truth 的 masks 的 IOU 来计算一个置信度：</p>
<p>$$U_{s}^{k}=\frac{1}{K} \sum_{i=1}^{K} \text{IOU}\left(\hat{M}_{s}^{i \mid k}, M_{s}^{i}\right)$$</p>
<p>则对于给定的 query 图像的最终预测 score map：</p>
<p>$$\hat{P}_{q}=\text{softmax}\left(\frac{1}{K} \sum_{k=1}^{K} U_{s}^{k} \mathcal{G}\left(I_{q} \mid I_{s}^{k}\right)\right)$$</p>
<p>可以看到有更大的 $U_{s}^{k}$ 的 support 图像对于最终的预测有更大的贡献。</p>
<p>实验结果：</p>
<!-- 所以说设计即插即用的模块就是好，加在 SOTA 上就是新的 SOTA。 -->

<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-table1.png"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-table2.png"></p>
<h3 id="Adaptive-Prototype-Learning-and-Allocation-for-Few-Shot-Segmentation"><a href="#Adaptive-Prototype-Learning-and-Allocation-for-Few-Shot-Segmentation" class="headerlink" title="Adaptive Prototype Learning and Allocation for Few-Shot Segmentation"></a>Adaptive Prototype Learning and Allocation for Few-Shot Segmentation</h3><p>CVPR 2021 | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Adaptive_Prototype_Learning_and_Allocation_for_Few-Shot_Segmentation_CVPR_2021_paper.html">link</a> | <a target="_blank" rel="noopener" href="https://github.com/Reagan1311/ASGNet">github</a></p>
<p>本文认为虽然相比需要用稠密 affinity 矩阵来解决欠约束的像素匹配问题而容易过拟合的 affinity learning，常用的 prototypical learning 能够比单纯的像素特征更加鲁棒，但是仅用一个 prototype 不足以表示包含空间信息在内的所有信息。特别地，本文希望能够根据图像内容自适应地调整 prototypes 的数量和空间范围，从而能够更好地处理物体在尺寸和形状上的变化。例如对一个尺寸较小的物体，可能一个或少量 prototypes 就足够了；而对于尺寸较大的物体，可能需要更多的 prototypes 来表示所有重要的信息。</p>
<p>因此，本文提出 <b>Adaptive Superpixel-guided Network (ASGNet)</b>，包含 <strong>superpixel-guided clustering (SGC)</strong> 和 <strong>guided prototype allocation (GPA)</strong> 两个模块用于提取和分配多个 prototypes。SGC 模块在 support 图像上进行快速的、基于特征的 superpixel 提取，得到的 superpixel centroids 可以被视为 prototypical 特征，同时 superpixel 的数量和形状都是适应于图像内容的，因此得到的 prototypes 是自适应的。GPA 模块用一个 attention-like 机制来讲最相关的 support prototype 特征分配给 query 图像中的每个像素。最后，ASGNet 使用 <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2008.01449">PFENet (TPAMI 2020)</a> 中的特征增强结构并建立一个 FPN-like top-down 结构来引入 multi-scale 信息。当 shot 数大于 1 时，每张 support 图像都可以得到一组 prototypes，所有的 prototypes 被一块作为 GPA 的选择范围。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure2.png"></p>
<p><strong>Superpixel-guided Clustering</strong>：受到 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.09518">maskSLIC (2016)</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.10174">Superpixel sampling networks (ECCV 2018)</a> 等工作的启发，SGC 模块被用于将特征图用聚类的方式集成到多个 superpixel centroids 中。给定 support 特征 $F_s \in \mathbb{R}^{c \times h \times w}$ 和 support mask $M_s \in \mathbb{R}^{h \times w}$，假设已经获得初始 superpixel 种子，SGC 首先将 support 特征图上每个像素的坐标的值进行放缩后和特征图进行拼接，从而引入位置信息。之后用 support mask 来筛除背景信息，这样我们获得 $F_s^{`} \in \mathbb{R}^{(c+2) \times N_m}$，$N_m$ 是在 support mask 中的像素的数量（注意这里我们对变量及其维度表示和原论文略有差别，我在阅读了源码后觉得这样表述会更清晰）。同样，我们也将初始 superpixel 种子的特征和其值放缩后的坐标拼接，有 $S^{0} \in \mathbb{R}^{(c+2) \times N_{sp}}$（$N_{sp}$ 是 superpixel 的数量。获得初始种子的方法见本节最后一段）。接下来，SGC 通过迭代式的方法更新 superpixel-based prototypes：在第 $t$ 轮迭代，首先计算每个像素 $p$ 和所有 superpixels 的 association map $Q^t$：</p>
<p>$$Q^t_{pi} = e^{- || F^{`}_p - S^{t-1}_i ||^2}$$</p>
<p>接下来，新的 superpixel centroids 被更新为 masked 特征的加权和：</p>
<p>$$S^t_i = \frac{1}{\sum_pQ^t_{pi}} \sum^{N_m}_{p=1} Q^t_{pi} F^{`}_p$$</p>
<!-- 这和那个基于 E-M 的有什么区别？区别好像是在于 E-M 会更新 base、attention 和 query features，而这个只更新 superpixel centroids -->


<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure3.png"></p>
<p>通常 superpixel 算法通过将图像划分为均匀大小的 grid cell 来选取初始种子（i.e., superpixel），但由于只需要从前景区域来初始化种子，本文参考 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.09518">maskSLIC (2016)</a> 来迭代式地安置每个初始种子，流程如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure5.png"></p>
<p><strong>Guided Prototype Allocation</strong>：首先计算每个 prototype 和 query 特征每个位置的 cosine 相似度 $C^{x,y}_i$，这个相似度信息被输入到一个双分支结构。<strong>第一个分支</strong>计算每个位置的像素和哪个 prototype 最相似：</p>
<p>$$G^{x,y} = \text{argmax}_{i \in {0, \dots, N_{sp}}} C^{x,y}_i$$</p>
<p>根据得到的 guide map $G \in \mathbb{R}^{h \times w}$，可以通过将对应 prototype 放到 guide map 的每个位置得到 guide feature $F_G \in \mathbb{R}^{c \times h \times w}$。<strong>第二个分支</strong>将相似度信息 $C$ 在所有的 superpixels 相加来得到概率图 $P$。最终将 $P$、$F_G$ 和原本的 query 特征 $F_Q$ 拼接并过 $1 \times 1$ 卷积得到 refined query 特征 $F^{`}_Q$。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure4.png"></p>
<p>实验结果：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-table5.png"></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-table7.png"></p>
<h3 id="Few-Shot-3D-Point-Cloud-Semantic-Segmentation"><a href="#Few-Shot-3D-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Few-Shot 3D Point Cloud Semantic Segmentation"></a>Few-Shot 3D Point Cloud Semantic Segmentation</h3><p>CVPR 2021 | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Few-Shot_3D_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.html">link</a> | <a target="_blank" rel="noopener" href="https://github.com/Na-Z/attMPTI">github</a></p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-figure1.png"></p>
<p>第一篇做小样本 3D 点云语义分割的论文。相比 2D 图像，点云更加无结构和无序，因此做语义分割的难度会更大。任务定义如上图所示，基本就是把 2D 数据换成 3D 点云。方法上，本文提出了 <strong>attention-aware multi-prototype transductive inference</strong> 框架。其包含以下三点：</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-figure2.png"></p>
<p><strong>embedding network</strong>：对标 2D 视觉中的 CNN 等 backbone 网络。论文提出这个网络需要满足 (1) 能够<strong>根据局部上下文编码几何结构</strong>；(2) 能够<strong>根据全局上下文编码点云的语义信息和它们间的语义关系</strong>；(3) 能够快速适应不同的小样本任务。因此，本文提出一种 attention-aware multi-level feature learning network 来结合局部几何特征、全局语义特征和 metric-adaptive 特征。具体来说，该网络由三部分组成：(1) feature extractor，选用动态图 CNN 架构的 <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1801.07829">DGCNN (ACM Trans. Graph. 2019)</a> 来得到局部几何特征（第一个 EdgeConv 层的输出）和语义特征（整个 feature extractor 的输出）；(2) attention learner，选用了 self-attention network (SAN)；(3) metric learner，选用了 MLP 的堆叠并以一个相对更大的学习率更新。</p>
<!-- embedding 具体怎么运算论文（至少在正文）也没提，需要看代码理解 -->

<p><strong>multi-prototype generation</strong>：对于 support set 中 $N+1$ 类的每一个，都通过聚类生成 $n$ 个 prototypes。具体地，$n$ 个种子点通过 farthest point sampling 被从 support 点中采样得到。直觉上来说，如果 embedding space 学得够好，这个空间中最远的那些点能够可以内在地表示一个类别的不同视角。之后，我们计算其他点和这些种子点的距离并根据最近邻原则分配，最后计算每个簇的均值作为 prototypes。</p>
<p><strong>transductive inference</strong>：该方法首先构建了一张包含 $n \times (N+1)$ 个 prototypes 和 $T \times M$ 个 query 点、总计 $V = n \times (N+1) + T \times M$ 个节点的 k-NN graph。稀疏邻接矩阵 $\mathbf{A} \in \mathbb{R}^{V \times V}$ 通过计算每个点和其 $k$ 个最近邻的高斯相似度得到。为了让邻接矩阵是非负且对称的，有 $\mathbf{W} = \mathbf{A} + \mathbf{A}^{T}$，并进一步进行正则化有 $\mathbf{S} = \mathbf{D}^{-1/2}\mathbf{W}\mathbf{D}^{-1/2}$。同时定义标签矩阵 $\mathbf{Y} \in \mathbb{R}^{V \times (N+1)}$，其中对应有标签 prototypes 的行是 one-hot ground-truth 标签，其余为零向量。给定 $\mathbf{S}$ 和 $\mathbf{Y}$，标签传播有解析解 $\mathbf{Z}^{*} = (\mathbf{I} - \alpha \mathbf{S})^{-1}\mathbf{Y}$。最后每个点云对应的预测通过 softmax 后用交叉熵计算损失。</p>
<p>数值的实验结果如下表所示。原论文还有一些 ablation study 和分割结果的可视化。</p>
<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-table.png"></p>
<!-- ### Zero-Shot Instance Segmentation

CVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Zero-Shot_Instance_Segmentation_CVPR_2021_paper.html) | [github](https://github.com/zhengye1995/Zero-shot-Instance-Segmentation)

这篇论文写的有点烂...有些细节好模糊 -->



<!-- ### UniT: Unified Knowledge Transfer for Any-shot Object Detection and Segmentation

CVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Khandelwal_UniT_Unified_Knowledge_Transfer_for_Any-Shot_Object_Detection_and_Segmentation_CVPR_2021_paper.html) | [arxiv](http://arxiv.org/abs/2006.07502) | [github](https://github.com/ubc-vision/UniT) -->

<!-- ![](https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-unified-knowledge-transfer-for-any-shot-object-detection-and-segmentation-figure2.png) -->

<!-- CVPR2021-unified-knowledge-transfer-for-any-shot-object-detection-and-segmentation -->


<!-- ### Anti-Aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation

CVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Anti-Aliasing_Semantic_Reconstruction_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.html) | [github（代码不完整）](https://github.com/Bibkiller/ASR) -->

<!-- ### Scale-Aware Graph Neural Network for Few-Shot Semantic Segmentation

CVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Xie_Scale-Aware_Graph_Neural_Network_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.html) | 代码未开源 -->


<!-- ### Rich Embedding Features for One-Shot Semantic Segmentation

TNNLS 2021 | [link](https://ieeexplore.ieee.org/abstract/document/9463398) | 代码未开源

方法看示意图基本就了解得十有八九了。没源码，就不细看了。 -->




<!-- ### Hypercorrelation Squeeze for Few-Shot Segmenation

ICCV 2021 | [arxiv](http://arxiv.org/abs/2104.01538) | [github](https://github.com/juhongm999/hsnet) | [知乎解读](https://zhuanlan.zhihu.com/p/452427807)

### Mining Latent Classes for Few-shot Segmentation

ICCV 2021 | [arxiv](http://arxiv.org/abs/2103.15402) | [github](https://github.com/LiheYoung/MiningFSS)

### Learning Meta-class Memory for Few-Shot Semantic Segmentation

ICCV 2021 | [pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Learning_Meta-Class_Memory_for_Few-Shot_Semantic_Segmentation_ICCV_2021_paper.pdf) | [github](https://github.com/wu-zhonghua/MM-Net)

### Few-Shot Semantic Segmentation with Cyclic Memory Network

ICCV 2021 | [pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Xie_Few-Shot_Semantic_Segmentation_With_Cyclic_Memory_Network_ICCV_2021_paper.pdf) | 代码未开源 -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']]}
});
</script>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Kyon Huang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://kyonhuang.top/blog/few-shot-segmentation-2021/" title="【paper reading】2021 小样本分割论文选读">https://kyonhuang.top/blog/few-shot-segmentation-2021/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/blog/tags/few-shot-segmentation/" rel="tag"><i class="fa fa-tag"></i> few-shot segmentation</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blog/ICLR2021-notes/" rel="prev" title="【paper reading】ICLR 2021 论文选读">
                  <i class="fa fa-chevron-left"></i> 【paper reading】ICLR 2021 论文选读
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2016 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kyon Huang</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/next-boot.js"></script>

  
<script src="/blog/js/third-party/search/local-search.js"></script>






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
