<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kyon Huang 的博客</title>
  
  <subtitle>Github 连接不稳定，请科学上网以获取最佳访问效果</subtitle>
  <link href="https://kyonhuang.top/blog/atom.xml" rel="self"/>
  
  <link href="https://kyonhuang.top/blog/"/>
  <updated>2021-12-17T16:33:35.145Z</updated>
  <id>https://kyonhuang.top/blog/</id>
  
  <author>
    <name>Kyon Huang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【paper reading】2021 小样本分割论文选读</title>
    <link href="https://kyonhuang.top/blog/few-shot-segmentation-2021/"/>
    <id>https://kyonhuang.top/blog/few-shot-segmentation-2021/</id>
    <published>2021-12-14T08:39:35.000Z</published>
    <updated>2021-12-17T16:33:35.145Z</updated>
    
    <content type="html"><![CDATA[<p>根据手头想法的需要，读一读 2021 年顶会顶刊的小样本分割相关论文并做笔记于此。有开源代码的论文优先，持续更新。</p><!-- [参考](https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers) --><span id="more"></span><h3 id="Prior-Guided-Feature-Enrichment-Network-for-Few-Shot-Segmentation"><a href="#Prior-Guided-Feature-Enrichment-Network-for-Few-Shot-Segmentation" class="headerlink" title="Prior Guided Feature Enrichment Network for Few-Shot Segmentation"></a>Prior Guided Feature Enrichment Network for Few-Shot Segmentation</h3><p>TPAMI 2020 | <a href="http://arxiv.org/abs/2008.01449">link</a> | <a href="https://github.com/dvlab-research/PFENet">github</a></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure3.png"></p><p>这篇工作被很多 2021 年的相关论文作为 baseline 并且效果仍然有竞争力，因此先介绍一下。该论文提出了 <strong>Prior Guided Feature Enrichment Network (PFENet)</strong> 来解决两个问题：(1) 很多分割方法都依赖于 high-level 特征，然而 <a href="http://arxiv.org/abs/1903.02351">CANet, CVPR 2019</a> 的实验结果表明<strong>小样本模型仅仅使用 high-level 特征会导致表现下降</strong>；(2) 样本数量过少会导致 support 物体的尺寸和姿态都和 query target 有很大不同，在该文中被称为 <strong>spatial inconsistency</strong>，并且 ablation study 的实验结果表明仅仅集成 multi-scale 结构对于该问题只是次优解。因此，本文提出了以下两点贡献：</p><p><strong>prior generation</strong>：使用 <strong>high-level 特征</strong>（实践中是 conv5_x 的最后一层的输出）来生成 prior mask。在用 support mask 遮盖提取的 support 特征后，计算 support 和 query 特征的 pixel-wise cosine 相似度。对于 query 中的每个像素，取其和所有 support 像素的相似度中的最大值来得到 prior mask。之后 prior mask 经过 min-max normalization 来将所有值放缩到 [0, 1] 范围。如果 shot 数大于 1，将同一 query 特征和不同 support 特征得到的多张 prior masks 求平均来得到最终的 prior mask 输入到 FEM。<!-- 源码显示这里是先单独做 normalization 再总体求平均 --></p><!-- 按照 CyCTR 的意思，这里只用 support 的前景信息来计算这个 prior mask 就不合适了。正好 OSAD 任务没有完全的前景信息，如何在这一块上做一些改变，让它不像现在的样子？ --><!-- 和 OSADNet_v2 的最后一个模块有点像，不过那个是 query 和其他 query 算的一个 transductive 方法 --><!-- 另外在 5-shot 设置能不能不要单纯求均值？感觉会有信息损失 --><p><strong>Feature Enrichment Module (FEM)<strong>：这一块看下图比较好理解，就是用多个 average pooling 来将 query 特征、support prototype 和 prior mask 的拼接产物放缩到不同大小，然后做 multi-scale 的信息交互。注意 query 特征和 support 特征是通过</strong>拼接 middle-level 特征</strong>（实践中是 conv3_x、conv_4x 的最后一层的输出）得到，另外如果 shot 数大于 1，直接取所有处理后的 support 特征的均值作为新的 support 特征。inter-scale interaction 的每个 scale 的产物也会过一个 3x3 + 1x1 的卷积层组成的分类头得到损失 $\mathcal{L}^i_1$，然后所有 scale 产物拼接后经过 1x1 卷积也用分类头得到损失 $\mathcal{L}<em>2$。最终的总损失为 $\mathcal{L} = \frac{\sigma}{n}\sum^n</em>{i=1} \mathcal{L}^i_1 + \mathcal{L}_2$，$\sigma$ 在所有实验中设为 1.0。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure4.png"></p><p>一个稍微需要注意的点是图中带圈的 M 的结构如下图所示，其中 auxiliary 特征指 finer 特征，main 特征指 coarse 特征。对于没有 auxiliary 特征的 main 特征（如上图最上面的一个 scale），省略下图的第一步拼接操作。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-figure5.png"></p><p>实验结果：</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-table1.png"></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TPAMI2020-prior-guided-feature-enrichment-network-for-FSS-table3.png"></p><h3 id="Few-Shot-Segmentation-Via-Cycle-Consistent-Transformer"><a href="#Few-Shot-Segmentation-Via-Cycle-Consistent-Transformer" class="headerlink" title="Few-Shot Segmentation Via Cycle-Consistent Transformer"></a>Few-Shot Segmentation Via Cycle-Consistent Transformer</h3><p>NeurIPS 2021 | <a href="https://papers.nips.cc/paper/2021/hash/b8b12f949378552c21f28deff8ba8eb6-Abstract.html">link</a> | <a href="https://github.com/GengDavid/CyCTR">github（尚未包含代码）</a></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-figure1.png"></p><p>已有的小样本分割工作通常从 support 的图像特征提取 semantic-level prototypes，其中根据可以分为两类：(1) 如上图 (a) 所示的 class-wise mean pooling (<a href="http://arxiv.org/abs/1908.06391">PANet, ICCV 2019</a>; <a href="http://arxiv.org/abs/1810.09091">SG-One, IEEE Trans. Cybern., 2020</a>; <a href="http://arxiv.org/abs/1903.02351">CANet, CVPR 2019</a>)，将属于不同类别的区域的特征算均值得到 prototypes；(2) 如上图 (b) 所示的 clustering，使用 EM 算法 (<a href="https://arxiv.org/abs/2008.03898">RPMM, ECCV 2020</a>) 或者 K-means (<a href="https://arxiv.org/abs/2007.06309">PPNet, ECCV 2020</a>) 来生成多个 prototypes 。然而，基于 prototypes 的方法会导致不同程度的 support information 的损失。因此，一些工作 (<a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.html">PGNet, ICCV 2019</a>; <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580715.pdf">DAN, ECCV 2020</a>) 用 attention 机制来从 support 前景像素提取信息。然而，<strong>这些方法忽略了存在对分割有帮助的信息的 support 背景像素</strong>；同时，support 和 query 图像间的包含尺度、颜色、场景等差别使得只有一小部分 support 像素是对 query 图像的分割有帮助，而<strong>另一些 support 像素会和 query 的前景像素有很大差别，导致 attention 无法准确表示语义上的对齐</strong>。</p><p>本文提出了 <strong>Cycle-Consistent Transformer (CyCTR)</strong> 模块，包含两种 Transformer 部件：(1) <strong>self-alignment block</strong>：通过聚合相关的上下文信息来编码 query 的图像特征，Query、Key 和 Value 都来自同个 embedding；(2) <strong>cross-alignment block</strong>：将 support 图像的 pixel-wise 特征聚合到 query 图像的 pixel-wise 特征中，query 图像的特征是 Query，support 图像的特征作为 Key 和 Value。另外，一种新的 <strong>cycle-consistent attention</strong> 被部署在 cross-alignment block 上。从一个 support 像素的特征出发，我们找到它在 query 特征上的最近邻，然后在找到与这个最近邻最相似的 support 特征。如果起点和终点的 support 特征属于同一类别，就称作建立起了 <strong>cycle-consistency relationship</strong>。这个操作被集成到 attention 中来让 query 特征只去注意 cycle-consistent support 特征。因此，我们可以使用 support 的背景像素，而避免将 bias 引入到 query 特征。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-figure3.png"></p><!-- 具体地 --><p>实验结果：</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table1.png"></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-cycle-consistent-TRansformer-table2.png"></p><!-- 思考：1. 在 FSAD 中，support 和 query 中与 affordance 不相关的像素更多，如何解决？能否参考 weak-shot segmentation 先把边界弄出来？ 2. 用一个 CyCTR 模块来代替 FSAD 的中间处理部分呢？--><h3 id="Few-Shot-Segmentation-Without-Meta-Learning-A-Good-Transductive-Inference-Is-All-You-Need"><a href="#Few-Shot-Segmentation-Without-Meta-Learning-A-Good-Transductive-Inference-Is-All-You-Need" class="headerlink" title="Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?"></a>Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?</h3><p>CVPR 2021 | <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Boudiaf_Few-Shot_Segmentation_Without_Meta-Learning_A_Good_Transductive_Inference_Is_All_CVPR_2021_paper.html">link</a> | <a href="https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation">github</a></p><p>本文提出现有的小样本分割工作所遵循的两个假设在现实场景中通常是不可用的：(1) episodic training 假设 testing tasks 的 support shots 数量和 meta-training 阶段使用的任务保持一致；(2) base 和 novel 类别通常被假设从同一数据集中采样得到。针对假设 (1)，本文的思想与 <a href="https://openreview.net/forum?id=rylXBkrYDS">Transductive Fine-Tuning, ICLR 2020</a> 等小样本识别方法有共通之处，提出精心设计的 transductive fine-tuning 的方法 **RePRI (Region Proportion Regularized Inference)**，能够用线性的分类器取得 SOTA。针对假设 (2)，本文也引入了 cross-domain 的设置，即 base 和 novel 类别来自不同的数据集，并在 COCO-&gt;PASCAL 上取得 SOTA。</p><p>在 meta-training 阶段，RePRI 不采用 episodic training，而是用整个 base set 和标准交叉熵来训练特征提取器。在每个 testing task 进行 inference 时，RePRI 在优化时采用的损失包含三项：</p><ol><li>$\mathrm{CE}=-\frac{1}{K|\Psi|} \sum_{k=1}^{K} \sum_{j \in \Psi} \widetilde{y}<em>{k}(j)^{\top} \log \left(p</em>{k}(j)\right)$：标准的交叉熵 (cross-entropy, CE)，作用于 support 图像的有标签像素上。注意只使用这一项来进行优化通常导致过拟合于 support set，尤其是 1-shot 设置时；</li><li>$\mathcal{H}=-\frac{1}{|\Psi|} \sum_{j \in \Psi} p_{\mathcal{Q}}(j)^{\top} \log \left(p_{\mathcal{Q}}(j)\right)$：香农熵 (Shannon entropy)，作用于 query 图像像素的预测上，来使模型对 query 图像的预测更加有信心。直观来说，这一项让线性分类器的决策边界推向 query 特征空间的低密度区域。在对最初置信度较低的区域的预测有帮助的同时，仅仅将这项加入到损失中并不能解决 CE 导致的问题，甚至可能使表现进一步恶化，如下图 Figure 1 所示；</li><li>$\mathcal{D}<em>{\mathrm{KL}}=\widehat{p}</em>{\mathcal{Q}}^{\top} \log \left(\frac{\widehat{p}<em>{\mathcal{Q}}}{\pi}\right), \widehat{p}</em>{\mathcal{Q}}=\frac{1}{|\Psi|} \sum_{j \in \Psi} p_{\mathcal{Q}}(j)$：KL 散度，鼓励模型预测的背景/前景 (B/F) 比例接近于一个参数 $\pi \in [0, 1]^2$。论文指出这一项在损失中占关键位置，首先当参数 $\pi$ 与 query 图像的精确 B/F 比例不匹配时，该项有助于避免因 $\mathrm{CE}$ 和 $\mathcal{H}$ 最小化而导致的退化解；而如果能够准确估计 query 图像中的 B/F 比例（即有这个先验知识可供使用时），该项可以大幅提高方法整体的性能。</li></ol><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-figure1.png"></p><p><strong>分类器的选择</strong>：RePRI 的线性分类器和 <a href="http://arxiv.org/abs/1904.04232">Baseline++, ICLR 2019</a> 相似：$s^{(t)}(j)=\operatorname{sigmoid}\left(\tau\left[\cos \left(z(j), w^{(t)}\right)-b^{(t)}\right]\right)$。其中原型 $w^{(0)}$ 是 support 前景特征的均值：$w^{(0)}=\frac{1}{K|\Psi|} \sum_{k=1}^{K} \sum_{j \in \Psi} \widetilde{y}<em>{k}(j)</em>{1} z_{k}(j)$；$b^{(0)}$ 是对 query 前景的 soft predictions的均值 ：$b^{(0)}=\frac{1}{|\Psi|} \sum_{j \in \Psi} p_{\mathcal{Q}}(j)_{1}$。</p><p><strong>B/F 比例 $\pi$ 的联合估计</strong>：当没有先验知识时，RePRI 使用 $\widehat{p}<em>{\mathcal{Q}}$ 来联合学习 $\pi$ 和分类器的参数，这时 $\mathcal{D}</em>{\mathrm{KL}}$ 可以被视为 self-regularization 来防止模型的 marginal distribution 发生偏移。具体实现中，只在初始化后在之后的某一轮迭代 $t_{\pi}$ 更新一次 $\pi$ 即可，即</p><p>$$<br>\pi^{(t)}= \begin{cases}\widehat{p}<em>{\mathcal{Q}}^{(0)} &amp; 0 \leq t \leq t</em>{\pi} \ \hat{p}<em>{\mathcal{Q}}^{\left(t</em>{\pi}\right)} &amp; t&gt;t_{\pi}\end{cases}<br>$$</p><p>实验结果：</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-table1.png"></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/NeurIPS2021-a-good-transductive-inference-is-all-you-need-table2.png"></p><p>可以看到在 5-shot 上 RePRI 取得了全面的 SOTA。本文同样做了包含 (1) 训练时采用 1-shot task、测试时用 1-\5-\10-shot task；(2) cross-domain 等实验。具体请看原论文。</p><h3 id="Self-Guided-and-Cross-Guided-Learning-for-Few-Shot-Segmentation"><a href="#Self-Guided-and-Cross-Guided-Learning-for-Few-Shot-Segmentation" class="headerlink" title="Self-Guided and Cross-Guided Learning for Few-Shot Segmentation"></a>Self-Guided and Cross-Guided Learning for Few-Shot Segmentation</h3><p>CVPR 2021 | <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Self-Guided_and_Cross-Guided_Learning_for_Few-Shot_Segmentation_CVPR_2021_paper.html">link</a> | <a href="https://github.com/zbf1991/SCL">github</a></p><p>本文同样提出常用的 masked Global Average Pooling (GAP) 来将 support 图像转换为特征向量的方式会导致有区分性的信息因为求均值操作而损失。另外，当 shot 数量不为 1 时，常用的对所有 support 向量求均值会强制使得所有的 support 图像贡献相同，而不同的 support 图像的对于表示类别的贡献实际上是不同的。</p><p>为了解决这两个问题，本文提出 **Self-Guided and Cross-Guided Learning (SCL)**，首先用初始原型来对 support 图像做初始预测，预测覆盖和没有覆盖到的前景区域被用 masked GAP 编码成 primary 和 auxiliary support 向量来在 query 图像的分割上取得更好表现。同时，针对 shot 数量不为 1 的场景，本文提出 <strong>Cross-Guided Module (CGM)</strong> 来使用其他有标注 support 图像评估每张 support 图像的预测质量，使高质量的 support 图像能够对最终的融合做出更大的贡献。相比 attention 等复杂的方法，CGM 无需重新训练模型，可以直接在 inference 时被应用来提升最终的表现。SCL 遵循 episodic training 的方式，总损失为 $\mathcal{L}=\mathcal{L}<em>{c e}^{s 1}+\mathcal{L}</em>{c e}^{s 2}+\mathcal{L}_{c e}^{q}$，前两项来自 support set 的 Self-Guided Learning，后一项来自 query set。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure3.png"></p><p><strong>Self-Guided Learning on Support Set</strong>：这里首先以 1-shot 为例。用 masked GAP 获得的初始 support 向量 $\mathcal{v}<em>s$ 被扩展成和 support 特征图 $F_s$ 同样大小的 $V_s$，然后拼接得到新的特征图 $F</em>{sv} = Concat([F_s, V_s, V_s])$。通过将 $F_{sv}$ 输入到 <strong>support FPM</strong> 和 decoder，可以得到support 图像的概率图 $P_{s1} = softmax(\mathcal{D}(FPM_s(F_{sv})))$，其中 $\mathcal{D}(\cdot)$ 指 decoder。由于 SCL 是一个即插即用的模块，因此 support FPM 和 decoder 的设计遵循所选择的小样本分割 baseline。本文的实验部分选择插入到两种 baseline 中：(1) <a href="http://arxiv.org/abs/1903.02351">CANet (CVPR 2019)</a>，其 decoder 是 single-scale 结构的，因此 SCL 也采用 single-scale support FPM；(2) <a href="http://arxiv.org/abs/2008.01449">PFENet (TPAMI 2020)</a>，其 decoder 是 multi-scale 结构的，因此 SCL 也采用 multi-scale support FPM。single-scale 和 multi-scale 的 support FPM 结构如上图右边所示。</p><p>使用预测的 mask $\hat{M_s} = \argmax(P_{s1})$ 和 ground-truth mask $M_s$，可以得到 primary support 向量 $\mathcal{v}<em>{pri}$ 和 auxiliary support 向量 $\mathcal{v}</em>{aux}$，前者代表预测正确的实际前景信息，后者代表预测错误的实际前景信息。换句话说，$\mathcal{v}<em>{pri}$ 保持着主要的 support 信息，而 $\mathcal{v}</em>{aux}$ 包含用 $\mathcal{v}<em>s$ 无法预测的、损失的重要信息。为了保证 $\mathcal{v}</em>{pri}$ 能够从 support 的特征图收集到大多数的信息，对 $P_{s1}$ 施加交叉熵得到 $\mathcal{L}_{c e}^{s 1}$：</p><p>$$<br>\mathcal{L}<em>{c e}^{s 1}=-\frac{1}{h w} \sum</em>{i=1}^{h w} \sum_{c_{j} \in{0,1}}\left[M_{s}(i)=c_{j}\right] \log \left(P_{s 1}^{c_{j}}(i)\right)<br>$$</p><p>接下来，$\mathcal{v}<em>{pri}$ 和 $\mathcal{v}</em>{aux}$ 也被扩展并与 $F_s$ 拼接得到 $F^A_{s} = Concat([F_s, V^{pri}<em>s, V^{aux}<em>s])$，然后得到 $P</em>{s2} = softmax(\mathcal{D}(FPM_s(F^A</em>{s})))$。为了确保集成 $\mathcal{v}<em>{pri}$ 和 $\mathcal{v}</em>{aux}$ 能够得到精准的分割 mask，同样施加交叉熵得到 $\mathcal{L}_{c e}^{s 2}$：</p><p>$$<br>\mathcal{L}<em>{c e}^{s 2}=-\frac{1}{h w} \sum</em>{i=1}^{h w} \sum_{c_{j} \in{0,1}}\left[M_{s}(i)=c_{j}\right] \log \left(P_{s 2}^{c_{j}}(i)\right)<br>$$</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure2.png"></p><p><strong>Training on Query Set</strong>：对于 query 的特征图 $F_q$，流程为 $F^A_{q} = Concat([F_q, V^{pri}<em>q, V^{aux}<em>q])$，其中 $V^{pri}<em>q$ 和 $V^{aux}<em>q$ 是 $\mathcal{v}</em>{pri}$ 和 $\mathcal{v}</em>{aux}$ 扩展大小得到；$P</em>{q} = softmax(\mathcal{D}(FPM_q(F^A</em>{q})))$，注意这里有一个单独的 <strong>query FPM</strong> $FPM_q$ 而非 $FPM_s$。同样计算交叉熵得到 $\mathcal{L}_{c e}^{q}$：</p><p>$$<br>\mathcal{L}<em>{c e}^{q}=-\frac{1}{h w} \sum</em>{i=1}^{h w} \sum_{c_{j} \in{0,1}}\left[M_{q}(i)=c_{j}\right] \log \left(P_{q}^{c_{j}}(i)\right)<br>$$</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-figure5.png"></p><p><strong>Cross-Guided Multiple Shot Learning</strong>：当 shot 数 &gt; 1 时，对于第 k 张 support 图像，首先将其作为 support 图像，将所有 K 张 support 图像作为 query 图像来输入到所提出的面向 1-shot 的模型中。对于第 i 张 support 图像，得到在第 k 张图像的支持下的预测 mask $\hat{M^{i|k}_s}$。因为第 i 张 support 图像的 ground-truth mask $M_s^i$ 是可得的，因此可以用预测和 ground-truth 的 masks 的 IOU 来计算一个置信度：</p><p>$$<br>U_{s}^{k}=\frac{1}{K} \sum_{i=1}^{K} \operatorname{IOU}\left(\hat{M}<em>{s}^{i \mid k}, M</em>{s}^{i}\right)<br>$$</p><p>则对于给定的 query 图像的最终预测 score map：</p><p>$$<br>\hat{P}<em>{q}=\operatorname{softmax}\left(\frac{1}{K} \sum</em>{k=1}^{K} U_{s}^{k} \mathcal{G}\left(I_{q} \mid I_{s}^{k}\right)\right)<br>$$</p><p>可以看到有更大的 $U_{s}^{k}$ 的 support 图像对于最终的预测有更大的贡献。</p><p>实验结果：</p><!-- 所以说设计即插即用的模块就是好，加在 SOTA 上就是新的 SOTA。 --><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-table1.png"></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-self-guided-and-cross-guided-learning-for-FSS-table2.png"></p><h3 id="Adaptive-Prototype-Learning-and-Allocation-for-Few-Shot-Segmentation"><a href="#Adaptive-Prototype-Learning-and-Allocation-for-Few-Shot-Segmentation" class="headerlink" title="Adaptive Prototype Learning and Allocation for Few-Shot Segmentation"></a>Adaptive Prototype Learning and Allocation for Few-Shot Segmentation</h3><p>CVPR 2021 | <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Adaptive_Prototype_Learning_and_Allocation_for_Few-Shot_Segmentation_CVPR_2021_paper.html">link</a> | <a href="https://github.com/Reagan1311/ASGNet">github</a></p><p>本文认为虽然相比需要用稠密 affinity 矩阵来解决欠约束的像素匹配问题而容易过拟合的 affinity learning，常用的 prototypical learning 能够比单纯的像素特征更加鲁棒，但是仅用一个 prototype 不足以表示包含空间信息在内的所有信息。特别地，本文希望能够根据图像内容自适应地调整 prototypes 的数量和空间范围，从而能够更好地处理物体在尺寸和形状上的变化。例如对一个尺寸较小的物体，可能一个或少量 prototypes 就足够了；而对于尺寸较大的物体，可能需要更多的 prototypes 来表示所有重要的信息。</p><p>因此，本文提出 **Adaptive Superpixel-guided Network (ASGNet)**，包含 <strong>superpixel-guided clustering (SGC)</strong> 和 <strong>guided prototype allocation (GPA)</strong> 两个模块用于提取和分配多个 prototypes。SGC 模块在 support 图像上进行快速的、基于特征的 superpixel 提取，得到的 superpixel centroids 可以被视为 prototypical 特征，同时 superpixel 的数量和形状都是适应于图像内容的，因此得到的 prototypes 是自适应的。GPA 模块用一个 attention-like 机制来讲最相关的 support prototype 特征分配给 query 图像中的每个像素。最后，ASGNet 使用 <a href="http://arxiv.org/abs/2008.01449">PFENet (TPAMI 2020)</a> 中的特征增强结构并建立一个 FPN-like top-down 结构来引入 multi-scale 信息。当 shot 数大于 1 时，每张 support 图像都可以得到一组 prototypes，所有的 prototypes 被一块作为 GPA 的选择范围。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure2.png"></p><p><strong>Superpixel-guided Clustering</strong>：受到 <a href="https://arxiv.org/abs/1606.09518">maskSLIC (2016)</a>, <a href="https://arxiv.org/abs/1807.10174">Superpixel sampling networks (ECCV 2018)</a> 等工作的启发，SGC 模块被用于将特征图用聚类的方式集成到多个 superpixel centroids 中。给定 support 特征 $F_s \in \mathbb{R}^{c \times h \times w}$ 和 support mask $M_s \in \mathbb{R}^{h \times w}$，假设已经获得初始 superpixel 种子，SGC 首先将 support 特征图上每个像素的坐标的值进行放缩后和特征图进行拼接，从而引入位置信息。之后用 support mask 来筛除背景信息，这样我们获得 $F_s^{‘} \in \mathbb{R}^{(c+2) \times N_m}$，$N_m$ 是在 support mask 中的像素的数量（注意这里我们对变量及其维度表示和原论文略有差别，我在阅读了源码后觉得这样表述会更清晰）。同样，我们也将初始 superpixel 种子的特征和其值放缩后的坐标拼接，有 $S^{0} \in \mathbb{R}^{(c+2) \times N_{sp}}$（$N_{sp}$ 是 superpixel 的数量。获得初始种子的方法见本节最后一段）。接下来，SGC 通过迭代式的方法更新 superpixel-based prototypes：在第 $t$ 轮迭代，首先计算每个像素 $p$ 和所有 superpixels 的 association map $Q^t$：</p><p>$$<br>Q^t_{pi} = e^{- || F^{‘}_p - S^{t-1}_i ||^2}<br>$$</p><p>接下来，新的 superpixel centroids 被更新为 masked 特征的加权和：</p><p>$$<br>S^t_i = \frac{1}{\sum_pQ^t_{pi}} \sum^{N_m}<em>{p=1} Q^t</em>{pi} F^{‘}_p<br>$$</p><!-- 这和那个基于 E-M 的有什么区别？区别好像是在于 E-M 会更新 base、attention 和 query features，而这个只更新 superpixel centroids --><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure3.png"></p><p>通常 superpixel 算法通过将图像划分为均匀大小的 grid cell 来选取初始种子（i.e., superpixel），但由于只需要从前景区域来初始化种子，本文参考 <a href="https://arxiv.org/abs/1606.09518">maskSLIC (2016)</a> 来迭代式地安置每个初始种子，流程如下图所示。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure5.png"></p><p><strong>Guided Prototype Allocation</strong>：首先计算每个 prototype 和 query 特征每个位置的 cosine 相似度 $C^{x,y}_i$，这个相似度信息被输入到一个双分支结构。<strong>第一个分支</strong>计算每个位置的像素和哪个 prototype 最相似：</p><p>$$<br>G^{x,y} = \argmax_{i \in {0, \dots, N_{sp}}} C^{x,y}_i<br>$$</p><p>根据得到的 guide map $G \in \mathbb{R}^{h \times w}$，可以通过将对应 prototype 放到 guide map 的每个位置得到 guide feature $F_G \in \mathbb{R}^{c \times h \times w}$。<strong>第二个分支</strong>将相似度信息 $C$ 在所有的 superpixels 相加来得到概率图 $P$。最终将 $P$、$F_G$ 和原本的 query 特征 $F_Q$ 拼接并过 $1 \times 1$ 卷积得到 refined query 特征 $F^{‘}_Q$。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-figure4.png"></p><p>实验结果：</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-table5.png"></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-adaptive-prototype-learning-and-allocation-for-FSS-table7.png"></p><h3 id="Few-Shot-3D-Point-Cloud-Semantic-Segmentation"><a href="#Few-Shot-3D-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Few-Shot 3D Point Cloud Semantic Segmentation"></a>Few-Shot 3D Point Cloud Semantic Segmentation</h3><p>CVPR 2021 | <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Few-Shot_3D_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.html">link</a> | <a href="https://github.com/Na-Z/attMPTI">github</a></p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-figure1.png"></p><p>第一篇做小样本 3D 点云语义分割的论文。相比 2D 图像，点云更加无结构和无序，因此做语义分割的难度会更大。任务定义如上图所示，基本就是把 2D 数据换成 3D 点云。方法上，本文提出了 <strong>attention-aware multi-prototype transductive inference</strong> 框架。其包含以下三点：</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-figure2.png"></p><p><strong>embedding network</strong>：对标 2D 视觉中的 CNN 等 backbone 网络。论文提出这个网络需要满足 (1) 能够<strong>根据局部上下文编码几何结构</strong>；(2) 能够<strong>根据全局上下文编码点云的语义信息和它们间的语义关系</strong>；(3) 能够快速适应不同的小样本任务。因此，本文提出一种 attention-aware multi-level feature learning network 来结合局部几何特征、全局语义特征和 metric-adaptive 特征。具体来说，该网络由三部分组成：(1) feature extractor，选用动态图 CNN 架构的 <a href="http://arxiv.org/abs/1801.07829">DGCNN (ACM Trans. Graph. 2019)</a> 来得到局部几何特征（第一个 EdgeConv 层的输出）和语义特征（整个 feature extractor 的输出）；(2) attention learner，选用了 self-attention network (SAN)；(3) metric learner，选用了 MLP 的堆叠并以一个相对更大的学习率更新。</p><!-- embedding 具体怎么运算论文（至少是正文）也没提，需要看代码理解 --><p><strong>multi-prototype generation</strong>：对于 support set 中 $N+1$ 类的每一个，都通过聚类生成 $n$ 个 prototypes。具体地，$n$ 个种子点通过 farthest point sampling 被从 support 点中采样得到。直觉上来说，如果 embedding space 学得够好，这个空间中最远的那些点能够可以内在地表示一个类别的不同视角。之后，我们计算其他点和这些种子点的距离并根据最近邻原则分配，最后计算每个簇的均值作为 prototypes。</p><p><strong>transductive inference</strong>：该方法首先构建了一张包含 $n \times (N+1)$ 个 prototypes 和 $T \times M$ 个 query 点、总计 $V = n \times (N+1) + T \times M$ 个节点的 k-NN graph。稀疏邻接矩阵 $\mathbf{A} \in \mathbb{R}^{V \times V}$ 通过计算每个点和其 $k$ 个最近邻的高斯相似度得到。为了让邻接矩阵是非负且对称的，有 $\mathbf{W} = \mathbf{A} + \mathbf{A}^{T}$，并进一步进行正则化有 $\mathbf{S} = \mathbf{D}^{-1/2}\mathbf{W}\mathbf{D}^{-1/2}$。同时定义标签矩阵 $\mathbf{Y} \in \mathbb{R}^{V \times (N+1)}$，其中对应有标签 prototypes 的行是 one-hot ground-truth 标签，其余为零向量。给定 $\mathbf{S}$ 和 $\mathbf{Y}$，标签传播有解析解 $\mathbf{Z}^{*} = (\mathbf{I} - \alpha \mathbf{S})^{-1}\mathbf{Y}$。最后每个点云对应的预测通过 softmax 后用交叉熵计算损失。</p><p>数值的实验结果如下表所示。原论文还有一些 ablation study 和分割结果的可视化。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CVPR2021-FS-3D-point-cloud-semantic-segmentation-table.png"></p><!-- ### Zero-Shot Instance SegmentationCVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Zero-Shot_Instance_Segmentation_CVPR_2021_paper.html) | [github](https://github.com/zhengye1995/Zero-shot-Instance-Segmentation)这篇论文写的有点烂...有些细节好模糊 --><!-- ### Anti-Aliasing Semantic Reconstruction for Few-Shot Semantic SegmentationCVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Anti-Aliasing_Semantic_Reconstruction_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.html) | [github（代码不完整）](https://github.com/Bibkiller/ASR) --><!-- ### Scale-Aware Graph Neural Network for Few-Shot Semantic SegmentationCVPR 2021 | [link](https://openaccess.thecvf.com/content/CVPR2021/html/Xie_Scale-Aware_Graph_Neural_Network_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.html) | 代码未开源 --><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;根据手头想法的需要，读一读 2021 年顶会顶刊的小样本分割相关论文并做笔记于此。有开源代码的论文优先，持续更新。&lt;/p&gt;
&lt;!-- [参考](https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers) --&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="few-shot segmentation" scheme="https://kyonhuang.top/blog/tags/few-shot-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】ICLR 2021 论文选读</title>
    <link href="https://kyonhuang.top/blog/ICLR2021-notes/"/>
    <id>https://kyonhuang.top/blog/ICLR2021-notes/</id>
    <published>2021-07-16T02:52:22.000Z</published>
    <updated>2021-10-13T03:29:33.388Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博文简单记录一下我快速阅读 ICLR 2021 其中一些<strong>我感兴趣</strong>的论文时的笔记和感想等。由于论文数量比较多，因此可能后续还会更新。接收列表见 <a href="https://www.paperdigest.org/2021/01/iclr-2021-highlights/">Paper Digest: ICLR 2021 Highlights</a>。</p><p>我比较关注的 topic 包括 transfer learning 下的 few-shot learning、domain adaptation、domain generalization 等，以及包括 VQA、visual grounding 在内的一些多模态学习的任务。我用 “#topic” 来标明这篇论文所属的 topic，这样既不用生硬地将属于多个 topic 的论文强行归类到单一 topic 下，也方便各位读者在页面内用 CTRL+F 来搜索自己感兴趣的 topic。</p><span id="more"></span><h2 id="Few-shot-Learning"><a href="#Few-shot-Learning" class="headerlink" title="Few-shot Learning"></a>Few-shot Learning</h2><ul><li><a href="https://openreview.net/forum?id=oZIvHV04XgC">Wandering Within A World: Online Contextualized Few-shot Learning</a><br>#continual learning  #lifelong learning</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-online-contextualized-few-shot-learning.png"></p><p>本文提出了一种新的学习范式 online contextualized few-shot learning，要求模型考虑上下文切换来进行在线的小样本学习。同时，本文提出 contextual prototypical memory（CPM）模型，通过集成 RNN 来编码上下文信息，并且有一个独立的原型存储（prototype memory）来记住已经学过的类别，在所有原型半径外的样本会被预测为新类。该模型在本文构建的三个数据集上能够比扩展后的经典 FSL 算法表现更好。</p><ul><li><a href="https://openreview.net/forum?id=JWOiYxMG92s">Free Lunch for Few-shot Learning: Distribution Calibration</a><br>#data augmentation</li></ul><p>对于每个 support sample，寻找 base classes 中原型与其最相似的 k 个类别，用这些类的均值和协方差来同该 support sample 一起得到新的均值和协方差作为分布的参数，从而通过从该分布中采样来进行特征级别的数据增强。</p><ul><li><a href="https://openreview.net/forum?id=04cII6MumYV">A Universal Representation Transformer Layer for Few-Shot Image Classification</a><br>#cross-domain  #self-attention  #metric-based</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-universal-representation-transformer.png"></p><p>本文提出了多头的泛用表征转换层（multi-head universal representation transformer layer），在 Meta-Dataset 上表现良好。在单头 URT 层中，对于每个域有一个预训练好的特征提取器，模型对各类原型进行自注意力计算。query 是每个类 $c$ 的原型经线性层的产物 $q_c$，key 是每个域 $i$ 的特征提取器得到的每个类 $c$ 的原型经线性层的产物 $k_{i, c}$。计算得到的权重求平均得到每个域的权重，和对应特征提取器的特征得到调整后的表征。多头 URT 层将单头 URT 层得到的表征拼接，以用于原型分类等。</p><ul><li><a href="https://openreview.net/forum?id=eJIJF3-LoZO">Concept Learners for Few-Shot Learning</a><br>#compositionality  #auxiliary semantic  #metric-based</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-concept-learners-for-FSL.png"></p><p>本文使用了额外的辅助模态，以向量的形式来表现图像中包含的概念。每个概念有一个对应的特征提取器（即卷积骨干网络，实验中分为参数共享和不共享两种），概念向量与样本进行点积后过该特征提取器来得到概念特定的特征，最后进行原型分类的距离就是查询样本的概念特定特征和支持样本的概念特定原型间的欧氏距离对于所有概念的总和。这些概念也可以通过已有的 landmark discovery 方法来无监督式地得到。</p><p>提供了特征提取器共享参数的实验结果可以部分消除对参数量大幅增加的质疑，但是在提供有监督概念时只和只用视觉信息的 baselines 比较，我认为是不公平的。</p><ul><li><a href="https://openreview.net/forum?id=xzqLpqRzxLq">IEPT: Instance-Level and Episode-Level Pretext Tasks for Few-Shot Learning</a><br>#self-supervised</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-instance-level-and-episode-level-pretext-tasks.png"></p><p>引入旋转预测的自监督任务后，该方法对于每个旋转角度将 episode 内所有支持和查询样本进行对应旋转来得到一个扩展任务。总 loss 由四部分组成：(1) 预测每个旋转后的样本的旋转角度的 loss；(2) 在每个旋转角度对应的扩展任务中，查询样本分类的 loss；(3) 跨所有扩展任务中，同一查询样本对应的预测概率分布的一致性 loss，具体做法是将该样本的各扩展任务中的预测概率分布与所有扩展任务的预测概率分布均值求 KL 散度；(4) 将同一样本在不同旋转角度后得到的特征视为同一序列中的不同 token，进行自注意力计算后拼接来用于分类得到 loss。</p><ul><li><a href="https://openreview.net/forum?id=vujTf_I8Kmc">Constellation Nets for Few-Shot Learning</a><br>#metric-based  #self-attention  #compositionality  #clustering</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-ConstellationNet.png"></p><p>本文将特征图上每个位置的局部特征 $u \in R^C$ 进行软聚类，将所有局部特征到所有聚类中心的距离组织为一张距离图 $D \in R^{B \times H \times W \times K}$，之后再将 $D$ 和一个位置编码 $P \in R^{B \times H \times W \times K}$ 相加来作为 query 和 key、$D$ 作为 value 进行多头自注意力运算。上述运算结果和特征图拼接后过 1x1 卷积，从而在骨干网络的每个卷积层后无缝接入 constellation 模型，以结合隐式和显式的 part-based representations。</p><ul><li><a href="https://openreview.net/forum?id=9z_dNsC4B5t">MetaNorm: Learning to Normalize Few-Shot Batches Across Domains</a><br>#meta-learning  #normalization</li></ul><p>本文认为当 batch size 较小或者 distribution shift 较大（训练集的统计数据不适用于测试集）时，batch 的统计数据是不可靠的。因此本文提出一种基于原学习的 batch normaliztion 方法，旨在利用一个神经网络来根据输入的样本生成统计数据来最小化不同 domain 或者 support 和 query 间的 KL 散度。该方法在小样本分类、domain generalization 以及提出的 few-shot domain generalization（meta-train 和 meta-test 每个 episode 中类别不同，每个 episode 内部 support 和 query 的 domain 不同）等任务上普遍优于现有的 normalization 方法。</p><h2 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h2><ul><li><a href="https://openreview.net/forum?id=XOjv2HxIF6i">Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models</a><br>#unsupervised learning  #generative adversarial networks </li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-latent-space-interpolation-unsupervised-meta-learning.png"></p><p>现有的无监督元学习方法主要依靠包含聚类、数据增强、随机采样等方法来生成合成任务，这些方法的缺点是根据领域的不同，它们需要较大的调整。本文提出使用现成的、预训练好的生成模型，通过在隐式空间在进行插值来生成合成任务。该方法的优点在于基本不需要根据特定领域进行调整，仅有的一些超参数调整也具有可理解性。</p><h2 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h2><ul><li><a href="https://openreview.net/forum?id=TgSVWXw22FQ">Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning</a></li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-zero-shot-voice-style-transfer.png"></p><p>本文针对在不重新训练模型的情况下迁移到训练期间不可见的发声者风格的任务，认为现有的基于预训练分割编码器或者 AdaIN 的方法都无法较好地分离风格和内容编码。因此，本文最小化风格和内容编码间基于样本的互信息上界，并且最大化两个新的多组互信息下界来增强编码的表示能力。</p><h2 id="DA-DG"><a href="#DA-DG" class="headerlink" title="DA / DG"></a>DA / DG</h2><ul><li><a href="https://openreview.net/forum?id=b9PoimzZFJ">Systematic generalisation with group invariant predictions</a><br>#systematic generalisation  #invariance penalty  #semantic anomaly detection  #domain generalization</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-systematic-generalisation-with-group-invariant-predictions.png"></p><p>本文提出，数据集通常存在和目标变量相关的简单特征（但不健壮）以及更加复杂但是健壮的特征。在三个合成的数据集 coloured-MNIST、COCO-on-colours 和 COCO-on-places 上的实验结果表明，用一个 soft-partition predicting network 来将数据集划分为 majority and minority groups，并且对不同 group 同类样本的特征施加不变性惩罚（例如 KL 散度）有助于模型关注这些更加健壮的特征。</p><p>Spotlight。本文对不同类型的 shift 的划分还是挺有意思的，感觉和一些在视觉上做因果学习的工作在思路上有共通之处。方法上比较简单，可能这就是大巧不工吧。然后我比较关心的问题包括能否在真实数据集上使用这个方法，效果如何？Reviewer 3 也提出了相同的问题，举例问能否在 ImageNet 上找到一个合适的划分。作者的回答是像 ImageNet 这种数据来源丰富、相对杂乱的数据集可能难度比较大，而像医学图像这种数据来源相对单一的就要好一些。</p><ul><li><a href="https://openreview.net/forum?id=lQdXeXDoWtI">In Search of Lost Domain Generalization</a><br>#domain generalization  #benchmark</li></ul><p>本文为 DG 实现了一个名为 DomainBed 的 testbed，包含 7 个数据集、14 种算法和 3 种模型选择策略。结论是 ERM 仍然是一个非常有竞争力的 baseline。</p><p>在我看来这篇论文的一大贡献是讨论了不同模型选择策略带来的影响，包括 training-domain validation（将训练数据划分为 train 和 val set）、leave-one-domain-out validation（给定多个训练域，每次将一个域作为 val，将平均准确率作为指标选择出超参后再在所有训练域上重新训练模型）、test-domain validation（允许用非常有限的测试数据来挑选模型。注意这通常不是一个合理的方法）。另一大贡献自然就是这个还挺大的实验规模，以及给后续工作留下一个还不错的 testbed。</p><ul><li><a href="https://openreview.net/forum?id=6xHJ37MVxxp">Domain Generalization with MixStyle</a><br>#domain generalization</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-MixStyle.png"></p><p>本文受 instance normalization（IN）和 adaptive instance normalization（AdaIN）启发，提出在将样本进行分 domain 打乱或者随机打乱后，用一个随机的权重来混合两个样本的特征统计信息，从而模拟新风格来约束 CNN 的训练。该方法在分类、检索和 RL 上都取得了效果。</p><ul><li><a href="https://openreview.net/forum?id=BVSM0x3EDK6">Robust and Generalizable Visual Representation Learning via Random Convolutions</a><br>#domain generalization  #data augmentation</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-randconv.png"></p><p>本文旨在从单源域数据学习和局部纹理和颜色无关的健壮视觉表征，因此提出一种数据增强技术 RandConv，用随机得卷积来生成有随机纹理但是保留全局形状的图像来提升泛化性。</p><h2 id="VQA"><a href="#VQA" class="headerlink" title="VQA"></a>VQA</h2><ul><li><a href="https://openreview.net/forum?id=Pd_oMxH8IlF">Iterated learning for emergent systematicity in VQA</a><br>#visual question-answering  #modular networks  #iterated learning  #compositional generalization</li></ul><p>神经模块网络（neural module networks, NMN）通常需要一个程序生成器（program generator）来生成符号化的程序将模块组织成计算图。然而，生成的程序很难组合式泛化。本文提出用迭式学习（iterated learning, IL）的方式，将 NMN 的程序看作来自“布局语言”的样本。在本文提出的 SHAPES-SyGet 数据集以及 CLEVR、CLOSURE 上，本方法可以用 1/10 甚至更少的程序标注与 SOTA 相媲美。 </p><p>Oral。评审的意见主要集中在实验选用的数据集比较 toyish。之后作者补做了在 GQA 数据集上的实验， Vector-NMN+IL 用 4000 个程序标注能够和 Vector-NMN 用 943000 个程序标注的效果持平。</p><!-- 【接下来可以把算法部分2页看一下】[paper](https://openreview.net/pdf?id=Pd_oMxH8IlF) --><h2 id="RL"><a href="#RL" class="headerlink" title="RL"></a>RL</h2><ul><li><a href="https://openreview.net/forum?id=Y87Ri-GNHYu">Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning</a><br>#reinforcement learning  #natural language  #imitation learning</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-ask-your-humans.png"></p><p>本文引入了一个提供自然语言指令和动作轨迹作为人类演示，一个类似 Minecraft 的 5x5 gridworld 数据集，包括 35 个任务的 6000 条指令。提出的新模型 (1) 在给定状态和目标时，学习为要执行的高级子任务生成自然语言描述；(2) 学习语言控制的低级策略来实际执行这些步骤。模型首先通过模仿学习进行预训练，之后用 RL 的 PPO 算法和稀疏 reward 进行微调（微调过程不使用 ground-truth 语言指令）。实验证明该模型能够在 zero-shot 设置下泛化到不可见任务中生成的自然语言指令，并且提供了可解释的行为。</p><ul><li><a href="https://openreview.net/forum?id=9SS69KwomAM">Solving Compositional Reinforcement Learning Problems via Task Reduction</a><br>#reinforcement learning  #imitation learning</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-self-imitation-via-reduction.png"></p><p>本文提出一种新的学习范式 Self-Imitation via Reduction (SIR) 来解决有稀疏 reward 的组合性持续控制问题。该范式首先将较难的任务简化为解决方案已知的简单任务（称为任务缩减），然后在获得的解决方案的轨迹上执行模仿学习，从而加速训练，同时无需显式确定低级技能或选项就可向学到的策略引入组合性的偏执归纳。上面的示意图是一个任务缩减的具体例子。另外，SIR 可以将学到地任务作为新的缩减目标，从而循环式地学习任意复杂的策略。SIR 通过共同学习一个 goal-conditioned policy 和一个 universal value function 来实现，因此任务缩减可以通过对价值函数的状态搜索和不同目标的策略执行来完成。</p><p>作为一个 RL 门外汉，我觉得 intution 还是挺有意思的，不过由于我没细看，我比较好奇这种任务缩减如果没有解决方案已知的简单任务的话怎么办呢？实验是是否需要假设较难的任务都存在解决方案已知的简单任务？（我看 program chair 的最终意见也有差不多的观点，即本文关注的是如何将问题分解为子问题，而在设计任务空间时确实包含了一定先验知识，即并不是学来的。）</p><ul><li><a href="https://openreview.net/forum?id=vcopnwZ7bC">Learning Task Decomposition with Ordered Memory Policy Network</a><br>#reinforcement learning  #hierarchical imitation learning</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-ordered-memory-policy-network.png"></p><p>本文提出 Ordered Memory Policy Network (OMPN) ，能够通过从演示中学习来探索子任务的层级结构，从而在监督和弱监督设置下利用非结构化的演示实现任务分解来复用学过的技能。该方法将子任务视为有限状态机（finite state machine），它们被表示为通过自顶向下和自底向上循环更新的内存库。</p><p>看 Chap 2.2 开头，还是需要在 detection 阶段让用户给定子任务数量以及动作空间，以生成 task boundaries。</p><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul><li><a href="https://openreview.net/forum?id=PS3IMnScugk">Learning to Recombine and Resample Data For Compositional Generalization</a><br>#compositional generalization  #data augmentation  #natural language processing  #few-shot morphology learning</li></ul><p>本文关注小样本时态学习问题，旨在从单词形式预测包括第三人称、单数、现在时等各种语言特征。以前的工作集中于例如符号语法建模数据或应用基于规则的数据扩增等方法，这些方法很难在没有显式规则的归纳偏置下进行泛化。本文提出一种新的基于原型的神经序列模型，通过显式重组训练示例片段来重构其他输入-输出对，并对模型的输出进行重采样来选择稀有现象的高质量合成样本，从而进行样本扩增。</p><h2 id="Continual-Learning"><a href="#Continual-Learning" class="headerlink" title="Continual Learning"></a>Continual Learning</h2><ul><li><a href="https://openreview.net/forum?id=ADWd4TJO13G">Lifelong Learning of Compositional Structures</a><br>#lifelong learning  #continual learning  #modular networks</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ICLR21-lifelong-compositional-learning.png"></p><p>本文将组合学习和终身学习结合，提出一个框架来分阶段地学习如何最佳地组合已有组件来理解新任务，以及如何调整现有组件（以及选择增加新组件）来适应新问题。3x3=9 种组合被用于说明该框架，其中知识保留机制包括简单微调、经验回放（experience replay）、弹性权重整合（elastic weight consolidation）；组合性结构包括线性模型的线性组合、软性层排序（soft layer ordering）、门网络的一个软版本（a soft version of gating networks）。八个数据集上的实验结果表明将终身学习过程划分为两个阶段能够减少灾难性遗忘。</p><h2 id="generative-models"><a href="#generative-models" class="headerlink" title="generative models"></a>generative models</h2><ul><li><a href="https://openreview.net/forum?id=sjuuTm4vj0">Using latent space regression to analyze and leverage compositionality in GANs</a><br>#image synthesis  #generative adversarial networks  #image editing</li></ul><p>本文使用一个回归器（regressor）来根据属性预测给定图像的隐式编码，从而分析和修改预训练 GAN 的隐式空间。另外编码器的输入可以加上一个 binary mask 来学习在图像不完整的情况进行重构。该方法在图像组合、属性修改、图像补齐和多模态编辑上等应用上都有良好的表现。</p><p>我比较感兴趣的是该方法声称能够在图像组合时处理全局不一致和像素缺失的问题；另外研究预训练 GAN 对生成图像的局部进行混合和匹配的能力，而非直接提出新的 GAN 模型的思路也挺有意思（NLP 里很多工作已经是完全建立在预训练模型上进行研究了，当然我不太清楚这种思路是不是也在 GAN 这里成为主流了）。我对引用中 <a href="https://openreview.net/forum?id=SJxDDpEKvH">Besserve et al., 2018</a> 有一点兴趣，不过不一定有时间去读。</p><ul><li><a href="https://openreview.net/forum?id=qbH974jKUVy">The role of Disentanglement in Generalisation</a><br>#disentanglement  #compositionality  #variational autoencoders</li></ul><p>本文旨在研究神经网络是否能够利用解纠缠表征来支持组合性泛化和外推。研究建立在两个图像数据集（dSprites 和 3DShape）上，模型包含有不同解纠缠压力的 β-VAEs 和 FactorVAEs，以及去掉编码器的解码器（直接输入解纠缠隐编码）。本文的结论是学习非纠缠表征确实提高了可解释性和样本效率，但这些模型只支持最简单的组合泛化，并且解纠缠的程度对于泛化的程度没有影响。</p><!-- 【还没仔细看评审意见】 --><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这篇博文简单记录一下我快速阅读 ICLR 2021 其中一些&lt;strong&gt;我感兴趣&lt;/strong&gt;的论文时的笔记和感想等。由于论文数量比较多，因此可能后续还会更新。接收列表见 &lt;a href=&quot;https://www.paperdigest.org/2021/01/iclr-2021-highlights/&quot;&gt;Paper Digest: ICLR 2021 Highlights&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我比较关注的 topic 包括 transfer learning 下的 few-shot learning、domain adaptation、domain generalization 等，以及包括 VQA、visual grounding 在内的一些多模态学习的任务。我用 “#topic” 来标明这篇论文所属的 topic，这样既不用生硬地将属于多个 topic 的论文强行归类到单一 topic 下，也方便各位读者在页面内用 CTRL+F 来搜索自己感兴趣的 topic。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="ICLR 2021" scheme="https://kyonhuang.top/blog/tags/ICLR-2021/"/>
    
  </entry>
  
  <entry>
    <title>IJCAI2020 教训总结</title>
    <link href="https://kyonhuang.top/blog/my-summary-of-IJCAI20/"/>
    <id>https://kyonhuang.top/blog/my-summary-of-IJCAI20/</id>
    <published>2020-04-23T08:34:27.000Z</published>
    <updated>2020-04-23T08:46:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>很久没有写博客了，恰逢 IJCAI2020 前两天公布了录取结果。去年的我还在时间序列分析上捣鼓着我的毕业设计，今年的我在小样本学习上谋求一个落脚点，变的是年纪、是阅历、是心态，不变的是来自 IJCAI 的 reject。相比于去年，也许是因为 IJCAI 的审稿质量有了显著提升（也许也归功于他们在 desk reject 时大手一挥斩掉了将近一半的论文），也许是因为我自己在方法、实验设计和论文写作方面的进步使得评审能够更好地理解论文，总之这次的论文得以收获更加一针见血的点评，对我改进这份工作以及思考未来前进的方向有着不小的帮助。因此，我也想简单地总结一下评审们给出的意见，也是和博客的读者朋友们讨论一下（虽然可能这种讨论是单向的）身为科研菜鸟如何能够完善每一份工作。考虑到这篇论文还要修改后再投，这里把一些论文内容的相关信息模糊，只宏观地分析一下存在的问题。</p><p>这次的四名评审给我的论文的评分是 1 个 accept，1 个 weak accept，2 个 reject。给 accept 的那一位评审简单地写了两行夸赞之词，并表示对于如何改进这份工作没有任何建议。在感谢这位评审的认同后，从另外三位评审的评语（为了方便，后文中称为 review）以及 SPC（在这四位评审之上决定是否录用的人）的评语（后文中称为 meta-review）中，我感觉体现了以下问题：</p><ul><li><p>文献调研不够充分。在做相关工作的文献调研时，我漏掉了一些论文，使得我在论文中错误地评估了这份工作对领域的贡献。这也是 SPC 给出的两条拒稿的主要理由之一。这给我的教训是在做文献调研的工作时，第一是不能仅用关键字的方式去搜查相关工作（当然这是主要方法之一），还应该顺着那些最相关的工作的引用和被引链条上下溯源，来找到那些在题目和摘要中不易看出但实际非常相关的工作；第二是要及时地去“扫荡”正好处于你实验和写作过程中发布的论文，尤其是被认为是正式发表的会议论文。这次我就吃了亏，有一篇论文在我投稿前不久刚刚发表，逃过了我的搜查。之后要及时快速阅读新出的会议论文，并做好归类工作；第三是要培养自己在看论文时的专注力，以及能够在短时间内评估论文贡献、和手头工作是否密切相关、以及对自己是否有进一步阅读价值的能力。另外，在平时不着急赶论文时，也要多读论文多思考，从广度和深度两个角度加深自己对领域的理解，这样在写论文的相关工作时才能更有的放矢。</p></li><li><p>故事讲得不够好。尽管人工智能领域仍然发展迅速，每周都会有开创性的工作面世，但绝大多数会议收录的工作都建立在已有工作的基础上，做了一定程度的改进。而信息的传播是有噪音的，这意味着不是所有人都能够正确地理解你的工作以及评估你的创新与贡献。论文是研究工作最主要的载体，想要让自己的改进在所有评审的眼里都能够达到会议要求的标准线，就需要在论文中对工作的每个细节有更加清晰、连贯的表述。这次有几位评审都对模型中的同一个细节提出了疑问，这是因为我在表述时没能够考虑到这里虽然是引用了已有的模型，但仍可能会有理解上的门槛。另外，也有评审对我的方法的实验设置有一些质疑，认为其不公平、没有实际应用价值。这样负面且充满误解的评审意见其实是能够在写作的过程中通过更详细的举例等说明来消除的。因此，在之后的写作过程中，一是要把自己的视角放的更低来审视自己的表述，二是要邀请实验室的一些同学来严格地审阅自己写好的论文，请他们从自己的角度积极地提出意见。</p></li><li><p>选择合适的会议，或者根据不同的会议来调整自己的表述。投稿经验丰富的研究者在自己心中对于经常打交道的会议期刊应该都有这方面的经验了，但我是第一次看到有评审在评审意见中提出 IJCAI 不应该过多接收纯计算机视觉的论文，因此综合考虑决定给出 reject。尽管我不认为我是做纯计算机视觉的，但我后来发现我在投稿时因为没找到最合适的 topic，因此 primary subject area 选的是 computer vision 下的子 topic 了…为自己的工作选择最合适的会议，既是增加一份被接收的希望，也让更多同领域的研究者有机会看到自己的工作，增加自己被引用和声名大振的概率。</p></li></ul><p>如果每一份工作都是一面镜子，清晰地反映着你的不足，那这次 IJCAI 的经历无疑是最好的清洁剂，让这面镜子更加明亮。尽管一年来我收获了很多，但是以成果论的话，进步的速度还远远不够。除开重新投递这份工作，目前我手头还有其他的工作也将要投交。希望自己能够吸取教训，在把握宏观前进方向的同时也做好每一个细节，来更快地到达目的地。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;很久没有写博客了，恰逢 IJCAI2020 前两天公布了录取结果。去年的我还在时间序列分析上捣鼓着我的毕业设计，今年的我在小样本学习上谋求一个落脚点，变的是年纪、是阅历、是心态，不变的是来自 IJCAI 的 reject。相比于去年，也许是因为 IJCAI 的审稿质量有了显</summary>
      
    
    
    
    <category term="翻滚吧博士生" scheme="https://kyonhuang.top/blog/categories/%E7%BF%BB%E6%BB%9A%E5%90%A7%E5%8D%9A%E5%A3%AB%E7%94%9F/"/>
    
    
    <category term="总结" scheme="https://kyonhuang.top/blog/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】Prototypical Networks for Few-shot Learning</title>
    <link href="https://kyonhuang.top/blog/ProtoNet-notes/"/>
    <id>https://kyonhuang.top/blog/ProtoNet-notes/</id>
    <published>2019-11-26T12:02:18.000Z</published>
    <updated>2019-11-27T01:31:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/ProtoNet-paper-title.png"></p><blockquote><p>一直没有完整看过这篇论文。这两天在复现 Prototypical Networks 时发现自己对 metric-based few-shot learning 的认知上存在一些问题，于是决定把这篇经典论文拿出来好好读一遍。</p></blockquote><h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>本文提出了原型网络（Prototypical Networks），通过将每个类别的样本求均值得到每个类的原型表示（prototypical representation），简化了 n-shot 分类时 n &gt; 1 的情况，并可以将最近邻分类器成功应用在小样本分类问题上。基于度量学习的小样本学习方法因为这篇经典论文的出世从此自成一派。</p><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><ul><li>作者：Jake Snell, Kevin Swersky, Richard S. Zemel</li><li>出处：NIPS 2017</li><li>机构：Twitter; University of Toronto</li><li>关键词：few-shot learning, metric learning</li><li><a href="https://arxiv.org/pdf/1703.05175.pdf">论文链接</a></li><li>开源代码：<ul><li><a href="https://github.com/jakesnell/prototypical-networks">jakesnell/prototypical-networks</a></li><li><a href="https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch">orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch</a></li></ul></li><li>其他资料：<ul><li><a href="https://www.semanticscholar.org/paper/Prototypical-Networks-for-Few-shot-Learning-Snell-Swersky/c269858a7bb34e8350f2442ccf37797856ae9bca">Semantic Scholar</a></li></ul></li></ul><span id="more"></span><h2 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h2><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://d3i71xaburhd42.cloudfront.net/c269858a7bb34e8350f2442ccf37797856ae9bca/2-Figure1-1.png"></p><p>考虑到小样本分类问题上数据量受到限制，分类器应该有一个非常简单的归纳偏执（根据 [1]，我认为是因为这样可以减小所需要的样本复杂性）。<strong>原型网络（Prototypical Networks）</strong>基于“存在一个嵌入空间，使得对于每一类存在一个原型表征，该类中的所有点都围绕着这个原型表征”的假设。在实现上，使用一个神经网络来实现输入到嵌入空间的非线性映射，每类的原型选择嵌入空间中该类的支持集的均值。而对于新的查询样本的分类通过简单地查找最近邻类别原型来实现。</p><p>零样本学习问题中，新的（或者说训练阶段不可见的）类别不再包含少量的有标签数据，而是每一类会提供一些元数据（或者称为辅助信息），例如属性、描述等。在实现上，即是每一类有一个元数据向量。因此，对于零样本学习，Prototypical Network 学习将元数据嵌入到一个共享空间中来作为类别原型。也就是说，类别原型和查询原型不来自同一个域。</p><h3 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h3><h4 id="作为混合密度估计"><a href="#作为混合密度估计" class="headerlink" title="作为混合密度估计"></a>作为混合密度估计</h4><p>Prototypical Network 的学习过程可以理解为混合概率估计。Bregman 散度是一类特别的距离度量，包含欧式距离和 Mahalanobis 距离。采用 Bregman 散度时，聚类中心即是整个簇最具代表性的点（即质心），使得该类的所有点到质心的总距离之和最小。因此，Prototypical Network 使用类均值作为原型表示，并采用欧氏距离度量。而对于 Matching Network [2] 采用的余弦距离，满足满足和簇的其他点之间总距离最小的质心是使信息损失最小化的点。</p><h4 id="作为线性模型进行解释"><a href="#作为线性模型进行解释" class="headerlink" title="作为线性模型进行解释"></a>作为线性模型进行解释</h4><p>当使用欧氏距离时 $d\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=\left|\mathbf{z}-\mathbf{z}^{\prime}\right|^{2}$，softmax 的内部可以相当于有特定参数的线性模型。具体来说，将欧式距离展开得到</p><p>$$<br>-\left|f_{\phi}(\mathbf{x})-\mathbf{c}_{k}\right|^{2}=-f_{\phi}(\mathbf{x})^{\top} f_{\phi}(\mathbf{x})+2 \mathbf{c}_{k}^{\top} f_{\phi}(\mathbf{x})-\mathbf{c}_{k}^{\top} \mathbf{c}_{k}<br>$$</p><p>其中，第一项对于类别 k 来说是常量，不影响 softmax 的概率结果；而后两项可以写为</p><p>$$<br>2 \mathbf{c}_{k}^{\top} f_{\phi}(\mathbf{x})-\mathbf{c}_{k}^{\top} \mathbf{c}_{k}=\mathbf{w}_{k}^{\top} f_{\phi}(\mathbf{x})+b_{k}, \text { where } \mathbf{w}_{k}=2 \mathbf{c}_{k} \text { and } b_{k}=-\mathbf{c}_{k}^{\top} \mathbf{c}_{k}<br>$$</p><p>因此变为一个线性模型。</p><p>本文猜想，嵌入函数内部已经包含所需要的非线性转换，因此可以直接使用欧氏距离，使得方法更加简单有效。</p><h4 id="与-Matching-Network-的比较"><a href="#与-Matching-Network-的比较" class="headerlink" title="与 Matching Network 的比较"></a>与 Matching Network 的比较</h4><p>Matching Network 在给定支持集的情况下生成一个加权最近邻分类器，而 Prototypical Network 使用欧氏距离生成一个线性分类器。当支持集中每个类只有一个样本时，二者等价。Matching Network 有相对复杂的结构，而 Prototypical Network 采用简单的设计达到相当甚至更好的效果。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h4><p>在小样本分类问题上，本文在 Omniglot 数据集上的结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/c269858a7bb34e8350f2442ccf37797856ae9bca/6-Table1-1.png"></p><p>在 miniImageNet 数据集上的结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/c269858a7bb34e8350f2442ccf37797856ae9bca/6-Table2-1.png"></p><p>在零样本分类问题上，本文在 CUB 数据集上的结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/c269858a7bb34e8350f2442ccf37797856ae9bca/7-Table3-1.png"></p><p>另外有一些有趣的结论。下图的实验结果证明：</p><ol><li>当训练阶段在一个任务中使用更多的类别数时，测试进行 5-way 分类的结果更好；</li><li>比起 Matching Network 采用的余弦距离，欧式距离在 Prototypical Network 上的效果更好。原因已在前文进行解释。</li></ol><p><img src="https://d3i71xaburhd42.cloudfront.net/c269858a7bb34e8350f2442ccf37797856ae9bca/7-Figure3-1.png"></p><h3 id="个人笔记"><a href="#个人笔记" class="headerlink" title="个人笔记"></a>个人笔记</h3><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>简单聊一聊实现上的事情。我之前以为，在 metric-based few-shot learning 中，loss 是从 meta-train 的 training data 上得到的，因此。这是一个没实现过小样本学习方法，只凭借监督问题上的经验容易产生的误区。在小样本学习中，要抛弃掉 training data 和 test data 的概念，用支持集（support set）和查询集（query set）来代表会更好地理解。模型训练阶段的 loss 实际上是由 meta-train 的 query set 得到的，因此，实际上 Prototypical Network 在 meta-test 阶段也不存在微调了，直接用 query prototype 来进行最近邻分类得到准确率。</p><p>有一个问题是，零样本学习任务中，本文提出可用固定原型embedding的长度为单位长度，对query embedding不限制。没有很看懂这个设置，这个反映到代码里是怎么实现的？但是源码中没有给出零样本学习部分的代码，有点难受。</p><!-- 还有一个问题是，发现虽然小样本分类的论文绝大部分都在同样的几个数据集（以 Omniglot，miniImageNet，tiredImageNet，CUB 为主）上进行评估，但是目前还没有一套固定的预处理和训练优化器和超参数设置。因此，想要直接使用很多篇论文的实验结果进行比较（而不用自己实现的实验结果）有些困难。当然这也是正常情况了，就是需要自己找一套能够说服评审的设置了。--><h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ul><li>[1] Generalizing from a Few Examples: A Survey on Few-Shot Learning, arXiv:1904.05046v2.</li><li>[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pages 3630–3638, 2016.</li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/ProtoNet-paper-title.png&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一直没有完整看过这篇论文。这两天在复现 Prototypical Networks 时发现自己对 metric-based few-shot learning 的认知上存在一些问题，于是决定把这篇经典论文拿出来好好读一遍。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;一句话总结&quot;&gt;&lt;a href=&quot;#一句话总结&quot; class=&quot;headerlink&quot; title=&quot;一句话总结&quot;&gt;&lt;/a&gt;一句话总结&lt;/h2&gt;&lt;p&gt;本文提出了原型网络（Prototypical Networks），通过将每个类别的样本求均值得到每个类的原型表示（prototypical representation），简化了 n-shot 分类时 n &amp;gt; 1 的情况，并可以将最近邻分类器成功应用在小样本分类问题上。基于度量学习的小样本学习方法因为这篇经典论文的出世从此自成一派。&lt;/p&gt;
&lt;h2 id=&quot;论文信息&quot;&gt;&lt;a href=&quot;#论文信息&quot; class=&quot;headerlink&quot; title=&quot;论文信息&quot;&gt;&lt;/a&gt;论文信息&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;作者：Jake Snell, Kevin Swersky, Richard S. Zemel&lt;/li&gt;
&lt;li&gt;出处：NIPS 2017&lt;/li&gt;
&lt;li&gt;机构：Twitter; University of Toronto&lt;/li&gt;
&lt;li&gt;关键词：few-shot learning, metric learning&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.05175.pdf&quot;&gt;论文链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开源代码：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jakesnell/prototypical-networks&quot;&gt;jakesnell/prototypical-networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch&quot;&gt;orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他资料：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Prototypical-Networks-for-Few-shot-Learning-Snell-Swersky/c269858a7bb34e8350f2442ccf37797856ae9bca&quot;&gt;Semantic Scholar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="few-shot learning" scheme="https://kyonhuang.top/blog/tags/few-shot-learning/"/>
    
    <category term="小样本学习" scheme="https://kyonhuang.top/blog/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="metric learning" scheme="https://kyonhuang.top/blog/tags/metric-learning/"/>
    
    <category term="度量学习" scheme="https://kyonhuang.top/blog/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】A Closer Look at Few-shot Classification</title>
    <link href="https://kyonhuang.top/blog/CloserLookFewShot-notes/"/>
    <id>https://kyonhuang.top/blog/CloserLookFewShot-notes/</id>
    <published>2019-09-26T10:38:53.000Z</published>
    <updated>2019-09-26T11:42:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CloserLookFewShot-paper-title.png"></p><h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>本文研究了骨干网络的能力、数据集领域差异对现有小样本学习方法性能的影响，并强调了现有小样本学习方法的领域自适应能力普遍较差，需要多加注意。</p><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><ul><li>作者：Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin Huang</li><li>出处：ICLR 2019</li><li>机构：CMU; Georgia Tech; National Taiwan University; Virginia Tech</li><li>关键词：few-shot learning, domain shift, domain adaptation</li><li><a href="https://openreview.net/pdf?id=HkxLXnAcFQ">论文链接</a></li><li>开源代码：<a href="https://github.com/wyharveychen/CloserLookFewShot">wyharveychen/CloserLookFewShot</a></li><li>其他资料：<ul><li><a href="https://www.semanticscholar.org/paper/A-Closer-Look-at-Few-shot-Classification-Chen-Liu/9d5ec23154fb278a765f47ba5ee5150bd441d0de">Semantic Scholar</a></li><li><a href="https://zhuanlan.zhihu.com/p/76596852">知乎上的一份解读</a></li><li><a href="https://zhuanlan.zhihu.com/p/64672817">知乎上的另一份解读</a></li></ul></li></ul><span id="more"></span><h2 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h2><h3 id="背景与主要贡献"><a href="#背景与主要贡献" class="headerlink" title="背景与主要贡献"></a>背景与主要贡献</h3><p>尽管小样本学习（Few-Shot Learning, FSL）近年来有很大的进展，但各种方法实现上的细节差异掩盖了相对表现增益，缺少在同一框架下进行公平的比较。例如，是否在数据增强的情况下进行训练（意思是简单的数据增强能够大幅提高 baseline 的效果）。本文有三个主要贡献：</p><ol><li>本文对于几种代表性的小样本分类算法进行分析，特别实验了基础骨干网络对这些方法的影响。结果表明，更深的骨干网络（即特征提取能力越强），在领域差异有限的数据集上，显著降低了方法之间的性能差异；</li><li>本文建立了两个 baseline，其中一种基于距离的分类 baseline 能够在 mini-ImageNet 和 CUB 数据集上与 SOTA 方法相媲美；</li><li>虽然目前的评估侧重于识别具有有限训练样例的新类，但这些新类从同一数据集中采样。而在现实中，新类和基础类通常存在领域漂移（domain shift）问题。因此，本文强调了小样本学习任务中的领域自适应问题。设置了评估 FSL 算法的跨域泛化能力的实验，其中，基础类和新类别从不同领域中采样。实验表明，目前的 FSL 分类 SOTA 算法在领域漂移的情况下表现相当不好，效果甚至比 baseline 要差，从而表明 FSL 中学习适应领域差别的重要性。</li></ol><h4 id="领域自适应（domain-adaptation）"><a href="#领域自适应（domain-adaptation）" class="headerlink" title="领域自适应（domain adaptation）"></a>领域自适应（domain adaptation）</h4><p>前文提到的有关领域的相关概念可能在迁移学习领域更为常见。<strong>领域（domain）</strong>主要由两部分组成：<strong>数据</strong>和<strong>生成这些数据的概率分布</strong>。而<strong>领域漂移（domain shift）</strong>指的就是源域和目标域的数据分布差异较大。<strong>领域自适应（domain adaptation）</strong>就是旨在解决领域漂移问题的研究内容。</p><p>在本文提到的场景中，领域自适应和小样本分类的区别：</p><ul><li>领域自适应：旨在将<strong>源数据集</strong>的知识迁移到<strong>目标数据集</strong>的<strong>相同</strong>类别上；</li><li>小样本分类：从基础类学习去分辨<strong>同一数据集</strong>的<strong>新</strong>类。</li></ul><p>如果基础类和新类都来自同一数据集，则基础类和新类之间存在的领域漂移极少。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/CloserLookFewShot-paper-Table-A1.png"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>本文提出的 Baseline 和 Baseline++：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/4-Figure1-1.png"></p><p>在 Baseline++ 中，权重矩阵 $\mathbf{W}_{b} \in \mathbb{R}^{d \times c}$ 可以被分为 $c$ 个 $d$ 维向量，每个向量表示一个类。在训练阶段，每一个输入特征，与 $c$ 个 $d$ 维向量计算 cos 相似度，之后用一个 softmax 函数对相似度进行 normalization。因此，分类结果选取相似度最大的权重向量所代表的类。因此，学到的权重向量可以看作每一类的 prototype，而分类器基于输入特征与学到的 prototype 的距离来运作。softmax 函数可防止学到的权重向量变为零向量。</p><p>Baseline++ 显著减少了类内（intra-class）变化性。当然，更深的骨干网络也能做到这一点。</p><hr><p>对比的方法包括 MatchingNet、ProtoNet、RelationNet、MAML：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/5-Figure2-1.png"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>三个场景对应的数据集：</p><ol><li>通用对象识别：mini-ImageNet；</li><li>细粒度图像分类：CUB-200-2011；</li><li>跨域自适应：mini-ImageNet (base class) -&gt; CUB (novel class)。</li></ol><h4 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h4><p>对比各方法在对应的论文公布的结果和本文作者实现的结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/7-Table1-1.png"></p><p>带<code>*</code>号的是没有进行数据增强的 Baseline。带<code>#</code>号指 way 数更多的 PropoNet。可以看到，数据增强能够有效缓解 Baseline 的过拟合，从而提升效果。</p><hr><p>在 mini-ImageNet 和 CUB 数据集上的实验结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/7-Table2-1.png"></p><p>可以看到，Baseline++ 可以与现有 SOTA 方法相媲美。同时，Baseline++ 要比 Baseline 表现好很多，说明<strong>减少类内变化是现在小样本分类问题设置中的一个重要因素</strong>。</p><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>考虑到一个更深的骨干网络也能够减少类内变化，因此在同一方法上换用不同的骨干网络比较效果，结果如下：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/8-Figure3-1.png"></p><p>可以看到，更深的骨干网络一般都能够提升各方法的效果。但在 mini-ImageNet 数据集的 5-shot 设置上，一些元方法的效果变差（一种看法是说明，在领域差异较大的情况下，随着特征提取能力的提高，这些方法性能差异越来越大）。考虑到 mini-ImageNet 数据集的基础类与新类的领域差异要比 CUB 数据集更大，因此，接下来通过实验分析这种域差异如何影响小样本分类的效果。</p><h4 id="分析领域差异大小对各方法的影响"><a href="#分析领域差异大小对各方法的影响" class="headerlink" title="分析领域差异大小对各方法的影响"></a>分析领域差异大小对各方法的影响</h4><p>即分析各方法的跨域能力。在 mini-ImageNet (base class) -&gt; CUB (novel class) 的 5-shot，骨干网络为 ResNet-18 的分类准确率效果如下：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/9-Table3-1.png"></p><p>在不同场景下的 5-shot，骨干网络为 ResNet-18 的分类准确率表现对比如下：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/9-Figure4-1.png"></p><p>可以看到，在 base class 和 novel class 域差异较大时，Baseline 模型表现相对较好。说明现有元学习方法难以适应于域差异与 base support class 过大的新类。而 Baseline 模型简单地基于少数新类别数据微调分类器，允许快速适应新类别并受领域漂移地影响较小。同时，Baseline 模型比 Baseline++ 方法表现更好，<strong>可能是因为额外减少类内变化会影响跨域的适应性</strong>。</p><h4 id="领域适应能力提升的影响"><a href="#领域适应能力提升的影响" class="headerlink" title="领域适应能力提升的影响"></a>领域适应能力提升的影响</h4><p>为了让小样本学习方法能够有更好的适应能力，对于 MatchingNet 和 ProtoNet，可以固定特征并训练一个新的 softmax 训练器；对于 MAML，由于初始化方法 MAML 无法固定特征，因此设置为 用 support 更新更多次迭代（实验中设置为 100 次）来训练新的分类层；对于 RelationNet，特征是卷积映射而非特征向量，因此无法用 softmax 替换。作为替代，将新类中的少数训练数据随机分为 3 个 support 和 2 个 query 数据，来用 100 个 epochs 微调关系模块。</p><p><img src="https://d3i71xaburhd42.cloudfront.net/9d5ec23154fb278a765f47ba5ee5150bd441d0de/9-Figure5-1.png"></p><p>实验表明，缺少适应是之前元学习方法比 Baseline 差的原因。因此，<strong>在元训练阶段学习去学会适应领域漂移，对于未来的元学习研究是一个重要方向</strong>。</p><h4 id="量化分析骨干网络深度对类内变化的影响"><a href="#量化分析骨干网络深度对类内变化的影响" class="headerlink" title="量化分析骨干网络深度对类内变化的影响"></a>量化分析骨干网络深度对类内变化的影响</h4><p>使用了 [1] 中的 Davies-Bouldin 指数，这是一种评估聚类（在本文中即为类）的紧密度的度量。结论是，使用更深的骨干网络时，base class 和 novel class 的特征的类内变化都会减少。</p><h3 id="个人笔记"><a href="#个人笔记" class="headerlink" title="个人笔记"></a>个人笔记</h3><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>本文没有提出新颖的模型，而是指出小样本学习领域一些容易被忽视的问题，特别强调了骨干网络（特征提取网络）的能力、数据集的差异性以及领域自适应问题对小样本学习任务的影响。从帮助研究者认清小样本学习领域面临的挑战而言，本文是有价值的。</p><h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ul><li>[1] David L Davies and Donald W Bouldin. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1979.</li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/CloserLookFewShot-paper-title.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;一句话总结&quot;&gt;&lt;a href=&quot;#一句话总结&quot; class=&quot;headerlink&quot; title=&quot;一句话总结&quot;&gt;&lt;/a&gt;一句话总结&lt;/h2&gt;&lt;p&gt;本文研究了骨干网络的能力、数据集领域差异对现有小样本学习方法性能的影响，并强调了现有小样本学习方法的领域自适应能力普遍较差，需要多加注意。&lt;/p&gt;
&lt;h2 id=&quot;论文信息&quot;&gt;&lt;a href=&quot;#论文信息&quot; class=&quot;headerlink&quot; title=&quot;论文信息&quot;&gt;&lt;/a&gt;论文信息&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;作者：Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin Huang&lt;/li&gt;
&lt;li&gt;出处：ICLR 2019&lt;/li&gt;
&lt;li&gt;机构：CMU; Georgia Tech; National Taiwan University; Virginia Tech&lt;/li&gt;
&lt;li&gt;关键词：few-shot learning, domain shift, domain adaptation&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=HkxLXnAcFQ&quot;&gt;论文链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开源代码：&lt;a href=&quot;https://github.com/wyharveychen/CloserLookFewShot&quot;&gt;wyharveychen/CloserLookFewShot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;其他资料：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/A-Closer-Look-at-Few-shot-Classification-Chen-Liu/9d5ec23154fb278a765f47ba5ee5150bd441d0de&quot;&gt;Semantic Scholar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/76596852&quot;&gt;知乎上的一份解读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/64672817&quot;&gt;知乎上的另一份解读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="few-shot learning" scheme="https://kyonhuang.top/blog/tags/few-shot-learning/"/>
    
    <category term="domain shift" scheme="https://kyonhuang.top/blog/tags/domain-shift/"/>
    
    <category term="domain adaptation" scheme="https://kyonhuang.top/blog/tags/domain-adaptation/"/>
    
    <category term="小样本学习" scheme="https://kyonhuang.top/blog/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="领域漂移" scheme="https://kyonhuang.top/blog/tags/%E9%A2%86%E5%9F%9F%E6%BC%82%E7%A7%BB/"/>
    
    <category term="领域自适应" scheme="https://kyonhuang.top/blog/tags/%E9%A2%86%E5%9F%9F%E8%87%AA%E9%80%82%E5%BA%94/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】Adaptive Cross-Modal Few-Shot Learning</title>
    <link href="https://kyonhuang.top/blog/AM3-notes/"/>
    <id>https://kyonhuang.top/blog/AM3-notes/</id>
    <published>2019-09-13T01:52:12.000Z</published>
    <updated>2019-09-13T01:59:54.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/AM3-paper-title.png"></p><h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>本文提出了一种自适应模态混合机制，可以根据要学习的新图像类别自适应地组合来自视觉和语言两种模态的信息，比模态对齐方法更适用于小样本学习。</p><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><ul><li>作者：Chen Xing, Negar Rostamzadeh, Boris N. Oreshkin, Pedro H. O. Pinheiro</li><li>出处：NeurIPS 2019</li><li>机构：Element AI, Montreal, Canada; Nankai University</li><li>关键词：few-shot learning, metric learning, multimodal</li><li><a href="https://arxiv.org/pdf/1902.07104.pdf">论文链接</a></li><li>开源代码：<a href="https://github.com/ElementAI/am3">ElementAI/am3</a></li><li>其他资料：<ul><li><a href="https://www.semanticscholar.org/paper/Adaptive-Cross-Modal-Few-Shot-Learning-Xing-Rostamzadeh/7d113621ab50a8b875a12cea3ad5b0263e9520ac">Semantic Scholar</a></li></ul></li></ul><span id="more"></span><h2 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>当来自视觉模态的数据有限时，利用辅助模态（例如，属性，未标记的文本语料库）来帮助图像分类主要由无样本学习（Zero-Shot Learning, ZSL）驱动。与少样本学习（Few-Shot Learning, FSL）对比，无样本学习没有少量标记样本来帮助识别新类别。大多数方法在训练阶段将两种模态对齐，以此迫使模态被映射在一起并具有相同的语义结构。这样，来自辅助模态的知识在测试时被迁移到视觉方面用于识别新类别。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/AM3-Figure-1.png"></p><p>然而，视觉和自然语言语义特征根据定义具有不同的结构。对于不同的概念，视觉特征可能比文本特征更丰富和更具辨识度；而对于另一些概念，可能情况是相反的。此外，当图像数量非常少时，从视觉模态提供的信息往往是嘈杂和局部的，而（从无监督的文本语料库中学习的）语义表示可以提供强大的先验知识和上下文以帮助学习。在无样本学习中，测试阶段没有给视觉信息，因此算法只能完全依靠辅助模态（例如，文本）；而当有标记的图像样本数量很多时，神经网络倾向于忽略辅助模态，因为它已经有能力通过大量样本学会泛化。小样本学处于在这两个极端情况的中间态，因此我们可以假设视觉和语义信息都可用于小样本学习。</p><p>因此，比起将两种模态对齐并进行知识迁移，在测试阶段进行从两种模态获得信息的小样本学习时，最好<strong>将各模态视为独立的知识来源，并根据不同的场景自适应地利用不同的模态</strong>。</p><p>根据上述分析，本文提出<strong>自适应模态混合机制（Adaptive Modality Mixture Mechanism, AM3）</strong>，可以根据要学习的新图像类别自适应地组合来自两种模态的信息，用于小样本学习。</p><p>实验表明，效果超过当前的单模态小样本学习方法和模态对齐方法。另外，试验还表明，该模型可以有效调整其对两种模态的关注。当镜头数量非常小时，性能的提升特别高。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://d3i71xaburhd42.cloudfront.net/7d113621ab50a8b875a12cea3ad5b0263e9520ac/5-Figure2-1.png"></p><p>本文提出的自适应模态混合机制（Adaptive Modality Mixture Mechanism, AM3）可以应用于任何基于度量的方法。本文选用原型网络（Prototypical Networks）和 TADAM。对于类别 $c$，新的原型（prototype）计算如下：</p><p>$$<br>\mathbf{p}_{c}^{\prime}=\lambda_{c} \cdot \mathbf{p}_{c}+\left(1-\lambda_{c}\right) \cdot \mathbf{w}_{c}<br>$$</p><p>其中，$\mathbf{w}_{c}=g\left(\mathbf{e}_{c}\right)$ 是一个对于类别 $c$ 的标签嵌入的转换版本，$\mathbf{e}_{c}$ 是对于类别 $c$ 标签的预训练词向量，而转换 $g : \mathbb{R}^{n_{w}} \rightarrow \mathbb{R}^{n_{p}}$ 让两个模态处于同一空间并可以合并。$\lambda_{c}$ 是自适应混合系数（adaptive mixture coefficient）：</p><p>$$<br>\lambda_{c}=\frac{1}{1+\exp \left(-h\left(\mathbf{w}_{c}\right)\right)}<br>$$</p><p>$h$ 是自适应混合网络（adaptive mixing network）。自适应混合系数 $\lambda_{c}$ 可以根据不同的变量进行控制。</p><p>训练过程与原始的原型网络相似，只是距离度量变为</p><p>$$<br>p_{\theta}\left(y=c | q_{t}, S_{e}, \mathcal{W}\right)=\frac{\exp \left(-d\left(f\left(q_{t}\right), \mathbf{p}_{c}^{\prime}\right)\right)}{\sum_{k} \exp \left(-d\left(f\left(q_{t}\right), \mathbf{p}_{k}^{\prime}\right)\right)}<br>$$</p><p>损失是每个查询集样本的真类的负对数似然（the negative loglikelihood of the true class of each query sample）：</p><p>$$<br>\mathcal{L}(\theta)=\underset{\left(\mathcal{S}_{e}, \mathcal{Q}_{e}\right)}{\mathbb{E}}-\sum_{t=1}^{Q_{e}} \log p_{\theta}\left(y_{t} | q_{t}, \mathcal{S}_{e}\right)<br>$$</p><p>其中，$\mathcal{S}_{e}$ 是支撑集，$\mathcal{Q}_{e}$ 是查询集。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h4><p>数据集：</p><ul><li>miniImageNet [2]</li><li>tieredImageNet [3]</li><li>CUB-200 [4]：这是一个无样本学习数据集。考虑到大多数模态对齐方法没有在小样本学习数据集上有公开的实验结果，这个数据集用于将 AM3 与模态对齐方法进行对比</li></ul><p>本文在  和 tieredImageNet [3]，将 AM3 与三种基线进行对比：单模态 FSL 方法，模态对齐方法，模态对齐方法的基于度量的扩展。</p><p>baselines：</p><ul><li>单模态 FSL 方法：MAML、LEO [6]、原型网络、TADAM [5] 等；</li><li>模态对齐方法：CADA-VAE [7] 等；</li><li>模态对齐方法的基于度量的扩展：使用了基于度量的损失以及原型网络的训练方式（episode training）。</li></ul><p>在 miniImageNet 数据集上的分类结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/7d113621ab50a8b875a12cea3ad5b0263e9520ac/7-Table1-1.png"></p><p>在 tieredImageNet 数据集上的分类结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/7d113621ab50a8b875a12cea3ad5b0263e9520ac/7-Table2-1.png"></p><p>结论：</p><ol><li>在所有数据集上，AM3 的表现都优于其主干方法。说明在正确使用时，文本模态可以帮助提高基于度量的 FSL 学习框架的性能；</li><li>AM3（主干方法选用 TADAM）取得 SOTA，且当 shot 数（即样本数）越少时，效果提升越多。说明当视觉模态提供的信息越少时，语义信息对分类的帮助越大；</li><li>当扩展到基于度量、episodic、FSL 框架时时，所有模态对齐方法的表现都显著提升。然而扩展前后这些模态对齐方法都比单模态 FSL 方法的 SOTA 要差。说明尽管模态对齐方法对于 ZSL 中的跨模态是有效的，但并不适合 FSL。<strong>一个可能的原因是，两个不同的结构被迫对齐导致双方的一些信息可能损失</strong>。</li></ol><p>在 CUB-200 数据集上的分类结果：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/7d113621ab50a8b875a12cea3ad5b0263e9520ac/14-Table3-1.png"></p><p>结论：对于无样本学习场景，AM3 降级为最简单的模态对齐方法，将文本语义空间映射到视觉空间。因此，在没有自适应机制的情况下，AM3 的表现与 DeViSE 大致相同。说明自适应机制在 FSL 场景中观察到的性能提升中起主要作用。</p><h4 id="自适应机制分析"><a href="#自适应机制分析" class="headerlink" title="自适应机制分析"></a>自适应机制分析</h4><p>对本文提出的自适应机制进行定量分析：</p><p><img src="https://d3i71xaburhd42.cloudfront.net/7d113621ab50a8b875a12cea3ad5b0263e9520ac/8-Figure3-1.png"></p><p>结论：</p><ol><li>当 shot 数（即样本数）增加时，AM3 与其主干方法的表现差距减小；</li><li>自适应混合系数 $\lambda_{c}$ 的平均值与 shot 数有关，当 shot 数减小时，AM3 在文本模态上的权重更大（在视觉模态上更小）。这种趋势表明，当视觉信息非常少时，AM3 可以自动将焦点更多地调整到文本模态以帮助分类；</li><li>当 $\lambda_{c}$ 的方差随着 shot 数增加而减小时，AM3 与其主干方法的性能差距也会缩小。这表明 AM3 在类别级别的适应性对性能提升有非常重要的作用（？？？觉得有点牵强）。</li></ol><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p><img src="https://d3i71xaburhd42.cloudfront.net/7d113621ab50a8b875a12cea3ad5b0263e9520ac/15-Table4-1.png"></p><p>所以选用变换后的语义特征来做自适应机制是实验得到的最好选择。</p><h3 id="个人笔记"><a href="#个人笔记" class="headerlink" title="个人笔记"></a>个人笔记</h3><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>我个人认为，转换 $g$ 本质上还是一种表示空间的强行对齐，会破坏模态本身的自然结构。因此，可能可以考虑一种更好的特征组合与自适应调整方式。</p><h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ul><li>[1] Zero-shot learning-the good, the bad and the ugly, CVPR 2017.</li><li>[2] Matching networks for one shot learning, NIPS 2016.</li><li>[3] Meta-learning for semi-supervised few-shot classification, ICLR 2018.</li><li>[4] Caltech-UCSD Birds 200, Technical Report CNS-TR-2010-001, 2010.</li><li>[5] Tadam: Task dependent adaptive metric for improved few-shot learning, NeurIPS 2018.</li><li>[6] Meta-learning with latent embedding optimization, ICML 2016.</li><li>[7] Generalized zero-and few-shot learning via aligned variational autoencoders, CVPR 2019.</li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/AM3-paper-title.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;一句话总结&quot;&gt;&lt;a href=&quot;#一句话总结&quot; class=&quot;headerlink&quot; title=&quot;一句话总结&quot;&gt;&lt;/a&gt;一句话总结&lt;/h2&gt;&lt;p&gt;本文提出了一种自适应模态混合机制，可以根据要学习的新图像类别自适应地组合来自视觉和语言两种模态的信息，比模态对齐方法更适用于小样本学习。&lt;/p&gt;
&lt;h2 id=&quot;论文信息&quot;&gt;&lt;a href=&quot;#论文信息&quot; class=&quot;headerlink&quot; title=&quot;论文信息&quot;&gt;&lt;/a&gt;论文信息&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;作者：Chen Xing, Negar Rostamzadeh, Boris N. Oreshkin, Pedro H. O. Pinheiro&lt;/li&gt;
&lt;li&gt;出处：NeurIPS 2019&lt;/li&gt;
&lt;li&gt;机构：Element AI, Montreal, Canada; Nankai University&lt;/li&gt;
&lt;li&gt;关键词：few-shot learning, metric learning, multimodal&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1902.07104.pdf&quot;&gt;论文链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开源代码：&lt;a href=&quot;https://github.com/ElementAI/am3&quot;&gt;ElementAI/am3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;其他资料：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Adaptive-Cross-Modal-Few-Shot-Learning-Xing-Rostamzadeh/7d113621ab50a8b875a12cea3ad5b0263e9520ac&quot;&gt;Semantic Scholar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="few-shot learning" scheme="https://kyonhuang.top/blog/tags/few-shot-learning/"/>
    
    <category term="小样本学习" scheme="https://kyonhuang.top/blog/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="multimodal" scheme="https://kyonhuang.top/blog/tags/multimodal/"/>
    
    <category term="metric learning" scheme="https://kyonhuang.top/blog/tags/metric-learning/"/>
    
    <category term="多模态" scheme="https://kyonhuang.top/blog/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="度量学习" scheme="https://kyonhuang.top/blog/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】Composing Text and Image for Image Retrieval - An Empirical Odyssey</title>
    <link href="https://kyonhuang.top/blog/TIRG-notes/"/>
    <id>https://kyonhuang.top/blog/TIRG-notes/</id>
    <published>2019-09-06T08:20:32.000Z</published>
    <updated>2019-11-27T01:31:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/TIRG-paper-title.png"></p><h2 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h2><p>作为第一篇研究用于图像检索的图像与文本特征组合问题的论文，本文提出一种采用门控机制和残差连接的特征组合方式，确保修改后的特征处于目标图像特征处在相同空间中，并通过度量学习的方式达到 SOTA。</p><h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><ul><li>作者：Nam S. Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays</li><li>出处：CVPR 2019</li><li>机构：Google AI, Stanford</li><li>关键词：Image retrieval, multimodal, metric learning, feature composition</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Vo_Composing_Text_and_Image_for_Image_Retrieval_-_an_Empirical_CVPR_2019_paper.pdf">论文链接</a></li><li>开源代码：<a href="https://github.com/google/tirg">https://github.com/google/tirg</a></li><li>其他资料：<ul><li><a href="https://www.semanticscholar.org/paper/Composing-Text-and-Image-for-Image-Retrieval-An-Vo-Jiang/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d">Semantic Scholar</a></li></ul></li></ul><span id="more"></span><h2 id="内容简记"><a href="#内容简记" class="headerlink" title="内容简记"></a>内容简记</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/1-Figure1-1.png"></p><p>本文展示在一种典型的图像检索场景——用户使用已经找到的图像作为参考，然后用文本表达其与查询意图的差异，来检索相关图像——下的解决方案。这种场景与基于属性的产品检索密切相关，但不同之处在于文本可以由多个词语组成，而不是单个属性。</p><p>主要的研究问题在于当由两种不同的输入模态时，如何表示查询，即<strong>如何为查询学习有意义的跨模态特征组合以便找到目标图像</strong>。文本与图像间的特征组合方法，从简单的拼接、浅前馈网络，到先进的机制（如关系 [1] 或参数哈希 [2]）已经广泛应用于相关问题。在本文研究的问题中，文本应该修改查询图像的特征，但是希望修改后得到的特征向量和目标图像处在相同的空间。因此，本文提出<strong>“文本图像残差门控”（Text Image Residual Gating，TIRG）</strong>，通过<strong>门控机制和残差连接</strong>来让文本修改图片特征。该方法在三个数据集上达到 SOTA。</p><p>已有的特征组合方法（即本文进行比较的 baselines）：</p><ul><li>拼接输入 MLP；</li><li>Show and Tell [3]：用一个 LSTM 同时编码图像和文本，先输入图像特征，后输入文本单词。LSTM 的最后一个状态作为表征；</li><li>Attribute as Operator [4]：将文本编码为转移矩阵，并应用到图像特征上得到组合特征；</li><li>Parameter hashing [2]：已被编码的文本特征被散列为变换矩阵，用于替代 CNN 网络中的全连接层，以得到组合特征；</li><li>Relationship [1]：首先使用 CNN 提取图像的 2d 特征图（feature map），然后创建一组关系特征，每个特征是 2d 特征图中的 2 个局部特征与文本特征的拼接。这组特征被传入一个 MLP 中并将输出平均化以得到组合特征；</li><li>FiLM [5]：文本特征被用于预测调制特征（modulation features） $\gamma^{i}, \beta^{i} \in \mathbb{R}^{C}$，其中 $i$ 为层的索引（index），$C$ 为特征的数量。之后执行图像特征的仿射变换：$\phi_{x t}^{i}=\gamma^{i} \cdot \phi_{x}^{i}+\beta^{i}$。</li></ul><p>其中，本文提出的 TIRG 与 FiLM 的区别在于：</p><ul><li>TIRG 使用<strong>非线性变换</strong>（而非 FiLM 中的线性变换），具有更多可学习参数，使得能够对图像进行较为复杂的修改；</li><li>TIRG <strong>只修改单层的 CNN</strong>。修改尽可能少的层有助于确保修改的特征处于目标图像的相同空间中。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/4-Figure2-1.png"></p><p><strong>TIRG</strong> 的公式为：</p><p>$$<br>\phi_{x t}^{r g}=w_{g} f_{\mathrm{gate}}\left(\phi_{x}, \phi_{t}\right)+w_{r} f_{\mathrm{res}}\left(\phi_{x}, \phi_{t}\right)<br>$$</p><p>其中，$\phi_{x}$ 是 CNN 得到的查询图像特征，$\phi_{t}$ 是 LSTM 得到的文本特征；$f_{\text {gate }}, f_{\text {res }} \in \mathbb{R}^{W \times H \times C}$；$w_{g}, w_{r}$ 是用于平衡的可学习权重。</p><p><strong>门控机制</strong>的计算为：</p><p>$$<br>f_{\text {gate }}\left(\phi_{x}, \phi_{t}\right)=\sigma\left(W_{g 2} * {RELU}\left(W_{g 1} *\left[\phi_{x}, \phi_{t}\right]\right) \odot \phi_{x}\right.<br>$$</p><p>其中，$W_{g 1}$ 和 $W_{g 2}$ 都是 3x3 卷积核。$\phi_{t}$ 将沿着高度和宽度的尺寸广播（broadcast along the height and width dimension），使其形状与图像特征 $\phi_{x}$ 匹配。</p><p><strong>残差连接</strong>的计算为：</p><p>$$<br>f_{\text {res }}\left(\phi_{x}, \phi_{t}\right)=W_{r 2} * {RELU}\left(W_{r 1} *\left(\left[\phi_{x}, \phi_{t}\right]\right)\right)<br>$$</p><p>TIRG 的直观理解是我们只想根据文本特征“修改”图像特征，而不是创建一个完全不同的特征空间。门控机制被设计成保留某些与文本修改无关的查询图像特征。</p><h4 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h4><p>训练目标是使“修改后”的图像特征与目标图像特征尽可能接近，同时使与非目标图像特征尽可能远离。因此，采用每次采样一个正例和 B 个负例计算损失，并重复 M 次，公式为：</p><p>$$<br>L=\frac{-1}{M B} \sum_{i=1}^{B} \sum_{m=1}^{M} \log \left\{\frac{\exp \left\{\kappa\left(\psi_{i}, \phi_{i}^{+}\right)\right\}}{\sum_{\phi_{j} \in \mathcal{N}_{i}^{m}} \exp \left\{\kappa\left(\psi_{i}, \phi_{j}\right)\right\}}\right\}<br>$$</p><p>其中 $\kappa$ 是相似度计算函数，在实现中采用点积或负欧式距离（为负时越小的距离能够获得更高的分数）。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h4><p>在三个数据集上进行实验：Fashion200k [6]，MIT-States [7]，本文提出的 CSS 数据集。三个数据集的区别在于，前两个数据集图像中物体不变，属性之一发生变化；而 CSS 数据集的变化更加复杂，可以增加、减少处于某个位置的物体，也可以改变物体的属性；前两个数据集的图像内容较为丰富，而 CSS 数据集的图像是生成的规律几何体摆放，且有 2D 和 3D 两种图像。</p><p>主要评价指标为 recall at rank k (R@K)，即正确的图像（即标签）在前 K 个检索出的图像中的百分比。MIT-States 同时汇报分类结果。本文提出的 TIRG 均达到 SOTA。</p><p>在 Fashion200k 和 MIT-States 数据集上的检索结果如下：</p><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/5-Table1-1.png"></p><p>在 MIT-States 数据集上的分类结果如下：</p><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/6-Table3-1.png"></p><p>在 CSS 数据集上的检索结果如下：</p><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/7-Table4-1.png"></p><h4 id="从组合特征中重建图像"><a href="#从组合特征中重建图像" class="headerlink" title="从组合特征中重建图像"></a>从组合特征中重建图像</h4><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/8-Figure6-1.png"></p><p>为了更深入地了解组合特征的本质，本文训练了一个转置卷积网络（transposed convolutional network），学习从查询图像的特征中重建图像，然后将其应用于组合特征。对比直接拼接、FiLM 和本文的 TIRG 三种方法得到的组合特征的重建图像。从 TIRG 特征表示生成的图像看起来更好，并且更接近目标图像。</p><h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d/7-Table5-1.png"></p><ol><li>去除残差连接和门控机制都会使模型表现显著变差；</li><li>TIRG 的特征修改可发生在最后一个卷积层或最后一个全连接层。实验表明，对于 Fashion200k 和 MIT-States 数据集在最后一个全连接层进行特征修改，以及对于 CSS 数据集在最后一个卷积层进行特征修改，效果更好。作者认为这是因为在 CSS 数据集上的修改更加空间局部化，而在其他两个数据集上的修改更加全局化；</li><li>设置训练时的每次采样的样本数量为 2（即损失变为 soft triplet loss）或 B（即实验中原本的设置）。当采样大小为 2 时，会发现对于 Fashion200k 这样较大的数据集，网络会欠拟合；而当采样大小为 B 时，网络对 Fashion200k 数据集拟合更好并得到更好的结果，但是对于其他两个数据集训练变得不稳定。</li></ol><h3 id="个人笔记"><a href="#个人笔记" class="headerlink" title="个人笔记"></a>个人笔记</h3><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p><strong>实现预训练的 CNN 和 LSTM</strong></p><p>CNN：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">img_model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>LSTM：</p><p>源码自己实现了一个应用于文本的 LSTM，详见 <a href="https://github.com/google/tirg/blob/master/text_model.py">text_model.py</a>。</p><!-- 暂时还没搞懂是怎么将 one-hot 向量转为词嵌入的，是 torch.nn.Embedding：https://pytorch.org/docs/stable/nn.html?highlight=torch%20nn%20embedding#torch.nn.Embedding 吗？ --><p><strong>实现可学习权重 w_g，w_r</strong></p><!-- 用 torch.nn.Linear 不太好实现 --><p>通过<code>torch.nn.Parameter</code>将张量添加到参数列表中：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">self.a = torch.nn.Parameter(torch.tensor([<span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">f = F.sigmoid(f1) * img_features * self.a[<span class="number">0</span>] + f2 * self.a[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 但是没有理解为什么只使用两个值，初始化时要用长度为 4 的向量</span></span><br></pre></td></tr></table></figure><p><strong>将文本特征沿着高度和宽度的尺寸广播后与图像特征拼接</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = text_features</span><br><span class="line">y = y.reshape((y.shape[<span class="number">0</span>], y.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>)).repeat(<span class="number">1</span>, <span class="number">1</span>, x.shape[<span class="number">2</span>], x.shape[<span class="number">3</span>])</span><br><span class="line"><span class="comment"># x 为图像特征</span></span><br><span class="line">z = torch.cat((x, y), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>实现在最后一个卷积层修改特征</strong></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TIRGLastConv</span>(<span class="params">ImgEncoderTextEncoderBase</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;The TIGR model with spatial modification over the last conv layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The method is described in</span></span><br><span class="line"><span class="string">  Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays.</span></span><br><span class="line"><span class="string">  &quot;Composing Text and Image for Image Retrieval - An Empirical Odyssey&quot;</span></span><br><span class="line"><span class="string">  CVPR 2019. arXiv:1812.07119</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, texts, embed_dim</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(TIRGLastConv, self).__init__(texts, embed_dim)</span><br><span class="line"></span><br><span class="line">    self.a = torch.nn.Parameter(torch.tensor([<span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]))</span><br><span class="line">    self.mod2d = torch.nn.Sequential(</span><br><span class="line">        torch.nn.BatchNorm2d(<span class="number">512</span> + embed_dim),</span><br><span class="line">        torch.nn.Conv2d(<span class="number">512</span> + embed_dim, <span class="number">512</span> + embed_dim, [<span class="number">3</span>, <span class="number">3</span>], padding=<span class="number">1</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Conv2d(<span class="number">512</span> + embed_dim, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], padding=<span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.mod2d_gate = torch.nn.Sequential(</span><br><span class="line">        torch.nn.BatchNorm2d(<span class="number">512</span> + embed_dim),</span><br><span class="line">        torch.nn.Conv2d(<span class="number">512</span> + embed_dim, <span class="number">512</span> + embed_dim, [<span class="number">3</span>, <span class="number">3</span>], padding=<span class="number">1</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Conv2d(<span class="number">512</span> + embed_dim, <span class="number">512</span>, [<span class="number">3</span>, <span class="number">3</span>], padding=<span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compose_img_text</span>(<span class="params">self, imgs, texts</span>):</span></span><br><span class="line">    text_features = self.extract_text_feature(texts)</span><br><span class="line"></span><br><span class="line">    x = imgs</span><br><span class="line">    x = self.img_model.conv1(x)</span><br><span class="line">    x = self.img_model.bn1(x)</span><br><span class="line">    x = self.img_model.relu(x)</span><br><span class="line">    x = self.img_model.maxpool(x)</span><br><span class="line"></span><br><span class="line">    x = self.img_model.layer1(x)</span><br><span class="line">    x = self.img_model.layer2(x)</span><br><span class="line">    x = self.img_model.layer3(x)</span><br><span class="line">    x = self.img_model.layer4(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mod</span></span><br><span class="line">    y = text_features</span><br><span class="line">    y = y.reshape((y.shape[<span class="number">0</span>], y.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>)).repeat(</span><br><span class="line">        <span class="number">1</span>, <span class="number">1</span>, x.shape[<span class="number">2</span>], x.shape[<span class="number">3</span>])</span><br><span class="line">    z = torch.cat((x, y), dim=<span class="number">1</span>)</span><br><span class="line">    t = self.mod2d(z)</span><br><span class="line">    tgate = self.mod2d_gate(z)</span><br><span class="line">    x = self.a[<span class="number">0</span>] * F.sigmoid(tgate) * x + self.a[<span class="number">1</span>] * t</span><br><span class="line"></span><br><span class="line">    x = self.img_model.avgpool(x)</span><br><span class="line">    x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">    x = self.img_model.fc(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ul><li>[1] A simple neural network module for relational reasoning, NIPS 2017.</li><li>[2] Image question answering using convolutional neural network with dynamic parameter prediction, CVPR 2016.</li><li>[3] Show and tell: A neural image caption generator, CVPR 2015.</li><li>[4] Attributes as operators, 2018.</li><li>[5] Film: Visual reasoning with a general conditioning layer, 2018.</li><li>[6] Automatic spatially-aware fashion concept discovery, ICCV 2017.</li><li>[7] Discovering states and transformations in image collections, CVPR 2015.</li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/TIRG-paper-title.png&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;一句话总结&quot;&gt;&lt;a href=&quot;#一句话总结&quot; class=&quot;headerlink&quot; title=&quot;一句话总结&quot;&gt;&lt;/a&gt;一句话总结&lt;/h2&gt;&lt;p&gt;作为第一篇研究用于图像检索的图像与文本特征组合问题的论文，本文提出一种采用门控机制和残差连接的特征组合方式，确保修改后的特征处于目标图像特征处在相同空间中，并通过度量学习的方式达到 SOTA。&lt;/p&gt;
&lt;h2 id=&quot;论文信息&quot;&gt;&lt;a href=&quot;#论文信息&quot; class=&quot;headerlink&quot; title=&quot;论文信息&quot;&gt;&lt;/a&gt;论文信息&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;作者：Nam S. Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays&lt;/li&gt;
&lt;li&gt;出处：CVPR 2019&lt;/li&gt;
&lt;li&gt;机构：Google AI, Stanford&lt;/li&gt;
&lt;li&gt;关键词：Image retrieval, multimodal, metric learning, feature composition&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Vo_Composing_Text_and_Image_for_Image_Retrieval_-_an_Empirical_CVPR_2019_paper.pdf&quot;&gt;论文链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;开源代码：&lt;a href=&quot;https://github.com/google/tirg&quot;&gt;https://github.com/google/tirg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;其他资料：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Composing-Text-and-Image-for-Image-Retrieval-An-Vo-Jiang/fd5129e8ebfaa5dcce3d4ce2839b90c6cd3ca39d&quot;&gt;Semantic Scholar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="multimodal" scheme="https://kyonhuang.top/blog/tags/multimodal/"/>
    
    <category term="metric learning" scheme="https://kyonhuang.top/blog/tags/metric-learning/"/>
    
    <category term="多模态" scheme="https://kyonhuang.top/blog/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
    <category term="度量学习" scheme="https://kyonhuang.top/blog/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="image retrieval" scheme="https://kyonhuang.top/blog/tags/image-retrieval/"/>
    
    <category term="feature composition" scheme="https://kyonhuang.top/blog/tags/feature-composition/"/>
    
    <category term="图像检索" scheme="https://kyonhuang.top/blog/tags/%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2/"/>
    
    <category term="特征组合" scheme="https://kyonhuang.top/blog/tags/%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>写在博士生开始前</title>
    <link href="https://kyonhuang.top/blog/before-being-a-PhD-candidate/"/>
    <id>https://kyonhuang.top/blog/before-being-a-PhD-candidate/</id>
    <published>2019-08-19T07:42:13.000Z</published>
    <updated>2019-08-19T07:47:10.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/know-nothing-without-writing-papers.jpg"></p><p>这周末就是博士生开学典礼了，对自己的近况进行一个简单的介绍与总结。</p><h2 id="近况"><a href="#近况" class="headerlink" title="近况"></a>近况</h2><p>这一学年的显式收获是一篇 CIKM 2019 的 short paper。在经过 IJCAI 以一个大概 borderline 的分数被拒、拆出一部分当作本科毕设后，这份工作终于可以准备收尾了。同时，我也要去寻找一个能够支撑我整个博士生研究的新方向了。</p><p>为什么不再做当前的时间序列分析方向了？简单来说，单纯的时间序列所包含的信息量较少，简单而更具有可解释性的一系列传统方法已经有很好的表现。在这种情况下，使用深度学习来进行预测或者分类不一定能够表现更好，同时还会有解释性差、计算量大等问题。另外一点是，时间序列分析目前确实不是学术界研究热点，它可能更适合有大量可用数据的工业界研究人员进行挖掘来真正创造经济效益，而不适合作为一个只能用着开源数据集、还需要在一个研究领域耕耘至少五年的博士生的选题。</p><p>能中论文，当然会陷入短暂的兴奋中。但是收到邮件没过半天，我又回到沮丧的常态中。首先邮件中的评审意见里对这篇论文列举了很多问题，大多数都是我早已心知肚明的：没有在更多更大的公共数据集上进行实验、没有用实验证明我们的方法到底适用于有什么特点的数据、没有展现参数的设置如何影响模型的总体表现，等等。总而言之，没有对论文中提出的每个结论设计缜密的实验来证明。</p><p>更为深层的原因是，在这个月里，我渐渐发现这一年我好像并没有什么什么实质的进步。我仍然效率低下、没有做好时间管理、经常废弃既定的学习计划，作息紊乱、无法集中注意力、迷失在杂乱的信息流中，得过且过、不求甚解。看似阅读了很多论文，了解了一个领域，研究出一种方法，发表了一篇会议。实际上每次随便找了两篇论文浅浅看过就开始拿别人的东西堆叠想模型，对整个领域/方向/任务没有将顶会论文全部搜查一遍，对共性问题没有深入思考，甚至连 evaluation metrics 该采用哪些为什么都没有总结和思考过。最终的结果就是，成长速度奇慢。</p><h2 id="博一的主要目标"><a href="#博一的主要目标" class="headerlink" title="博一的主要目标"></a>博一的主要目标</h2><ol><li>最主要的目标当然是把所有课程顺利修完。</li><li>对未来的研究方向进行考察。考虑因素包括（在五年后）有较好的发展前景、对外部条件要求不高、出成果不是非常困难、符合自己兴趣。考察过程中要全面地了解领域现有任务、问题、方法、领军学者/机构。</li><li>夯实理论基础。每次选定书/课学习就坚持下来，控制在一个月内学习完成。回想一下，本科阶段最大的提升之一就在于坚持修完了吴恩达深度学习课程。</li><li>通过论文复现练习和数据竞赛来提高编程、调参等水平。数据竞赛方面的最终目标在 Kaggle 能在一场比赛中持续参加并获得银牌以上。</li><li>能够及时将感兴趣的论文/blog 消化到能和别人随口讲清楚的程度。每周尽量读一篇和当前研究方向不太相关的、近期顶会的 best paper 或者引起广泛讨论的 paper，来理解并吸收其他领域的思路。</li><li>发表一篇自己较为满意、对领域有贡献的论文。</li></ol><p>最后，我在考虑把博客翻修一下。</p><!--## 给自己的十五问需要定期对自己进行反思：1. 最近看了些什么（论文、blog 等）？这些东西是否能给别人随口讲明白，是否能归纳成类（有一条思维的线连着）？看的深不深，能不能说自己对这方面是专家？2. 与之前相比，所需的能力有没有提高？增长大不大，提高快不快？3. 对领域了解是否更深入？对已有工作的调研是否全面完善？4. 读论文在保证质量的同时有没有更快速？有没有将笔记完整记录？要对每一个细节都提出质疑：为什么要加这个模块/步骤/算法？去掉行不行？它起到的作用是不是就是作者说的作用，什么实验证明了这一点？同时读代码5. 对算法、模型、解决方案、实验的设计是否更完善？每个细节的设置都能说明原因？对于要验证的结论是否都有对应的实验？6. 数学水平是否提高，能够严格解释算法中的细节在数学上的原因？7. 代码写的是否更高效？对同一任务是否有一套较为完善的 pipeline 可以复用？是否保证最终的代码都完善导到可开源级别？每天花 1-2 小时阅读论文，然后花一天时间编写代码来进行实验8. 对数据是否更有感觉（有一套方案对数据进行判断，分析数据存在的可以解决的问题【即别的算法没有关注到的可创新点】和 bias）？ 9. 论文写的是否更顺畅，更独立，大改次数更少，可读性更高？排版和配图更好看？10. 口头报告是否做的更好？能用全英文？更吸引人？blog 同理，语言是否顺畅，条理是否清晰，是否有自己的思考、最新的内容？11. 是否将自己对外展示地更好？包括个人主页、blog、github、知乎等。提高自己和整个组的影响力12. 是否提高了自己的协作水平，和他人能够在同一项目中按照长处合理分配任务，及时交流？13. 自己的交际能力是否提升？能否主动和学术会议上认识的朋友、研究课题的国内外相关学者交流？14. 是否健康饮食作息，锻炼身体，提高了自己的身体素质和精神专注力？是否提升了其他爱好的水平？15. 是否能够对抗 peer pressure 和 self pressure、焦虑？是否保持积极的态度、饱满的精神？-->]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/know-nothing-without-writing-papers.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;这周末就是博士生开学典礼了，对自</summary>
      
    
    
    
    <category term="翻滚吧博士生" scheme="https://kyonhuang.top/blog/categories/%E7%BF%BB%E6%BB%9A%E5%90%A7%E5%8D%9A%E5%A3%AB%E7%94%9F/"/>
    
    
    <category term="总结" scheme="https://kyonhuang.top/blog/tags/%E6%80%BB%E7%BB%93/"/>
    
    <category term="未来展望" scheme="https://kyonhuang.top/blog/tags/%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B/"/>
    
    <category term="随笔" scheme="https://kyonhuang.top/blog/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>自动编码器一览（二）</title>
    <link href="https://kyonhuang.top/blog/autoencoder-intro-2/"/>
    <id>https://kyonhuang.top/blog/autoencoder-intro-2/</id>
    <published>2019-07-07T11:07:07.000Z</published>
    <updated>2019-07-10T01:35:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是“自动编码器一览”系列的第二篇博文。在本文中，我会介绍一些最近看过并且比较感兴趣的自动编码器的变形，说是介绍，可能更像是论文笔记合集。比起第一篇提到过那些更为通用的经典变形，这些相对而言比较新颖的自动编码器在结构上进行修改，使得它们更符合所对应的特定任务的需求。如果我之后又看到一些有意思的自动编码器，也可能在这篇博文中继续更新。</p><p>本文中包含以下内容：</p><ul><li>递归自动编码器（Recursive Autoencoder）</li><li>Additional Stacked Denoising Autoencoder（aSDAE）</li></ul><p>“自动编码器一览”系列：</p><ul><li><a href="http://kyonhuang.top/autoencoder-intro-1/">自动编码器一览（一）</a></li><li><a href="http://kyonhuang.top/autoencoder-intro-2/">自动编码器一览（二）</a></li><li><a href="http://kyonhuang.top/autoencoder-intro-3/">自动编码器一览（三）</a></li></ul><span id="more"></span><h2 id="递归自动编码器"><a href="#递归自动编码器" class="headerlink" title="递归自动编码器"></a>递归自动编码器</h2><p>我们首先介绍论文 “Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions”[1] 提出的<strong>递归自动编码器（Recursive Autoencoder）</strong>。在这篇论文中，递归自动编码器被用于通过无监督或半监督的训练方式，学习不定长句子的表征向量。这样学习到的表征可以被用于下游任务。在情感预测任务上，递归自动编码器达到了当时的 state-of-the-art。递归自动编码器最大的创新之处是加入了独特的层次结构，并使用语义的组合来理解情感。递归自动编码器既可以在没有标签的领域数据上，也可以在有标签的情感数据上进行训练。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/Recursive-Autoencoder.png"></p><h3 id="传统递归自动编码器"><a href="#传统递归自动编码器" class="headerlink" title="传统递归自动编码器"></a>传统递归自动编码器</h3><p>传统递归自动编码器要求在以一组词向量的形式给出要输入的句子 $(x_1, x_2, \dots, x_m)$ 的同时，给出句子的二叉树形结构，这个树形结构通过一组父节点和两个子节点的三元组表示：$(p \rightarrow c_{1} c_{2})$。例如在图中，我们有 $((y_{1} \rightarrow x_{1} x_{2}), (y_{2} \rightarrow y_{1} x_{3}), \dots)$ 等三元组。因为对于每组子节点，我们使用同一个神经网络来处理，因此 $y_i$ 的维度和 $x_i$ 相同。</p><p>对于每个三元组 $(p \rightarrow c_{1} c_{2})$，我们通过公示：</p><p>$$p=f\left(W^{(1)}\left[c_{1} ; c_{2}\right]+b^{(1)}\right),$$</p><p>来得到父节点 $p$ 的表征。注意 $f(\cdot)$ 表示非线性的激活函数。为了评估父节点 $p$ 的表征有多好，我们从父节点的表征重建其两个子节点的表征（图中橙色的节点）：</p><p>$$\left[c_{1}^{\prime} ; c_{2}^{\prime}\right]=W^{(2)} p+b^{(2)}$$</p><p>在训练过程中，我们的目标就是最小化这个重建的误差。对于每个子节点对，我们使用欧式距离来计算误差：</p><p>$$E_{r e c}\left(\left[c_{1} ; c_{2}\right]\right)=\frac{1}{2}\left|\left[c_{1} ; c_{2}\right]-\left[c_{1}^{\prime} ; c_{2}^{\prime}\right]\right|^{2}$$</p><p>我们只需要自底向上来重复以上步骤即可。</p><h3 id="用于预测结构的无监督递归自动编码器"><a href="#用于预测结构的无监督递归自动编码器" class="headerlink" title="用于预测结构的无监督递归自动编码器"></a>用于预测结构的无监督递归自动编码器</h3><p>我们可以从三个方面来优化传统的递归自动编码器。</p><p>第一点是，对于输入的句子 $x$，不再需要给出其树形结构，而是自己构建。方法是使用贪婪算法在每一步去尝试每个可能选择的子节点，最后选择这一步重构损失最低的方式构建整棵树。</p><p>第二点是，之前采用的重构损失是平均的惩罚所有父节点的子节点重构损失。一个改进是，对于更上层的、包含更多单词的子节点对应的重构损失，给一个更高的权重，因为它对这个句子的表征的影响更大。例如，在重构时，设 $n_1$ 和 $n_2$ 是潜在子节点 $c_1$ 和 $c_2$ 包含的单词数量，则重新定义重构损失为</p><p>$$E_{r e c}\left(\left[c_{1} ; c_{2}\right] ; \theta\right)= \frac{n_{1}}{n_{1}+n_{2}}\left|c_{1}-c_{1}^{\prime}\right|^{2}+\frac{n_{2}}{n_{1}+n_{2}}| | c_{2}-c_{2}^{\prime}| |^{2}$$</p><p>第三点是，由于每次得到的父节点表征会马上被用于重建，因此为了降低重构误差，机器会倾向于学习值非常小的表征。为了避免这个现象出现，在每次计算得到父节点的表征 $p$ 时，可以做一个正则化 $p = \frac{p}{| p |}$。</p><h3 id="半监督递归自动编码器"><a href="#半监督递归自动编码器" class="headerlink" title="半监督递归自动编码器"></a>半监督递归自动编码器</h3><p>到现在为止，递归自动编码器已经可以无监督地生成捕捉了句子的语义结构的表征。我们将这个表征输入到下游任务的模型中，这样，可以通过下游任务的训练来进行微调，从而使模型学习在表征中包含更多对下游任务更为重要的信息。</p><h2 id="Additional-Stacked-Denoising-Autoencoder"><a href="#Additional-Stacked-Denoising-Autoencoder" class="headerlink" title="Additional Stacked Denoising Autoencoder"></a>Additional Stacked Denoising Autoencoder</h2><p>接下来我们来到推荐系统领域，介绍在论文 “A Hybrid Collaborative Filtering Model with Deep Structure for Recommender Systems”[2] 被提出的 <strong>Additional Stacked Denoising Autoencoder（aSDAE）</strong>。如果一定要翻译的话，这个模型应该称为“有附加信息的堆叠去噪自动编码器”，因为这个名字有点太长了，因此我们在接下来的叙述中用 aSDAE 来代指这个自动编码器。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/aDAE-aSDAE.png"></p><p>其实 aSDAE 的思想非常简单，就是在堆叠去噪自动编码器的基础上，受到 Seq2Seq 模型的启发，在每一层接受一个相同的、对应输入的附加信息（side information），然后在自动编码器的最后同时重构输入和附加信息。</p><p>用公式表示 aSDAE 模型。对于 aSDAE 的隐藏层 $l \in { 1, \dots, L-1 }$，其隐藏表征 $h_l$ 的计算方法为</p><p>$$h_{l}=g\left(W_{l} h_{l-1}+V_{l} \tilde{x}+b_{l}\right),$$</p><p>其中，$h_0 = \tilde{s}$ 是用噪声处理过的输入，$\tilde{x}$ 是附加信息，$f(\cdot)$ 和 $g(\cdot)$ 都是激活函数。</p><p>对于 aSDAE 的输出层 $L$，输出的计算为</p><p>$$\begin{aligned} \hat &amp;=f\left({W}_{L} {h}_{L}+{b}_{\hat{s}}\right) \\ \hat &amp;=f\left({V}_{L} {h}_{L}+{b}_{\hat{x}}\right) \end{aligned}$$</p><p>因此，aSDAE 的损失函数为</p><p>$$L = \alpha|{s}-\hat|_{F}^{2}+(1-\alpha)|{x}-\hat|_{F}^{2} + \lambda\left(\sum_{l}\left|{W}_{l}\right|_{F}^{2}+\left|{V}_{l}\right|_{F}^{2}\right),$$</p><p>其中，$\alpha$ 是一个用于保持平衡的超参数，$\lambda$ 是正则化参数。</p><p>aSDAE 本身就介绍完了。在论文 [2] 中，aSDAE 学习用户和商品对应的隐向量矩阵，并将用户的基本个人信息、用户画像信息、物品的基本信息等附加信息引入，从而解决协同过滤中的稀疏性与冷启动问题。这些和推荐系统相关的背景知识就不多讨论了，有兴趣的同学可以看参考资料 [3]。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/hybrid-collaborative-filtering-model-with-aSDAE.png"></p><h2 id="More-Autoencoder"><a href="#More-Autoencoder" class="headerlink" title="More Autoencoder"></a>More Autoencoder</h2><p>随着对自动编码器的深入了解，我们有两个问题可以讨论：</p><ol><li>为什么要最小化重构误差？有没有其他做法？</li><li>怎么让自动编码器学习到的表征更具可解释性？</li></ol><p>从这两个问题引入，李宏毅老师在他的机器学习课程上又对自动编码器的一些最新进展进行了介绍和讨论。有兴趣了解的同学可以查看我的《李宏毅机器学习》课程笔记：<a href="http://kyonhuang.top/Hung-yi-Lee-ML-notes/#/More-Autoencoder">More Autoencoder - 《李宏毅机器学习》课程笔记</a>，在此就不再重复写作了。而在“自动编码器一览”系列的下一篇中，我们会来看看可能是近年来的顶会上出现最多的自动编码器的一种形式——变分自动编码器。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>[1] Richard Socher, Jeffrey Pennington, Eric H.Huang, Andrew Y.Ng, Christopher D.Manning. “Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions”, EMNLP 2011</li><li>[2] Xin Dong, Lei Yu, Zhonghuo Wu, Yuxia Sun, Lingfeng Yuan, Fangxi Zhang. “A Hybrid Collaborative Filtering Model with Deep Structure for Recommender Systems”, AAAI 2017 </li><li>[3] 推荐系统中基于深度学习的混合协同过滤模型. <a href="https://zhuanlan.zhihu.com/p/25234865">https://zhuanlan.zhihu.com/p/25234865</a></li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是“自动编码器一览”系列的第二篇博文。在本文中，我会介绍一些最近看过并且比较感兴趣的自动编码器的变形，说是介绍，可能更像是论文笔记合集。比起第一篇提到过那些更为通用的经典变形，这些相对而言比较新颖的自动编码器在结构上进行修改，使得它们更符合所对应的特定任务的需求。如果我之后又看到一些有意思的自动编码器，也可能在这篇博文中继续更新。&lt;/p&gt;
&lt;p&gt;本文中包含以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;递归自动编码器（Recursive Autoencoder）&lt;/li&gt;
&lt;li&gt;Additional Stacked Denoising Autoencoder（aSDAE）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“自动编码器一览”系列：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/autoencoder-intro-1/&quot;&gt;自动编码器一览（一）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/autoencoder-intro-2/&quot;&gt;自动编码器一览（二）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/autoencoder-intro-3/&quot;&gt;自动编码器一览（三）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="深度学习笔记" scheme="https://kyonhuang.top/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://kyonhuang.top/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="自动编码器" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
    <category term="Autoencoder" scheme="https://kyonhuang.top/blog/tags/Autoencoder/"/>
    
    <category term="表征学习" scheme="https://kyonhuang.top/blog/tags/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Representation Learning" scheme="https://kyonhuang.top/blog/tags/Representation-Learning/"/>
    
    <category term="无监督学习" scheme="https://kyonhuang.top/blog/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>自动编码器一览（一）</title>
    <link href="https://kyonhuang.top/blog/autoencoder-intro-1/"/>
    <id>https://kyonhuang.top/blog/autoencoder-intro-1/</id>
    <published>2019-06-15T03:13:39.000Z</published>
    <updated>2019-07-10T01:35:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/autoencoder-roadmap.png"></p><p>最近在研究用自动编码器（Autoencoder, AE）做表征学习（Representation Learning），看了一些资料和论文，自己也有一些想法，现在准备开始写 related work，值此机会想要先整理一下。题图来自 THU 唐杰老师的微博，清晰地展现了自动编码器的发展历程。</p><p>本系列预计会写三篇博文，第一篇包含自动编码器的总体框架和思想，之后介绍为实现各种目标而产生的一些经典变体，会涵盖题图中提到的大部分自动编码器；第二篇包含一些我个人比较感兴趣、最近看过的一些变形，它们更适用于各自的任务；第三篇可能会拖得稍微久一点，我想把最近非常火热的变分自动编码器以及它的一些变形弄通。</p><p>本文中包含以下内容：</p><ul><li>表征学习简介（Representation Learning Intro）</li><li>自动编码器框架（Autoencoder Framework）</li><li>堆叠自动编码器（Stacked Autoencoder）</li><li>去噪自动编码器（Denoising Autoencoder, DAE）</li><li>收缩自动编码器（Contractive AutoEncoder, CAE）</li><li>稀疏自动编码器（Sparse Autoencoder）</li><li>LSTM 自动编码器（LSTM Autoencoder）</li><li>卷积自动编码器（Convolutional Autoencoder）</li></ul><p>“自动编码器一览”系列：</p><ul><li><a href="http://kyonhuang.top/autoencoder-intro-1/">自动编码器一览（一）</a></li><li><a href="http://kyonhuang.top/autoencoder-intro-2/">自动编码器一览（二）</a></li><li><a href="http://kyonhuang.top/autoencoder-intro-3/">自动编码器一览（三）</a></li></ul><span id="more"></span><h2 id="表征学习"><a href="#表征学习" class="headerlink" title="表征学习"></a>表征学习</h2><p>在开始介绍自动编码器之前，我们首先对<strong>表征学习（Representation Learning，也称表示学习）</strong>做一个简单的介绍，帮助我们在之后更好地理解自动编码器架构设计的目的。</p><p>表征学习（或者称表示学习）指从原始数据中自动学习有效的<strong>特征</strong>（或者更一般性称为<strong>表征</strong>），并提高最终机器学习模型的性能。表征学习目前已经成为机器学习社区的热点之一，在每届 NIPS、ICML、ICLR 等会议都有较多的相关论文以及 workshops。</p><p>表征学习的关键是解决<strong>语义鸿沟（Semantic Gap）</strong>问题。语义鸿沟问题是指输入数据的底层特征和高层的语义信息之间的不一致性和差异性。比如图片分类任务，图片中每个物体的颜色和形状等属性都不尽相同，不同图片在像素级别上的表示（即底层特征）差异性也会非常大。人类理解这些图片是建立在比较抽象的高层语义概念上的，因此如果让一个分类模型直接建立在底层特征之上，会对模型的能力要求过高。如果有一个好的表示在某种程度上可以反映出数据的高层语义特征，那么我们就可以相对容易地构建后续的机器学习模型 [1]。</p><p>考虑到直接将高维度的原始数据输入到各类算法中会导致计算量非常大，因此，表征学习最大的目标之一就是在尽量保留对后续任务有用的信息的同时，对数据进行降维。比起进行线性降维的 PCA，自动编码器可以进行非线性的降维。同时，一个好的表征应该比较稀疏（用极可能少的资源表示尽可能多的信息，计算速度快）、对噪声不敏感（学习到的表征尽量不受原始数据中噪声的影响）、具有一般性，是任务或领域独立的（学习到的表征能够尽可能在更多任务上取得较好效果）、具有较好的可重构性、具有很强的表示能力（同样大小的向量可以表示更多信息）等等。</p><p>根据 [2] 所述，对表征学习的研究大致分为两类，一类基于概率图模型，另一类基于神经网络。这两类研究的根本区别在于，深度学习模型的层级结构被解释为对概率图模型的描述还是对计算图的描述。更简单地说，区别在于深度学习模型的隐藏单元是被视为潜在的随机变量还是计算节点。其中，自动编码器是后者的代表之一。</p><h2 id="自动编码器框架"><a href="#自动编码器框架" class="headerlink" title="自动编码器框架"></a>自动编码器框架</h2><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/autoencoder_framework.png"></p><p>在这里，我们使用“自动编码器框架”来代指最基本的自动编码器架构和指导思想，以与单个自动编码器进行叙述上的区分。</p><p>自动编码器框架包含两个参数化函数。第一个函数 $f_{\theta}$ 称为<strong>编码器（encoder）</strong>，它从原始输入 $x$ 中提取特征向量 $h = f_{\theta}(x)$。第二个函数 $g_{\theta}$ 称为<strong>解码器（decoder）</strong>，它是一个从特征空间到输入空间的映射，产生一个<strong>重构（reconstruction）</strong> $r = g_{\theta}(h)$。通过让解码器产生的重构与作为输入的原始数据尽可能相似，即尝试在训练集上获得最小的重构损失 $L(x,r)$，编码器和解码器的参数组 $\theta$ 同时通过训练得到。具有较好泛化性意味着能够在测试集也取得较低的重构损失，而编码器所提取的特征向量即自动编码器框架学习得到的<strong>表征</strong>。</p><p>总结一下，自动编码器框架的训练目的是为了找到一组参数 $\theta$ 的值，使得重构损失最小。最经典的自动编码器使用多层感知器（Multi-Layer Perceptrons, MLPs）作为编码器和解码器，其常用的形式如下：</p><p>$$f_{\theta}(x) = s_f(Wx + b)$$</p><p>$$g_{\theta}(h) = s_g(W^{‘}h + b^{‘})$$</p><p>其中，$s_f$ 和 $s_g$ 是编码器和解码器所使用的激活函数，一般为非线性的 sigmoid、Relu 函数等。因此，模型整体的参数组 $\theta = { W, b, W^{‘}, b^{‘} }$，其中 $W$ 和 $W^{‘}$ 是编码器和解码器的权重矩阵，$b$ 和 $b^{‘}$ 是编码器和解码器的偏置向量。由于我们要求原始输入和输出尽可能相似，编码器和解码器可能最终被训练为恒等映射，不符合我们的要求，因此通常对表征做一定的约束。</p><p>由于自动编码器框架将原始输入同时作为模型的输入和输出，因此无需使用有标签的数据进行训练，这种无监督学习的方式使得自动编码器的通用性大为提升。其实早在 1988 年，自动编码器的思想就已经被提出，但是由于数据的稀疏高维，使得当时的模型很难优化，因此直到 2006 年，Hinton 采用梯度下降算法来逐层优化受限玻尔兹曼机（Restricted Boltzmann Machine，RBM）以获得对原始样本的抽象表示并取得显著效果后，自动编码器才得到广泛的关注。</p><h2 id="堆叠自动编码器"><a href="#堆叠自动编码器" class="headerlink" title="堆叠自动编码器"></a>堆叠自动编码器</h2><p>可以看到，最基本的自动编码器框架是一个简单的三层结构，包含输入层、隐藏层（中间层）和输出层。其中，输出层的存在是为了将原始数据作为假想的目标输出，以构建监督误差来训练整个网络。当训练结束后，我们就不再关心输出层，只关心从输入到中间层的映射。</p><p>考虑到深度神经网络是在逐层的学习数据的表征，在底层构建较为简单的特征，以此为基础在较高层构建更为复杂和抽象的表征，我们很容易想到，将最基本的自动编码器学习到的特征 $h$ 作为原始信息，再训练一个新的自动编码器，来得到新的特征表达。这就是论文 “Greedy Layer-Wise Training of Deep Networks” [3] 所提出的<strong>堆叠自动编码器（Stacked Autoencoder，又称栈式自动编码器）</strong>的基本思想。通过堆叠自动编码器学习到的高层表征更加抽象，具有更好的扩展性。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/Stacked-autoencoder.png"></p><p>显然，堆叠自动编码器的训练和普通的多层神经网络不同，不是每一遍逐层进行，而是首先训练得到最底层的自动编码器，得到其编码器函数，然后再训练更高层的自动编码器，最后堆叠得到整个堆叠自动编码器。因此，除开进行无监督表征学习，堆叠自动编码器也可以用来进行<strong>深度神经网络的初始化</strong>，在通过无监督学习得到堆叠自动编码器后可以在最后一个隐藏层后，根据具体任务加上一个输出层，并通过监督学习的方式，利用梯度下降对整个模型进行全局微调。这就是<strong>逐层非监督预训练（layer-wise unsuperwised pre-training）</strong>。</p><h2 id="去噪自动编码器"><a href="#去噪自动编码器" class="headerlink" title="去噪自动编码器"></a>去噪自动编码器</h2><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/denoising_autoencoder.png"></p><p>考虑到自动编码器框架中的编码器和解码器本质上还是神经网络，因此，当神经网络的层数加深、结构愈发复杂而使参数增加时，自动编码器一样面临着过拟合的风险。在 ICML 2008 上，论文 “Extracting and composing robust features with denoising autoencoders” [4] 首次提出去噪自动编码器（Denoising Autoencoder, DAE）模型。为了强制让隐藏层学习到更加健壮的特征，去噪自动编码器 DAE 在输入层对原始输入加入随机噪声。</p><p>为了和图中符号匹配，请注意我们在本节中使用原论文 [4] 中的符号表示进行叙述，可能与前文中符号不同。去噪自动编码器 DAE 的描述如下：</p><ol><li>设有纯净输入（clean input）$\mathbf{x} \in [0, 1]^d$，通过部分损坏纯净输入得到损坏输入（corrupted input）$\tilde{\mathbf{x}} \sim q_{\mathcal{D}}(\tilde{\mathbf{x}} | \mathbf{x})$。</li><li>原论文中的输入损坏过程 $q_{\mathcal{D}}(\tilde{\mathbf{x}} | \mathbf{x})$ 是随机选择纯净输入 $\mathbf{x}$ 中固定比例的一部分（一般不超过一半），将这些部分的值设置为 0。当然可以使用其他方法来损坏数据，例如引入高斯噪声。</li><li>编码器函数 $f_{\theta}$ 将损坏输入 $\tilde{\mathbf{x}}$ 映射到隐藏表征 $\mathbf{y} = f_{\theta}(\tilde{\mathbf{x}})$。</li><li>解码器函数 $g_{\theta^{‘}}$ 将 $\mathbf{y}$ 重构为 $\mathbf{z} = g_{\theta^{‘}}(\mathbf{y})$。</li><li>通过训练来最小化交叉熵重构损失 $L_{H}(\mathbf{x}, \mathbf{z})=-\sum_{k=1}^{d}\left[\mathbf{x}_{k} \log \mathbf{z}_{k}+\left(1-\mathbf{x}_{k}\right) \log \left(1-\mathbf{z}_{k}\right)\right]$。</li></ol><p>可以看到，DAE 有两个功能：一是对输入进行编码，即保留输入中的有关信息；二是尝试消除应用于输入的随机损坏过程的影响。其中，后者只能通过捕捉输入之间的统计依赖关系来完成。也就是说，DAE 试图从未损坏的值中预测随机损坏的值。注意到，能够从其余变量中预测任何变量子集是完全捕获一组变量之间的联合分布的充分条件，这也是 Gibbs 采样的工作原理 [5]。可以看出，这里提到的随机损坏过程与目前已经在训练神经网络时广泛使用的 Dropout 非常相似（然而，DAE 的提出时间要早于 Dropout）。</p><p>原论文同时通过流形学习、信息论、生成模型等多种角度对 DAE 进行解释。其中，从流形学习（manifold learning）视角来看，如下图所示，DAE 可以看作是一种学习一个流形的方式。假设训练数据（下图中标<code>x</code> 的数据）聚集在一个低维流形的周围，那么损坏数据（下图中标<code>.</code>的数据）通过损坏过程得到，它们距离流形较远。因此，整个模型试图将损坏数据投影回流形上，这样的模型更加稳健。而中间层表征可以被解释为流形上点所在的坐标系。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/DAE-in-manifold-learning-perspective.png" alt="DAE-in-manifold-learning-perspective"></p><h2 id="收缩自动编码器"><a href="#收缩自动编码器" class="headerlink" title="收缩自动编码器"></a>收缩自动编码器</h2><p>同样是为了抑制过拟合，<strong>收缩自动编码器（Contractive AutoEncoder, CAE）</strong>[6] 采用的方法是与正则化相结合，在损失函数中加入编码器的雅克比（Jacobian）矩阵范式来约束。</p><p>$$\mathcal{J}_{\mathrm{CAE}}(\theta)=\sum_{x \in D_{n}}\left(L(x, g(f(x)))+\lambda\left\|J_{f}(x)\right\|_{F}^{2}\right)$$</p><p>上式是 CAE 的损失函数，可以看到其分为两部分，第一部分是原始自动编码器的损失函数，用于最小化重构误差；而第二部分是 F 范式下的雅克比矩阵，如下：</p><p>$$\left\|J_{f}(x)\right\|_{F}^{2}=\sum_{i j}\left(\frac{\partial h_{j}(x)}{\partial x_{i}}\right)^{2}$$</p><p>想要最小化损失函数，就要使得偏导尽可能小，从而使模型对局部的抖动具有健壮性。这样，在第一部分保留最具代表性的特征信息的同时，第二部分倾向于丢掉所有特征信息，两者的共同作用即是<strong>只</strong>保留具有代表性的好的特征信息。</p><p>和 DAE 相比，CAE 主要挖掘样本内在的特征，使用样本本身的梯度信息，而不考虑样本中未出现的情况；而 DAE 对输入加入了噪声，改变了原数据的分布，因此对样本中未出现的测试数据同样具有健壮性。同时，DAE 的实现比 CAE 要简单，无需计算隐藏层的雅克比矩阵。</p><h2 id="稀疏自动编码器"><a href="#稀疏自动编码器" class="headerlink" title="稀疏自动编码器"></a>稀疏自动编码器</h2><p>自动编码器除了可以学习低维表征外，也可以学习高维的稀疏表征。为了学习得到稀疏的表征，<strong>稀疏自动编码器（Sparse Autoencoder）</strong>在原来的损失函数中加入了一个控制稀疏化的正则项。最常见的思路是使用 L1 范数（对所有输出简单求和）来作为度量表征稀疏性的正则项，然而少有稀疏自动编码器的相关论文采用这种方法。</p><p>由于神经网络的稀疏性还可以被解释为单个神经元被激活的概率很小，因此我们先指定一个稀疏性参数 $\rho$ 来代表隐藏层神经元的平均激活程度，$\rho$ 是一个很小的值（例如 0.05）。这样，我们就可以引入一个度量来衡量神经元的实际激活度 $\hat \rho$ 与期望激活度 $\rho$ 之间的差异，然后将这个度量作为惩罚项即可。一般我们会选择 KL 散度（Kullback-Liebler Divergence）来作为度量，则有</p><p>$$\mathrm{KL}\left(\rho \| \hat{\rho}_{j}\right)=\rho \log \frac{\rho}{\hat{\rho}_{j}}+(1-\rho) \log \frac{1-\rho}{1-\hat{\rho}_{j}}$$</p><p>损失函数为</p><p>$$\mathcal{J}_{\mathrm{sparse}}(\theta)=\mathcal{J}_{\mathrm{AE}}(\theta)+\beta \sum_{j=1}^{n} \mathrm{KL}\left(\rho \| \hat{\rho}_{j}\right)$$</p><p>稀疏自动编码器的优点是有很高的可解释性，同时进行了隐式的特征选择。</p><h2 id="LSTM-自动编码器-amp-卷积自动编码器"><a href="#LSTM-自动编码器-amp-卷积自动编码器" class="headerlink" title="LSTM 自动编码器 &amp; 卷积自动编码器"></a>LSTM 自动编码器 &amp; 卷积自动编码器</h2><p>在自动编码器的后续发展中，人们也开始想到，编码器和解码器不一定要表示为多层感知器。因此，自动编码器开始和各具特色的其他常用神经网络结构相结合，从而得到卷积自动编码器（Convolutional Autoencoder）、LSTM 自动编码器（LSTM Autoencoder）等模型。单纯的神经网络和与神经网络相结合的自动编码器的主要区别在于，单纯的神经网络通常用于监督学习的特定任务上，经过端到端的训练来学习内部的权重，提取的特征与任务紧密结合；而与神经网络相结合的自动编码器学习得到能够尽可能重建输入的结构权重，这样提取的特征可以应用于更多的任务。</p><p>在计算机视觉的各种任务中，精心设计的卷积自动编码器已经成为各大模型不可或缺的部分。不管结构如何改变，其核心思想还是先通过多个卷积层（以及池化层）将高维图片降维得到向量表示，之后将反卷积网络（Deconvolution Network）组成解码器，来将特征向量重构为输入。想进一步了解反卷积可见 [8]。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/Convolutional-AutoEncoder.png"></p><p>本身适用于各类序列建模的 LSTM 在与自动编码器结合后，更是焕发了强大的活力。以学习句子的表征为例，在下图中，作为编码器的 LSTM 的每一步接受一个单词（的 embedding）和上一步的隐藏状态作为输入。当最后一个单词被输入时，对应的神经元的隐藏状态作为表征，输入到作为解码器的 LSTM 中。在解码器中，每一个单元将上一步的隐藏状态和输出（也可以是输出对应的标签）作为输入，通过 softmax 层预测对应的词并输出，从而重构出输入的句子。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/LSTM-AE-Seq2Seq.png"></p><p>实际上，对于 NLP 略有所知的同学们应该早已看出，将输出目标从原始输入换成监督学习任务中的有标注数据，LSTM 自动编码器有一个更为响亮的名字：Seq2Seq（Sequence to Sequence）。Seq2Seq 结构存在两个问题，一是将整个句子的信息压缩为一个低维向量，会造成信息损失；二是句子如果过长，长程依赖很难被完全学习，从而使得准确率下降。因此，人们把 Seq2Seq 结构与注意力机制（Attention Mechanism）相结合，由此推动了 NLP 领域的蓬勃发展。当然，这与本篇博客的主要内容已相去甚远，因而不再详述。</p><p>可以看出，将常用神经网络结构与自动编码器相结合其实更多是一个大概的思想，在具体实现时，编码器和解码器可以自由变换，例如 LSTM 自动编码器中可以将 LSTM 换成 GRU，或者将单向 LSTM 换为双向。总而言之，两者的结合使得双方都获得了新的可能性。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>[1] 邱锡鹏. 神经网络与深度学习. <a href="https://nndl.github.io/">https://nndl.github.io/</a></li><li>[2] Yoshua Bengio, Aaron C. Courville, Pascal Vincent. “Representation Learning: A Review and New Perspectives”, IEEE Trans. Pattern Anal. Mach. Intell., 2013</li><li>[3] Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle. “Greedy Layer-Wise Training of Deep Networks”, NIPS 2006</li><li>[4] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol. “Extracting and composing robust features with denoising autoencoders”, ICML 2008</li><li>[5] Denoising Autoencoders (dA) — DeepLearning 0.1 documentation. <a href="http://deeplearning.net/tutorial/dA.html">http://deeplearning.net/tutorial/dA.html</a></li><li>[6] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, Yoshua Bengio. “<a href="http://www.icml-2011.org/papers/455_icmlpaper.pdf">Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</a>“, ICML 2011</li><li>[7] Convolutional Autoencoders – P. Galeone’s blog. <a href="https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/">https://pgaleone.eu/neural-networks/2016/11/24/convolutional-autoencoders/</a></li><li>[8] 反卷积(Transposed Convolution, Fractionally Strided Convolution or Deconvolution). <a href="https://blog.csdn.net/kekong0713/article/details/68941498">https://blog.csdn.net/kekong0713/article/details/68941498</a></li></ul><h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/68230198">机器学习中表征问题里的各种 trade-off - 知乎</a></li><li>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol. “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion”, Journal of Machine Learning Research, 2010</li><li>Jiwei Li, Minh-Thang Luong, Dan Jurafsky. “A Hierarchical Neural Autoencoder for Paragraphs and Documents”, ACL 2015</li><li><a href="https://blog.csdn.net/luohenyj/article/details/78394060">[深度学习]Contractive Autoencoder - 落痕月极的博客 - CSDN博客</a></li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/autoencoder-roadmap.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;最近在研究用自动编码器（Autoencoder, AE）做表征学习（Representation Learning），看了一些资料和论文，自己也有一些想法，现在准备开始写 related work，值此机会想要先整理一下。题图来自 THU 唐杰老师的微博，清晰地展现了自动编码器的发展历程。&lt;/p&gt;
&lt;p&gt;本系列预计会写三篇博文，第一篇包含自动编码器的总体框架和思想，之后介绍为实现各种目标而产生的一些经典变体，会涵盖题图中提到的大部分自动编码器；第二篇包含一些我个人比较感兴趣、最近看过的一些变形，它们更适用于各自的任务；第三篇可能会拖得稍微久一点，我想把最近非常火热的变分自动编码器以及它的一些变形弄通。&lt;/p&gt;
&lt;p&gt;本文中包含以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表征学习简介（Representation Learning Intro）&lt;/li&gt;
&lt;li&gt;自动编码器框架（Autoencoder Framework）&lt;/li&gt;
&lt;li&gt;堆叠自动编码器（Stacked Autoencoder）&lt;/li&gt;
&lt;li&gt;去噪自动编码器（Denoising Autoencoder, DAE）&lt;/li&gt;
&lt;li&gt;收缩自动编码器（Contractive AutoEncoder, CAE）&lt;/li&gt;
&lt;li&gt;稀疏自动编码器（Sparse Autoencoder）&lt;/li&gt;
&lt;li&gt;LSTM 自动编码器（LSTM Autoencoder）&lt;/li&gt;
&lt;li&gt;卷积自动编码器（Convolutional Autoencoder）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“自动编码器一览”系列：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/autoencoder-intro-1/&quot;&gt;自动编码器一览（一）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/autoencoder-intro-2/&quot;&gt;自动编码器一览（二）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/autoencoder-intro-3/&quot;&gt;自动编码器一览（三）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="深度学习笔记" scheme="https://kyonhuang.top/blog/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://kyonhuang.top/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="自动编码器" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
    <category term="Autoencoder" scheme="https://kyonhuang.top/blog/tags/Autoencoder/"/>
    
    <category term="表征学习" scheme="https://kyonhuang.top/blog/tags/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Representation Learning" scheme="https://kyonhuang.top/blog/tags/Representation-Learning/"/>
    
    <category term="无监督学习" scheme="https://kyonhuang.top/blog/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>记杨强老师关于联邦学习的讲座</title>
    <link href="https://kyonhuang.top/blog/whu-federated-learning-talk/"/>
    <id>https://kyonhuang.top/blog/whu-federated-learning-talk/</id>
    <published>2019-05-11T03:12:24.000Z</published>
    <updated>2019-05-11T03:12:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>5 月 6 日刚从杭州回到武大准备毕业答辩，得知赫赫有名的杨强老师第二天将来武大，介绍他带领的微众银行人工智能团队在联邦学习上的一些工作和思考。我们实验室的研究方向包括迁移学习的应用，而杨强老师是迁移学习领域的领头人，因此一定是要前往聆听的。老师的讲座内容没有很多算法模型上的硬核知识，但也非常有宏观上的启发性。下面，我基于讲座的内容笔记及个人思考做一个整理。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/whu-yangqiang.jpg"></p><span id="more"></span><h2 id="杨强《人工智能，数据孤岛和联邦学习》"><a href="#杨强《人工智能，数据孤岛和联邦学习》" class="headerlink" title="杨强《人工智能，数据孤岛和联邦学习》"></a>杨强《人工智能，数据孤岛和联邦学习》</h2><p>目前的 AI 系统在包括有偏性、和人类的合作、可解释性、可靠性、可信性、公平性等方面存在很多问题，其中一些问题的根因是数据的缺失和数据量的不足。杨强老师指出，目前的时代是“数据孤岛时代”，以法律、金融、医疗为首的各行业各公司都只拥有小数据。数据共享是一个容易被想到的解决方案，但其存在难度高、不合规、利益相关等问题，使得不但同一行业不同公司之间难以展开合作，甚至连同一公司各部门之间都存在“数据墙”。</p><p>为了解决数据量不足的固有问题，杨强老师在过去的近十年内致力研究大数据迁移到小数据，即<strong>迁移学习（Transfer Learning）</strong>。因此，杨强老师首先简单回顾了迁移学习，其中有这些结论值得注意：1）迁移学习有助于实现鲁棒性（同样的输入对应同样的输出）和边缘计算（模型的计算放在终端进行）。2）迁移学习的本质是找到不同领域数据的不变量。3）在进行模型迁移时，低层特征更通用和可迁移，高层特征更特定和不可迁移。4）当两个领域相距很远时，可以进行传递性迁移学习（[Distant Domain Transfer Learning, AAAI 17], [Transitive Transfer Learning, KDD 15]）。</p><p>在从学校转投工业界，发现上述工业界亟待解决的问题后，杨强老师提出了<strong>联邦学习（Federated Learning）</strong>的概念，致力于将拥有细碎数据的各控制方可以参与联合建立 AI 模型，并协作使用模型来进行决策。在这个过程中，各方数据不出本地，因此互相之间看不到彼此的真实数据，不会违反保护个人数据隐私的相关法案，并且能够保证数据的安全和各方的利益。同时，我们力求模型的效果不会受到损失（LOSSLESS）。</p><p>联邦学习的一个典型流程是<code>终端本地训练 -&gt; 秘钥加密梯度 -&gt; 上传云端 -&gt; 更新模型</code>。例如，将手机终端存储的特征维度相同的用户数据在本地训练，然后将梯度等信息加密后，上传到唯一的中心，在云端中心用所有终端的信息更新模型后，手机终端再下载更新的模型。这个流程的缺点是过程比较漫长。</p><p>除了如上述流程所示的安全模型建立，联邦学习还试图解决很多为了保护用户隐私而导致的问题，例如隐私保护下的样本 ID 匹配，如何能够在各家银行互相不泄露用户信息的前提下，找到同时在多家银行借贷而没有偿还能力的人。</p><p>联邦学习的研究子方向包括但不限于：</p><ul><li><p>系统效率</p><ul><li>模型压缩（Compression）</li><li>算法优化（Optimization algorithms）</li><li>参与方选取（Client selection）</li><li>边缘计算（Resource constraint, IoT, Edge computing）</li></ul></li><li><p>模型效果</p><ul><li>数据分布不均匀（Data distribution and selection）</li><li>个性化（Personalization）</li></ul></li><li><p>数据安全 </p></li></ul><p>可以看到，作为一个工业界实际需求催生的研究方向，联邦学习所涉及的领域实际上不只有机器学习，还包括密码学、网络安全等。例如，联邦学习联盟的激励机制需要用到博弈论，来讨论各方如何合理分配通过联邦学习共同收获的利益。目前，IEEE 正在建立纵向联邦学习标准，相信也有很多不限于人工智能领域的专家会参与到这个过程中。更多有关联邦学习的内容可以访问 <a href="https://www.fedai.org/">https://www.fedai.org</a>。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/federated-learning-ecological-construction.jpg"></p><p>以上是我对杨强老师讲座内容的总结。在提问环节，杨强老师对于科研起步者如何入门的问题做了简单解答。他认为，在研究开始时，应该找一个足够窄的领域中的一个足够清晰的问题，在走完一个完整的科研周期后<strong>成功</strong>发表自己的第一篇论文。他把这个过程比喻为找一个足够弱的敌人，打一场完整的胜仗；之后，就应该开始寻找当研究生毕业时什么领域会热门，而不是盲目投身现在大热的领域。找到未来的热门领域可能很难，需要多和别人交流和分析。在未来热门但现在可能是一片荒地的领域开荒也很难，杨强老师认为需要将在该领域的研究过程划分阶段，在每个小阶段取得成功后，将其变为故事能讲给同行学者、业界专家和普通人群。另外老师提出，能在人工智能行业取得成功的人需要懂人工智能、有商业头脑、有管理能力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;5 月 6 日刚从杭州回到武大准备毕业答辩，得知赫赫有名的杨强老师第二天将来武大，介绍他带领的微众银行人工智能团队在联邦学习上的一些工作和思考。我们实验室的研究方向包括迁移学习的应用，而杨强老师是迁移学习领域的领头人，因此一定是要前往聆听的。老师的讲座内容没有很多算法模型上的硬核知识，但也非常有宏观上的启发性。下面，我基于讲座的内容笔记及个人思考做一个整理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bighuang624/pic-repo/master/whu-yangqiang.jpg&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="讲座心得记录" scheme="https://kyonhuang.top/blog/categories/%E8%AE%B2%E5%BA%A7%E5%BF%83%E5%BE%97%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="人工智能" scheme="https://kyonhuang.top/blog/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="讲座" scheme="https://kyonhuang.top/blog/tags/%E8%AE%B2%E5%BA%A7/"/>
    
    <category term="迁移学习" scheme="https://kyonhuang.top/blog/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="联邦学习" scheme="https://kyonhuang.top/blog/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>【译】理解 LSTM</title>
    <link href="https://kyonhuang.top/blog/Understanding-LSTMs/"/>
    <id>https://kyonhuang.top/blog/Understanding-LSTMs/</id>
    <published>2018-12-18T02:57:32.000Z</published>
    <updated>2018-12-19T08:23:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>一直以来感觉自己对 LSTM 的理解缺了点什么。这次看到一篇不错的博客 <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>，对 LSTM 的机制和公式有逐条的解释。正好最近手头的工作也和门机制有一定联系，因此决定翻译一下，帮助自己理解和记忆公式。主要靠意译，省略了一点无关紧要的内容，有条件可以自己看看原文。</p><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>人类不是每分每秒都从头开始思考，而是基于所学所想进一步思考，有一定的持续性。RNN 可以通过前一个单元将消息传递给后一个单元，一定程度上模拟这个过程。</p><p>使用 RNN 面临着一个问题，有时候我们需要非常长的上下文，例如通过非常靠前的相关信息来预测下一个词。在实践中，RNN 难以处理这种“<strong>长期依赖</strong>”，原因和梯度爆炸（exploding gradient）/梯度消失（vanishing gradient）有关。不过，LSTM 可以解决这个问题，它会默认地长时间记忆信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png"></p><p>上图展示了包含了单个层的标准 RNN 单元组成的链式结构。LSTM 有着同样的链式结构，但是重复的组件结构不同，它有着四个交互的层：</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png"></p><h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><p>LSTM 的核心是 <strong>cell state（单元状态）</strong>，即贯穿图顶部的水平线。cell state 有点像传送带，沿着整个链向后，只有一些小的线形相互作用。因此，信息很容易不加改动地沿着它流动。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png"></p><p>而向 cell state 删除或者添加的信息通过<strong>门（gates）</strong>结构精心控制。门结构能够选择性地让信息通过，它有一个 sigmoid 层和一个 pointwise 乘积操作组成：</p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" style="width:20%; margin-top=10px; margin-bottom=10px;"><p>sigmoid 层输出一个 0 到 1 之间的实数，来描述允许多少信息通过，0 代表不允许任何信息通过，而 1 代表让全部信息通过。LSTM 有三个类似的门机构，用来保护和控制 cell state。</p><h2 id="一步一步看公式"><a href="#一步一步看公式" class="headerlink" title="一步一步看公式"></a>一步一步看公式</h2><p>LSTM 的第一步是决定从 cell state 中丢弃什么信息，这个决定通过称为“<strong>忘记门（forget gate）</strong>”的 sigmoid 层作出。输入是 $h_{t-1}$ 和 $x_t$，并为 cell state $C_{t-1}$ 中的每个数字输出一个 0 到 1 之间的值，1 表示完全保留，而 0 表示完全遗忘。</p><p>举一个基于前面所有单词来预测下一个单词的例子，在这个问题中，cell state 可能包括当前主语的性别，以便使用正确的代词。当我们看到一个新的主语时，我们想要忘记旧主语的性别。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png"></p><p>第二步我们决定将在 cell state 中存储什么新信息。首先，一个称为<strong>输入门（input gate）</strong>的 sigmoid 层决定我们将要更新的值。然后，一个 tanh 层创建一个候选的 $\tilde C_i$，准备加到 cell state 中。在下一步，我们将这两个值合并，来为 cell state 做一个更新。</p><p>继续之前的例子，我们想要将新主语的性别加入到 cell state，以取代我们遗忘的内容。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png"></p><p>接下来我们将旧的 cell state $C_{t-1}$ 更新为新的 cell state $C_t$。在例子中，我们在这一步中正式地丢弃关于旧主语性别的信息并添加新信息。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png"></p><p>最后，我们决定要输出的内容。输出取决于我们的 cell state，不过要经过一层过滤。首先，我们还是通过一个 sigmoid 层来决定 cell state 中哪些部分要被输出。之后，我们将 state cell 通过 tanh 来将值缩放到 -1 与 1 之间，并用 sigmoid 门的输出做乘法，决定输出的部分。</p><p>对于语言模型示例，由于它只看到了一个主语，因此可能希望输出与谓词相关的信息，以防接下来会出现与谓词相关的信息。例如，它可以输出主语是单数还是复数，这样我们就知道如果接下来是动词，动词应该用什么形式。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png"></p><h2 id="LSTM-的变体"><a href="#LSTM-的变体" class="headerlink" title="LSTM 的变体"></a>LSTM 的变体</h2><p>以上所述就是一个标准的 LSTM。但是，几乎所有的论文使用的 LSTM 都有或多或少的修改。虽然差异不大，但其中有些版本值得一提。</p><p><a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers &amp; Schmidhuber (2000)</a> 中的版本增加了<strong>窥视孔连接（peephole connections）</strong>，使得门层可以观察 cell state。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png"></p><p>上图在所有的门上都加了窥视孔。也有些论文只给一些门增加窥视孔。</p><p>另一种变体是将遗忘门和输入门耦合。我们不再单独决定要忘记什么或者应该添加什么新信息，而是共同做出这些决定，并且只在准备遗忘旧信息时添加新信息作为替代。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png"></p><p>一个变化稍大的变体是 <a href="arxiv.org/pdf/1406.1078v3.pdf">Cho, et al. (2014)</a> 提出的<strong>门控循环单元（Gated Recurrent Unit, GRU）</strong>。它将遗忘和输入合并到一个单独的“更新门”，并将 cell state 和 hidden state 合并到一块。GRU 比标准的 LSTM 更简单，并且也得到广泛的应用。</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png"></p><p>除了几个最值得注意的变体，还有很多版本。<a href="jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz, et al. (2015)</a> 测试了超过一万种 RNN 架构，发现一部分会在特定任务上比 LSTM 表现更好。</p><h2 id="推荐资料"><a href="#推荐资料" class="headerlink" title="推荐资料"></a>推荐资料</h2><ul><li><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780.</a>：提出 LSTM 的 paper。</li><li><a href="https://jalammar.github.io/">Jay Alammar’s Blog</a>：另一个致力于可视化讲解机器学习概念的好博客。最新的两篇讲 NLP 里的 Transformer 和 BERT，可见内容之新。</li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;一直以来感觉自己对 LSTM 的理解缺了点什么。这次看到一篇不错的博客 &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;，对 LSTM 的机制和公式有逐条的解释。正好最近手头的工作也和门机制有一定联系，因此决定翻译一下，帮助自己理解和记忆公式。主要靠意译，省略了一点无关紧要的内容，有条件可以自己看看原文。&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理笔记" scheme="https://kyonhuang.top/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="LSTM" scheme="https://kyonhuang.top/blog/tags/LSTM/"/>
    
    <category term="自然语言处理" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>越过山丘——写在大四开始时</title>
    <link href="https://kyonhuang.top/blog/over-the-hills/"/>
    <id>https://kyonhuang.top/blog/over-the-hills/</id>
    <published>2018-10-15T12:21:49.000Z</published>
    <updated>2019-02-24T14:20:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>在保研尘埃落地、回学校参加院队热身赛结果弄伤膝盖、从十一假期养到现在，终于提笔开始写这一篇博客。现在已经是十月中旬，开学都已经一个半月了，但对我来说，大四似乎才刚刚开始。且简单地和大家分享一下我保研的结果和做出决定的一些想法，也给自己即将面临的新挑战打个气。</p><span id="more"></span><h2 id="选择"><a href="#选择" class="headerlink" title="选择"></a>选择</h2><p>先谈一下我最终的选择吧：浙江大学和西湖大学的跨学科联合培养博士生项目，计算机科学与技术专业。也就是说，我明年这个时候就是一名博士生了。</p><p><img src="https://raw.githubusercontent.com/bighuang624/bighuang624.github.io/master/images/over-the-hills/postgraduate-recommendation-result.jpg"></p><p>简单地说一下我的保研经历。在夏令营周期中，我比较遗憾没拿到任何让我满意的 offer。而在九月本校保研资格公示后，我又因为复旦的报名系统已关闭而错过这所我心心念念的学校（结果我选择联培学校时最终也没选择复旦，哈哈）。因此，留给我可供选择的学校和导师都不太多了。</p><p>其实我最终拿到了很不错的综合排名，因此很多同学和学长学姐都鼓励我申请清北。但我最终没有在九月再申请清北，原因包括当然包括学校层面的难度较大，但更深层的原因有二，一是我知道清北的好导师竞争激烈程度到清北本科生都需要先在其实验室长时间工作才有机会，而导师是我在做选择中最看重的因素；二是我在暑假去北京时，感受到很明显的与这座城市的隔离感，且气候对我也不太友好，待的很不舒服。鉴于以上原因，考虑到专心准备比较有把握的学校的考核，我做出了上述的抉择，而且我现在仍觉得这是正确的做法。</p><p>在九月我最终申请了三所学校的计算机专业：浙大硕士、上海交大硕士和西湖大学联培博士（目前西湖大学只有博士生项目），严格意义上三所学校的 offer 我都拿到了。浙大是 9 月 27 日晚上给我发的邮件，但由于 9 月 28 日志愿填报系统开放，所有学校都要求当天走完所有填报流程，而我之前联系的浙大老师没有再回复我，因此我在浙大本校没有联系好的导师；上海交大一开始通知我没过，但 9 月 30 日又来电说有少量硕士名额，此时我早已被西湖大学录取，只可惜之前和上海交大 SpeechLab 的俞老师已经有比较深入的沟通，而且 SpeechLab 的研究方向和研究理念我都非常喜欢，最终遗憾错失这个机会。</p><p>攻读直博是半年前的我可能没法想象到的选择，因为我其实一直是想着先读硕士，之后看情况是能够去国内外更好的地方继续深造或者想直接就业，因此申请各校时基本都填报的是硕士。那么，为什么是西湖大学的博士生？我想，原因肯定包括 27 日下午就确定录取以及未来的导师、西湖较好的住宿和待遇、对杭州本身的喜爱，但是起决定性作用的还是以下几点：</p><ul><li><strong>西湖有非常优秀的老师和同龄人</strong>：老师自然不必说，基本都是从全世界各个高校引入的才俊。而我在西湖遇到的同届同学以及学长学姐也都非常优秀。这轮招生都是招收保研本科生，而工学院录取的同学中，有一半以上已经发表过高质量的论文，其余的同学（除开我）也都有专利等成果。在 27 日的晚上，我们很多同学在宿舍的走廊非常自然而愉快地聊了一场大天，来自天南地北不同学校、工学化学生物等不同专业的同学相互自我介绍、聊理想聊规划聊各色各样，这种氛围真的非常吸引人。说实话，作为一个可以说没有科研基础的学生，与这些同学共处有一定的压力。但是我相信在这样的环境才能更好地成长，也能更开心地学习与工作。</li><li><strong>作为西湖的建设者有更大的挑战</strong>：在知道我最终决定去西湖大学就读时，我父亲在电话非常严肃地说你等一等，随即询问几名同在高校工作的朋友来确定西湖是否“靠谱”。我理解西湖大学作为一所民办大学、一个刚起步的改革探索者，目前才有两届学生入学，在大众眼里和清北复交浙等传统名校会有一定的差距。但我觉得能够见证与参与这所研究型机构的初创期是一个非常难得的事情。这样的环境中的老师和同学们会更有个性和活力，大家都会成为学校的建设者。因此，我十分愿意和西湖大学一起迎接挑战。</li><li><strong>我的导师具备科研实力和学生培养能力</strong>：其实我最终确定的导师不是我一开始的目标导师。我一直是希望能够在 NLP 方向开始研究，因此一开始联系的是在此领域名声在外的张老师，但是本轮张老师唯一的名额给了一名暑假在他那实习并且出了一篇顶会 paper 的优秀同学。工学院的王老师第一次与我见面是在武大的招生宣讲会上，当时他叮嘱我一定要按时申请。而在西湖面试结束后，他非常诚恳地和我聊，希望我能够去他那里。我们的沟通都非常顺畅，我认为王老师有足够的耐心让我从零起步开始科研、给我足够的条件和重视、并且能够支持我在大方向下自由探索。因此，我愿意放弃（至少是目前阶段）我最喜欢的 NLP 方向，以及背负一定直博带来的风险。我相信这是值得的。</li></ul><p>综上所述，我最终决定在西湖大学开始我的下一个阶段。感谢在保研过程中非常多的老师、同学、学长学姐给了我很多建议和帮助。希望我能够不忘初心，终有所得。</p><p><img src="https://raw.githubusercontent.com/bighuang624/bighuang624.github.io/master/images/over-the-hills/WIAS.jpg"></p><h2 id="大四计划"><a href="#大四计划" class="headerlink" title="大四计划"></a>大四计划</h2><p>我目前计划是下周前往西湖高等研究院，开始在实验室里实习。还有一周的时间，这一周我计划是加强机器学习基础、调整自己的作息，以及处理完学校的一些事务。</p><p>而在大四这一年，我目前有这样的一些计划：</p><ol><li>打好专业基础和研究基础，包括数学、机器学习的知识和论文阅读笔记等等；</li><li>适应实验室的生活，积极参与研究项目；</li><li>提前准备毕业论文，以寻找未来的研究方向；</li><li>学习英语，可以考虑屠一个雅思/托福/GRE；</li><li>养成并坚持早睡早起的作息习惯，适当注意保养脸和头发:)；</li><li>认真打一次 Kaggle 的正式比赛，争取能够到前 10%，从而对 Python 代码编写、开源框架的使用、数据科学项目的思路以及优化的技巧都有一个极大的锻炼；</li><li>啃完 3-4 本技术书籍，并在 Github 上拥有一个 100+ star 的技术性项目；</li><li>看几本以前想看但是没抽时间看的书，例如《娱乐至死》、《乌合之众》、《我的职业是小说家》、《仿生人会梦见电子羊吗？》等。电影也可以选择性看看，最好能认真写几篇点评:)；</li><li>去武大里没去过的地方看看，计划一场毕业旅行；</li><li>向学长学姐了解博士生的生活，询问他们的规划，构建自己的蓝图。</li></ol><p>其实之前写在博客上的好多计划最后也就执行了大概一半，希望这一次能够努力实现吧。</p><!--## 博士生阶段规划暂时写不出来了。嘻嘻，主要还是争取很多高质量的 paper，开会 social、交流、合作，增加出国和实习经历吧。[](https://www.zhihu.com/question/274824864/answer/504505852)-->]]></content>
    
    
    <summary type="html">&lt;p&gt;在保研尘埃落地、回学校参加院队热身赛结果弄伤膝盖、从十一假期养到现在，终于提笔开始写这一篇博客。现在已经是十月中旬，开学都已经一个半月了，但对我来说，大四似乎才刚刚开始。且简单地和大家分享一下我保研的结果和做出决定的一些想法，也给自己即将面临的新挑战打个气。&lt;/p&gt;</summary>
    
    
    
    <category term="翻滚吧大学生" scheme="https://kyonhuang.top/blog/categories/%E7%BF%BB%E6%BB%9A%E5%90%A7%E5%A4%A7%E5%AD%A6%E7%94%9F/"/>
    
    
    <category term="总结" scheme="https://kyonhuang.top/blog/tags/%E6%80%BB%E7%BB%93/"/>
    
    <category term="未来展望" scheme="https://kyonhuang.top/blog/tags/%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B/"/>
    
    <category term="随笔" scheme="https://kyonhuang.top/blog/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Word2vec - 最经典的词嵌入学习算法</title>
    <link href="https://kyonhuang.top/blog/Word2vec-intro/"/>
    <id>https://kyonhuang.top/blog/Word2vec-intro/</id>
    <published>2018-09-13T10:55:28.000Z</published>
    <updated>2018-09-14T05:13:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果想要开始研究基于深度学习的自然语言处理，你很难逃开 Word2vec 这一最为经典的词嵌入学习算法。然而，想要透彻理解 Word2vec 又并非易事。因此，我写这篇博文，力图叙述清楚 Word2vec 的原理，以对学到的知识做一个梳理。</p><span id="more"></span><h2 id="为什么要使用-Word2vec？"><a href="#为什么要使用-Word2vec？" class="headerlink" title="为什么要使用 Word2vec？"></a>为什么要使用 Word2vec？</h2><h3 id="One-Hot-Representation"><a href="#One-Hot-Representation" class="headerlink" title="One-Hot Representation"></a>One-Hot Representation</h3><p>自然语言处理（NLP）相关任务中，要将自然语言交给机器学习中的算法来处理，通常需要首先将语言数学化，因为机器没法理解自然语言。作为语言最细粒度的单位，词语被转换为数值形式的产物通常为词向量，顾名思义就是把一个词表示成一个向量。</p><p>最简单的词向量方式是 <strong>One-Hot Representation</strong>。简单来说，就是每个词向量的长度为词典大小，向量的分量只有一个 1，其他全为 0， 1 的位置对应该词在词典中的位置。这样的做法非常简单，也容易实现，只需要用哈希表给词典中的每个词语分配一个编号即可。NLP 领域的很多主流任务完全可以通过这种简洁的表示方法，配合其他的算法来完成。</p><p>但是，One-Hot Representation 有两个不可忽视的缺点：</p><ol><li>One-Hot Representation 的词向量是高维稀疏的。当数据的维度越高，要找到最优解甚至达到稍低维度时模型的同等表现，所需要的数据越多，而且呈指数型增长。此问题被称为“维度灾难”。</li><li>One-Hot Representation 无法体现词语间的相似度。因为每个单词都背表示为表示为完全独立的个体，不同词向量都是正交的。因此，词向量缺少语法和语义的信息。</li></ol><h3 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h3><p>因此，One-Hot Representation 已经逐渐不能满足需求，人们寻求在词向量表示上的新方法。之后，Hinton 在 1986 年提出了 Distributed Representation，基本思想是将每个词表达成 n 维稠密、连续的实数向量，每个分量通过训练得到。例如：<code>[0.792, −0.177, −0.107, 0.109, −0.542, ...]</code>。</p><p>这样，两个问题都迎刃而解：由于词向量维度较低，“维度灾难”难以出现；而两个词语义、语法上的相似性也可以通过计算两个词之间的“距离”来表示，因为所有词向量一起可以组成一个词向量空间，只要计算欧式距离或者 cos 距离即可。</p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>接下来，我们介绍语言模型。在统计自然语言处理中，<strong>语言模型（Language Model）</strong>指的是计算一个句子是否是自然语言的概率模型，通常用于在机器翻译、语音识别等任务得到若干候选答案后挑选一个比较正常的结果。例如，我们有 Bigram 模型，通过前一个词语已确定、后一个词语出现的概率来计算整句话遵循自然语言规则的概率：</p><p>$$P(w_1, w_2, …, w_n) = \prod^n_{i=2}P(w_i | w_{i-1})$$</p><p>大部分的有监督机器学习模型，都可以归结为<code>f(x)-&gt;y</code>。将语言模型看作<code>f</code>，<code>x</code>为一个句子里的一个词语，那么<code>y</code>就是这个词语的上下文词语。根据语言学中一个非常重要的假设——<strong>分布相似性</strong>，类似的词有相似的语境，即其上下文词语<code>y</code>应该相似。两个语义相似的词对应的向量也是相似的，具体反映在夹角或距离上。</p><p>Word2vec 就是源于以上思想。通过词语及其上下文，我们可以训练出一个模型<code>f</code>。在 Word2vec 中，我们关心<strong>神经网络训练完后得到的权重</strong>（即模型参数），这些向量化表示的权重就是我们需要的词向量，或者也被称为<strong>词嵌入（Word Embeddings）</strong>。当然，得到词嵌入的不止 Word2vec 一种算法，但 Word2vec 是提出时间较早并且比较经典的一种，因此值得花时间去透彻理解。</p><h2 id="Word2vec-的两种语言模型"><a href="#Word2vec-的两种语言模型" class="headerlink" title="Word2vec 的两种语言模型"></a>Word2vec 的两种语言模型</h2><p>Word2vec 是一种基于迭代的词嵌入学习算法，其包含两种语言模型：</p><ul><li><strong>Skip-gram</strong>：根据词预测目标上下文</li><li>**Continuous Bag of Words (CBOW)**：根据上下文预测目标词</li></ul><h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><p>在 Skip-gram 中，我们用当前词<code>x</code>来预测上下文<code>y</code>。一般的数学模型只接受数值型输入，因此<code>x</code>的原始输入形式就使用前文提到的 One-Hot Representation，每个词语都有自己的唯一表示。<code>y</code>使用的就是词典中每个词是正确上下文的概率。</p><p>当<code>y</code>只取<code>x</code>的下一个词时，网络结构如下图：</p><p><img src="https://raw.githubusercontent.com/bighuang624/bighuang624.github.io/master/images/Word2vec-intro/Simple-Skip-gram.jpg"></p><p>设词典中词语数量为<code>V</code>，隐藏层节点数为<code>N</code>。当模型训练完后，当我们输入一个 One-Hot Representation 的向量作为<code>x</code>时，则在输入层到隐藏层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数为<code>N</code>，跟隐藏层节点数是一致的。由于每个词语的 One-Hot Representation 里 1 的位置是不同的，因此，对应 1 这个位置的权重组成的向量就可以作为该词的唯一表示。</p><p>同理，通过训练完后的隐藏层到输出层的权重同样可以得到另一种词向量。两种词向量分别称为“输入向量”和“输出向量”，一般我们使用“输入向量”。</p><p>由于<code>N</code>通常要远远小于<code>V</code>，因此我们通过 Word2vec 完成了对词向量的降维，以避免“维度灾难”的出现。</p><p>另外，<strong>隐藏层的激活函数其实是线性的</strong>，相当于没做任何处理。这是 Word2vec 相对于之前的语言模型的简化之处。在用反向传播算法训练神经网络时，本质上是<strong>链式求导</strong>。</p><p>更一般的情形下，上下文<code>y</code>应该有多个词，因此网络结构扩展如下图：</p><p><img src="https://raw.githubusercontent.com/bighuang624/bighuang624.github.io/master/images/Word2vec-intro/Multi-Skip-gram.jpg"></p><!--损失函数--><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>在 CBOW 中，我们用上下文预测当前词。其他与 Skip-gram 相似，因此有网络结构如下图：</p><p><img src="https://raw.githubusercontent.com/bighuang624/bighuang624.github.io/master/images/Word2vec-intro/CBOW.jpg"></p><p>具体步骤如下：</p><ol><li>生成所有的 One-Hot Representation；</li><li>根据上下文得到相对应的词向量；</li><li>将所有的上下文词向量求和后做一个平均，作为隐藏层的向量；</li><li>在隐藏层向量和输出向量相乘后，通过 Softmax 计算预测概率；</li><li>通过反向传播更新参数。</li></ol><p>损失函数为：</p><p>$$H(\hat y, y) = -\sum^V_{j=1}y_jlog(\hat y_j)$$</p><h3 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h3><p>Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。对于 Word2vec，提出了两种训练技巧：</p><ul><li><strong>负采样（Negative Sampling）</strong>：采样负例。本质是预测总体类别的一个子集。</li><li><strong>分级的 Softmax（Hierarchical Softmax）</strong>：用一种有效的树结构来计算所有词汇的概率。本质是把 N 分类问题变成 log(N) 次二分类。</li></ul><p>因其不是 Word2vec 的精髓，在这篇文章中，暂时不对这两种训练技巧做过多的介绍了。有兴趣的同学可以在参考资料中选择阅读，或者等待下一篇文章。</p><h2 id="如何使用-Word2Vec？"><a href="#如何使用-Word2Vec？" class="headerlink" title="如何使用 Word2Vec？"></a>如何使用 Word2Vec？</h2><p>gensim 是一款开源的第三方 Python 工具包，包含 Word2vec 在内的多种主题模型算法。这里有一个<a href="https://github.com/bighuang624/Python-Learning/tree/master/code/word2vec-test">简单的 demo</a>来演示如何使用 gensim 中的 Word2vec。你也可以查阅 gensim 的官方文档来学习更多的用法。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Word2vec 作为最经典的词嵌入学习算法，让词嵌入开始广泛被使用。然而，Word2vec 学习的向量其实和真正的语义还有差距，因为其学到的更多是具备相似上下文的词语，例如表示“good”和“bad”的词嵌入相似度就非常高。因此，后续又有一些新的词嵌入学习算法被提出并大量使用，其中最火的就是 Glove。</p><p>到目前为止，这篇博文介绍了为什么要使用 Word2vec 的产物，以及 Word2vec 的算法本质。我的目标是能够覆盖参考资料中的《N问word2vec》中的所有问题，因此后续学习还会更新此文来添加公式推导、训练技巧介绍等内容，或者写下一篇文章。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h3 id="写作参考"><a href="#写作参考" class="headerlink" title="写作参考"></a>写作参考</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/26306795">[NLP] 秒懂词向量Word2vec的本质</a>：本文主干参照了知乎上的这篇文章，写得非常好，推荐</li><li><a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf">课程 cs224n 的 notes1</a></li><li><a href="https://blog.csdn.net/mytestmy/article/details/26961315">深度学习word2vec笔记之基础篇 - CSDN博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/43214781">（No.39）N问word2vec</a>：可以通过试图回答这篇文章里的问题来检验自己对 Word2vec 到底了解多深</li></ul><h3 id="补充阅读"><a href="#补充阅读" class="headerlink" title="补充阅读"></a>补充阅读</h3><ul><li>Mikolov 关于 Word2vec 的两篇开山之作：<ul><li><a href="https://arxiv.org/pdf/1405.4053.pdf">Distributed Representations of Sentences and Documents</a>：在前人基础上提出 Word2vec</li><li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient estimation of word representations in vector space</a>：专门讲两个训练技巧</li></ul></li><li><a href="http://www.hankcs.com/nlp/Word2vec.html">Word2vec原理推导与代码分析-码农场</a></li><li><a href="https://arxiv.org/pdf/1411.2738.pdf">word2vec Parameter Learning Explained</a>：他人重点推荐</li><li><a href="https://www.zhihu.com/question/53011711">Word2vec 相比之前的 Word Embedding 方法好在什么地方？ - 知乎</a></li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;如果想要开始研究基于深度学习的自然语言处理，你很难逃开 Word2vec 这一最为经典的词嵌入学习算法。然而，想要透彻理解 Word2vec 又并非易事。因此，我写这篇博文，力图叙述清楚 Word2vec 的原理，以对学到的知识做一个梳理。&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理笔记" scheme="https://kyonhuang.top/blog/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="自然语言处理" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    <category term="Word2vec" scheme="https://kyonhuang.top/blog/tags/Word2vec/"/>
    
    <category term="语言模型" scheme="https://kyonhuang.top/blog/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="Skip-gram" scheme="https://kyonhuang.top/blog/tags/Skip-gram/"/>
    
    <category term="CBOW" scheme="https://kyonhuang.top/blog/tags/CBOW/"/>
    
  </entry>
  
  <entry>
    <title>《利用Python进行数据分析》笔记</title>
    <link href="https://kyonhuang.top/blog/python-for-data-analysis-notes/"/>
    <id>https://kyonhuang.top/blog/python-for-data-analysis-notes/</id>
    <published>2018-08-22T11:41:52.000Z</published>
    <updated>2018-08-22T11:57:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>《利用Python进行数据分析》是一本非常不错的 Cookbook。早在去年年底的时候我就一边看一边记录一遍敲代码，阅读完比较重要的章节，整理了这篇笔记。有时候有些 API 忘记了，就翻看查找，比直接看 pdf 要方便。现在为了查找更方便，将这篇笔记迁移到博客上来。</p><span id="more"></span><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="Python-的优点"><a href="#Python-的优点" class="headerlink" title="Python 的优点"></a>Python 的优点</h3><ul><li>动态编程（解释型语言）</li><li>科学计算社区活跃，有不断改良的库</li><li>胶水语言，轻松集成 C、C++ 以及 Fortran 代码</li><li>同时适用于研究和原型构建及构件生产系统</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>运行速度慢</li><li>Python 有全局解释器锁（GIL），防止解释器同时执行多条 Python 字节码指令的机制。因此，多线程并行代码不能在单个 Python 进程中执行。</li></ul><h3 id="重要的-Python-库"><a href="#重要的-Python-库" class="headerlink" title="重要的 Python 库"></a>重要的 Python 库</h3><h4 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h4><p>科学计算基础包，提供快速的<strong>数组处理</strong>能力，并作为在算法之间传递数据的容器（<code>ndarray</code>）。</p><h4 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h4><p>一组专门解决<strong>科学计算</strong>中各种标准问题域的包的集合。</p><h4 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h4><p>针对<strong>数据分析和预处理</strong>，提供快速<strong>处理结构化数据</strong>的大量数据结构和函数。</p><h4 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h4><p>提供好用的交互式数据绘图环境，<strong>绘制交互式图表</strong>。</p><h4 id="IPython"><a href="#IPython" class="headerlink" title="IPython"></a>IPython</h4><p>一个增强的 Python shell，以提高编写、测试、调试 Python 代码的速度。主要用于<strong>交互式数据处理</strong>和<strong>利用 matplotlib 对数据进行可视化处理</strong>。</p><p>除开标准的基于终端的 IPython shell 外，还提供了：</p><ul><li>一个 HTML 笔记本；</li><li>一个基于 Qt 框架的 GUI 控制台；</li><li>用于交互式并行和分布式计算的基础架构。</li></ul><h3 id="引入惯例"><a href="#引入惯例" class="headerlink" title="引入惯例"></a>引入惯例</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p><code>np.arange</code>引用的即是 NumPy 中的 arange 函数。在 Python 软件开发过程中，不建议直接引入类似 NumPy 这种大型库的全部内容（<code>from numpy import *</code>）。</p><h2 id="IPython：一种交互式计算和开发环境"><a href="#IPython：一种交互式计算和开发环境" class="headerlink" title="IPython：一种交互式计算和开发环境"></a>IPython：一种交互式计算和开发环境</h2><p>IPython 本身并没有提供任何的计算或数据分析功能，其设计目的是在交互式计算和软件开发这两个方面最大化地提高生产力。由于大部分的数据分析代码都含有<strong>探索式操作</strong>（试误法和迭代法），因此 IPython 将有助于提高工作效率。</p><h3 id="Tab-键自动完成"><a href="#Tab-键自动完成" class="headerlink" title="Tab 键自动完成"></a>Tab 键自动完成</h3><p>Tab 键自动完成功能不只可以用于搜索命名空间和自动完成对象和模块属性，还可以匹配路径（输入正斜杠<code>/</code>后）。</p><h3 id="内省"><a href="#内省" class="headerlink" title="内省"></a>内省</h3><p>在变量的前面或后面加上一个问号（<code>?</code>）可以显示有关该对象的一些通用信息，这叫做<strong>对象内省（object introspection）</strong>。</p><p>如果该对象是一个函数或实例方法，则其 docstring（如果有的话）也会被显示出来。使用<code>??</code>还将显示出该函数的源代码（如果可能的话）。</p><p>另外，<code>?</code>还可以用于<strong>搜索 IPython 命名空间</strong>，一些字符再配以通配符（<code>*</code>）即可显示出所有与该通配符表达式相匹配的名称：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: np.*load*?</span><br><span class="line">np.__loader__</span><br><span class="line">np.load</span><br><span class="line">np.loads</span><br><span class="line">np.loadtxt</span><br><span class="line">np.pkgload</span><br></pre></td></tr></table></figure><h3 id="run-命令"><a href="#run-命令" class="headerlink" title="%run 命令"></a>%run 命令</h3><p>在 IPython 环境中，可以通过 <strong><code>%run</code>命令将文件当作 Python 程序</strong>来运行。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: %run ipython_script_test.py</span><br></pre></td></tr></table></figure><p>脚本是在一个空的命名空间中运行的（没有任何 import，也没有定义任何其他的变量），所以其行为应该跟在标准命令行环境中一样。</p><p>如果希望脚本能够访问在交互式 IPython 命名空间中定义的变量，应该使用<code>%run -i</code>。</p><h3 id="键盘快捷键"><a href="#键盘快捷键" class="headerlink" title="键盘快捷键"></a>键盘快捷键</h3><p>挑选一些不太熟又有些实用的：</p><table><thead><tr><th align="left">命令</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">Ctrl + A</td><td align="left">将光标移动到行首</td></tr><tr><td align="left">Ctrl + E</td><td align="left">将光标移动到行尾</td></tr><tr><td align="left">Ctrl + U</td><td align="left">删除从光标开始至行首的文本</td></tr><tr><td align="left">Ctrl + K</td><td align="left">删除从光标开始至行尾的文本</td></tr><tr><td align="left">Ctrl + L</td><td align="left">清屏</td></tr></tbody></table><h3 id="魔术命令（Magic-Command）"><a href="#魔术命令（Magic-Command）" class="headerlink" title="魔术命令（Magic Command）"></a>魔术命令（Magic Command）</h3><p><strong>魔术命令</strong>是以百分号<code>%</code>为前缀的命令，为常见任务提供便利，或者让用户轻松控制 IPython 系统的行为。</p><p>例如，通过<code>%timeit</code>这个魔术命令可以检测任意 Python 语句（如矩阵乘法）的执行时间（多次执行计算平均执行时间，用于评估执行时间非常小的代码）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.randn(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: %timeit np.dot(a, a)</span><br><span class="line">The slowest run took <span class="number">2714.09</span> times longer than the fastest. This could mean that an intermediate result <span class="keyword">is</span> being cached.</span><br><span class="line"><span class="number">10000</span> loops, best of <span class="number">3</span>: <span class="number">41.9</span> µs per loop</span><br></pre></td></tr></table></figure><p>魔术命令可以看作运行于 IPython 系统中的命令行程序。它们大都还有一些“命令行选项”，使用<code>?</code>即可查看其选项。例如：<code>%reset?</code></p><p>魔术命令默认是可以不带百分号使用的，只要没有定义与其同名的变量即可。这个技术叫做 automagic，可以通过<code>%automagic</code>打开或关闭。</p><p>输入<code>%quickref</code>或<code>%magic</code>可以直接访问文档。</p><p>更多常用的 IPython 魔术命令可见原书 p58（pdf p69）。</p><h3 id="基于-Qt-的富-GUI-控制台"><a href="#基于-Qt-的富-GUI-控制台" class="headerlink" title="基于 Qt 的富 GUI 控制台"></a>基于 Qt 的富 GUI 控制台</h3><p>IPython 团队开发了一个基于 Qt 框架（目的是为终端应用程序提供诸如内嵌图片、多行编辑、语法高亮等富文本编辑功能）的 GUI 控制台。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipython qtconsole --pylab=inline</span><br></pre></td></tr></table></figure><h3 id="matplotlib-集成与-pylab-模式"><a href="#matplotlib-集成与-pylab-模式" class="headerlink" title="matplotlib 集成与 pylab 模式"></a>matplotlib 集成与 pylab 模式</h3><p>IPython 对各个 GUI 框架进行了专门的处理以使其能够和 shell 配合得天衣无缝。通常我们通过以下命令来在启动 IPython 时集成 matplotlib：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipython --pylab</span><br></pre></td></tr></table></figure><h3 id="交互式调试器"><a href="#交互式调试器" class="headerlink" title="交互式调试器"></a>交互式调试器</h3><p>IPython 紧密集成并加强了 Python 内置的 pdb 调试器。发生异常后马上输入<code>%debug</code>命令将会调用那个“事后”调试器，并直接跳转到引发异常的那个栈帧（stack frame）。</p><p>在这个调试器中，可以执行任意 Python 代码并查看各个栈帧中的一切对象和数据。默认是从最低级开始的（即错误发生的地方）。输入<code>u</code>和<code>d</code>即可在栈跟踪的各级别之间切换。</p><p>执行<code>%pdb</code>命令可以让 IPython 在出现异常之后自动调用调试器。</p><p>更多请查看原书 p67（pdf p78）。</p><h3 id="基本性能分析：-prun和-run-p"><a href="#基本性能分析：-prun和-run-p" class="headerlink" title="基本性能分析：%prun和%run -p"></a>基本性能分析：<code>%prun</code>和<code>%run -p</code></h3><p>代码的性能分析跟代码执行时间密切相关，但更关注耗费时间的位置。主要的 Python 性能分析工具是 <strong>cProfile 模块</strong>，它在执行一个程序或代码块时，会记录各函数所耗费的时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m cProfile cprof_example.py</span><br></pre></td></tr></table></figure><p>这样执行输出的结果会按照函数名排列。通常会再用<code>-s</code>标记指定一个排序规则：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m cProfile -s cprof_example.py</span><br></pre></td></tr></table></figure><p>除命令行用法之外，cProfile 还可以编程的方式分析人意代码块的性能。<code>%prun</code>的格式跟 cProfile 差不多，但它分析的是 Python 语句而不是整个 .py 文件：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: %prun -l <span class="number">7</span> -s cumulative run_expriment()</span><br></pre></td></tr></table></figure><p>执行<code>%run -p -s cumulative cprof_example.py</code>也能达到上面那条系统命令行命令一样的效果，但无需退出 IPython。</p><p>关于逐行分析函数性能，查看原书 p74（pdf p85）。</p><h3 id="IPython-HTML-Notebook"><a href="#IPython-HTML-Notebook" class="headerlink" title="IPython HTML Notebook"></a>IPython HTML Notebook</h3><p>IPython Notebook 是一种基于 Web 技术的交互式计算文档格式。通过下面这条命令即可启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipython notebook --pylab=inline</span><br></pre></td></tr></table></figure><h2 id="NumPy-基础：数组和矢量计算"><a href="#NumPy-基础：数组和矢量计算" class="headerlink" title="NumPy 基础：数组和矢量计算"></a>NumPy 基础：数组和矢量计算</h2><p>NumPy 是高性能科学计算和数据分析的基础包，其部分功能如下：</p><ul><li>ndarray，一个具有矢量算数运算和复杂广播能力的快速且节省空间的多位数组。</li><li>用于对整组数据进行快速运算的标准数学函数（无需编写循环）。</li><li>用于读写磁盘数据的工具以及用于操作内存映射文件的工具。</li><li>线性代数、随机数生成以及傅立叶变换功能。</li><li>用于集成由 C、C++、Fortran 等语言编写的代码的工具。</li></ul><h3 id="ndarray：一种多维数组对象"><a href="#ndarray：一种多维数组对象" class="headerlink" title="ndarray：一种多维数组对象"></a>ndarray：一种多维数组对象</h3><p>ndarray 是一个通用的<strong>同构数据多维容器</strong>，也就是说，其中的所有元素必须是相同类型的。</p><h4 id="几种属性"><a href="#几种属性" class="headerlink" title="几种属性"></a>几种属性</h4><ul><li>array.ndim # 维度</li><li>array.shape # 行数和列数</li><li>array.size # 元素个数</li></ul><h3 id="创建-ndarray"><a href="#创建-ndarray" class="headerlink" title="创建 ndarray"></a>创建 ndarray</h3><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">array</td><td align="left">将输入数据（列表、元组、数组和其他序列类型）转换为 ndarray。默认直接复制输入数据</td></tr><tr><td align="left">asarray</td><td align="left">将输入转换为 ndarray，如果输入本身就是一个 ndarray 就不进行复制</td></tr><tr><td align="left">arange</td><td align="left">类似于内置的 range，但返回一个 ndarray 而非列表</td></tr><tr><td align="left">ones</td><td align="left">根据指定的形状和 dtype 创建一个全 1 数组</td></tr><tr><td align="left">ones_like</td><td align="left">以另一个数组为参数，并根据其形状和 dtyoe 创建一个全 1 数组</td></tr><tr><td align="left">zeros、zeros_like</td><td align="left">类似于 ones 和 ones_like，产生全 0 数组</td></tr><tr><td align="left">empty、empty_like</td><td align="left">创建新数组，只分配内存空间但不填充任何值</td></tr><tr><td align="left">eye、identity</td><td align="left">创建一个正方的 N*N 单位矩阵（对角线为 1，其余为 0）</td></tr></tbody></table><h3 id="基本的索引和切片"><a href="#基本的索引和切片" class="headerlink" title="基本的索引和切片"></a>基本的索引和切片</h3><p>将一个标量值赋值给一个切片时，该值会自动传播到整个选区。跟列表最重要的区别在于，数组切片是原始数组的<strong>视图</strong>，这意味着数据不会被复制，<strong>视图上的任何修改都会直接反映到源数组上</strong>。如果想要得到 ndarray 切片的一份副本而非视图，就需要显式地进行复制操作，例如<code>arr[5:8].copy()</code>。</p><h3 id="布尔型索引"><a href="#布尔型索引" class="headerlink" title="布尔型索引"></a>布尔型索引</h3><p>跟算术运算一样，数组的比较运算（如<code>==</code>）也是矢量化的。因此有：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">61</span>]: names = np.array([<span class="string">&#x27;Bob&#x27;</span>,<span class="string">&#x27;Joe&#x27;</span>,<span class="string">&#x27;Will&#x27;</span>,<span class="string">&#x27;Bob&#x27;</span>,<span class="string">&#x27;Will&#x27;</span>,<span class="string">&#x27;Joe&#x27;</span>,<span class="string">&#x27;Joe&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">63</span>]: names == <span class="string">&#x27;Bob&#x27;</span></span><br><span class="line">Out[<span class="number">63</span>]: array([ <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], dtype=<span class="built_in">bool</span>)</span><br></pre></td></tr></table></figure><p>生成的布尔型数组可用于数组索引，并和切片、整数等混合使用。</p><p>要选择除某值以外的其他值，既可以使用不等于符号（<code>!=</code>），也可以通过负号（<code>-</code>）对条件进行否定。选取多个名字需要组合应用多个布尔条件，使用<code>&amp;</code>（和）、<code>|</code>（或）之类的布尔算术运算符即可。<strong>Python 关键字 and 和 or 在布尔型数组中无效</strong>。</p><p>通过布尔型索引选取数组中的数据，将总是创建数据的<strong>副本</strong>。</p><h3 id="花式索引"><a href="#花式索引" class="headerlink" title="花式索引"></a>花式索引</h3><p><strong>花式索引（Fancy indexing）</strong>是一个 NumPy 术语，指利用整数数组进行索引。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">64</span>]: arr = np.empty((<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">65</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    ...:     arr[i] = i</span><br><span class="line">    ...:</span><br><span class="line"></span><br><span class="line">In [<span class="number">66</span>]: arr</span><br><span class="line">Out[<span class="number">66</span>]:</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">4.</span>,  <span class="number">4.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">5.</span>,  <span class="number">5.</span>,  <span class="number">5.</span>,  <span class="number">5.</span>],</span><br><span class="line">       [ <span class="number">6.</span>,  <span class="number">6.</span>,  <span class="number">6.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">7.</span>,  <span class="number">7.</span>,  <span class="number">7.</span>,  <span class="number">7.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">67</span>]: arr[[<span class="number">4</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">6</span>]]</span><br><span class="line">Out[<span class="number">67</span>]:</span><br><span class="line">array([[ <span class="number">4.</span>,  <span class="number">4.</span>,  <span class="number">4.</span>,  <span class="number">4.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">6.</span>,  <span class="number">6.</span>,  <span class="number">6.</span>,  <span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: arr[[-<span class="number">3</span>,-<span class="number">5</span>,-<span class="number">7</span>]]</span><br><span class="line">Out[<span class="number">68</span>]:</span><br><span class="line">array([[ <span class="number">5.</span>,  <span class="number">5.</span>,  <span class="number">5.</span>,  <span class="number">5.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><p>花式索引和切片不一样，它总是将数据复制到新数组中。</p><h3 id="数组转置和轴对换"><a href="#数组转置和轴对换" class="headerlink" title="数组转置和轴对换"></a>数组转置和轴对换</h3><p>简单的转置可以使用<code>.T</code>，其实就是进行轴对换：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">71</span>]: arr.T</span><br><span class="line">Out[<span class="number">71</span>]:</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">4</span>,  <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>, <span class="number">20</span>, <span class="number">24</span>, <span class="number">28</span>],</span><br><span class="line">       [ <span class="number">1</span>,  <span class="number">5</span>,  <span class="number">9</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">21</span>, <span class="number">25</span>, <span class="number">29</span>],</span><br><span class="line">       [ <span class="number">2</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">14</span>, <span class="number">18</span>, <span class="number">22</span>, <span class="number">26</span>, <span class="number">30</span>],</span><br><span class="line">       [ <span class="number">3</span>,  <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">19</span>, <span class="number">23</span>, <span class="number">27</span>, <span class="number">31</span>]])</span><br></pre></td></tr></table></figure><p>对于高维数组，<code>transpose</code>需要得到一个由轴编号组成的元祖才能对这些轴进行转置：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: arr = np.arange(<span class="number">16</span>).reshape((<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: arr</span><br><span class="line">Out[<span class="number">75</span>]:</span><br><span class="line">array([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">76</span>]: arr.transpose((<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>))</span><br><span class="line">Out[<span class="number">76</span>]:</span><br><span class="line">array([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]])</span><br></pre></td></tr></table></figure><p>ndarray 还有一个<code>swapaxes</code>方法，需要接受一对轴编号：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">77</span>]: arr</span><br><span class="line">Out[<span class="number">77</span>]:</span><br><span class="line">array([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">78</span>]: arr.swapaxes(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">Out[<span class="number">78</span>]:</span><br><span class="line">array([[[ <span class="number">0</span>,  <span class="number">4</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">7</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">8</span>, <span class="number">12</span>],</span><br><span class="line">        [ <span class="number">9</span>, <span class="number">13</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">14</span>],</span><br><span class="line">        [<span class="number">11</span>, <span class="number">15</span>]]])</span><br></pre></td></tr></table></figure><p>对于一维数组（序列），普通的转置操作可能无法对其进行转置，此时需要借助其他的函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: A</span><br><span class="line">Out[<span class="number">10</span>]: array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: A.T</span><br><span class="line">Out[<span class="number">11</span>]: array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: A[np.newaxis,:]</span><br><span class="line">Out[<span class="number">13</span>]: array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: A[:,np.newaxis]</span><br><span class="line">Out[<span class="number">14</span>]:</span><br><span class="line">array([[<span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>],</span><br><span class="line">       [<span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p>转置返回的都是源数据的<strong>视图</strong>（不会进行任何复制操作）。</p><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><ul><li><code>np.vstack((A, B))</code>：上下合并；</li><li><code>np.hstack((A, B))</code>：左右合并；</li><li><code>np.concatenate()</code>：合并操作针对多个矩阵或序列，用<code>np.concatenate()</code>会更加方便。可以用一个<code>axis</code>参数控制合并方向。</li></ul><h3 id="通用函数"><a href="#通用函数" class="headerlink" title="通用函数"></a>通用函数</h3><p><strong>通用函数（即 ufunc）</strong>是一种对 ndarray 中的数据执行元素级运算的函数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: arr = np.arange(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: np.sqrt(arr)</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">array([ <span class="number">0.</span>        ,  <span class="number">1.</span>        ,  <span class="number">1.41421356</span>,  <span class="number">1.73205081</span>,  <span class="number">2.</span>        ,</span><br><span class="line">        <span class="number">2.23606798</span>,  <span class="number">2.44948974</span>,  <span class="number">2.64575131</span>,  <span class="number">2.82842712</span>,  <span class="number">3.</span>        ])</span><br></pre></td></tr></table></figure><h4 id="一元-ufunc"><a href="#一元-ufunc" class="headerlink" title="一元 ufunc"></a>一元 ufunc</h4><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">abs</td><td align="left">计算整数、浮点数或复数的绝对值</td></tr><tr><td align="left">fabs</td><td align="left">更快地计算非复数的绝对值</td></tr><tr><td align="left">sqrt</td><td align="left">计算个元素的平方根</td></tr><tr><td align="left">square</td><td align="left">计算各元素的平方</td></tr><tr><td align="left">exp</td><td align="left">计算各元素的指数 e ** x</td></tr><tr><td align="left">log、log10、log2、log1p</td><td align="left">分别为自然对数（底数为 e）、底数为 10 的 log、底数为 2 的 log、log(1 + x)</td></tr><tr><td align="left">sign</td><td align="left">计算各元素的正负号：1（正数）、0（零）、-1（负数）</td></tr><tr><td align="left">ceil</td><td align="left">计算各元素的 ceiling 值，即大于等于该值的最小整数</td></tr><tr><td align="left">floor</td><td align="left">计算各元素的 floor 值，即小于等于该值的最大整数</td></tr><tr><td align="left">rint</td><td align="left">将各元素四舍五入到最接近的整数，保留 dtype</td></tr><tr><td align="left">modf</td><td align="left">将数组的小数和整数部分以两个独立数组的形式返回</td></tr><tr><td align="left">logical_not</td><td align="left">计算各元素 not x 的真值。相当于 -arr</td></tr></tbody></table><h4 id="二元-ufunc"><a href="#二元-ufunc" class="headerlink" title="二元 ufunc"></a>二元 ufunc</h4><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">add</td><td align="left">将数组中对应的元素相加</td></tr><tr><td align="left">subtract</td><td align="left">从第一个数组中减去第二个数组中的元素</td></tr><tr><td align="left">multiply</td><td align="left">数组元素相乘</td></tr><tr><td align="left">divide、floor_divide</td><td align="left">除法或向下圆整除法（丢弃余数）</td></tr><tr><td align="left">power</td><td align="left">对第一个数组中的元素 A，根据第二个数组中的相应元素 B，计算 A ** B</td></tr><tr><td align="left">mod</td><td align="left">元素级的求模计算（除法的余数）</td></tr><tr><td align="left">greater、greater_equal、less、less_equal、equal、not_equal</td><td align="left">执行元素级的比较运算，最终产生布尔型数组</td></tr><tr><td align="left">logical_and、logical_or、logical_xor</td><td align="left">执行元素级的真值逻辑运算。相当于中缀运算符<code>&amp;</code>、`</td></tr></tbody></table><h3 id="利用数组进行数据处理"><a href="#利用数组进行数据处理" class="headerlink" title="利用数组进行数据处理"></a>利用数组进行数据处理</h3><p>NumPy 数组可以将许多种数据处理任务表述为简洁的数组表达式，使运算速度快上一两个数量级。这种做法通常被称为<strong>矢量化</strong>。</p><h4 id="将条件逻辑表述为数组运算"><a href="#将条件逻辑表述为数组运算" class="headerlink" title="将条件逻辑表述为数组运算"></a>将条件逻辑表述为数组运算</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: xarr = np.array([<span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.3</span>, <span class="number">1.4</span>, <span class="number">1.5</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: yarr = np.array([<span class="number">2.1</span>, <span class="number">2.2</span>, <span class="number">2.3</span>, <span class="number">2.4</span>, <span class="number">2.5</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: cond = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: result = np.where(cond, xarr, yarr)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: result</span><br><span class="line">Out[<span class="number">18</span>]: array([ <span class="number">1.1</span>,  <span class="number">2.2</span>,  <span class="number">1.3</span>,  <span class="number">1.4</span>,  <span class="number">2.5</span>])</span><br></pre></td></tr></table></figure><p><code>np.where</code>的第二个和第三个参数不必是数组，它们都可以是标量值。</p><h3 id="数学和统计方法"><a href="#数学和统计方法" class="headerlink" title="数学和统计方法"></a>数学和统计方法</h3><p>可以通过数组上的一组数学函数对整个数组或某个轴向的数据进行统计计算。<code>sum</code>、<code>mean</code>以及标准差<code>std</code>等聚合计算既可以当做数组的实例方法（<code>arr.mean()</code>）调用，也可以当做顶级 NumPy 函数使用（<code>np.mean(arr)</code>）。</p><p><code>mean</code>和<code>sum</code>这类的函数可以接受一个 axis 参数（用于计算该轴向上的统计值），最终结果是一个少一维的数组。</p><table><thead><tr><th align="left">方法</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">sum</td><td align="left">对数组中全部或某轴向的元素求和。零长度的数组的 sum 为 0</td></tr><tr><td align="left">mean</td><td align="left">算术平均数。零长度的数组的 mean 为 NaN</td></tr><tr><td align="left">std、var</td><td align="left">分别为标准差和方差，自由度可调（默认为 n）</td></tr><tr><td align="left">min、max</td><td align="left">最大值和最小值</td></tr><tr><td align="left">argmin、argmax</td><td align="left">分别为最大和最小元素的索引</td></tr><tr><td align="left">cumsum</td><td align="left">所有元素的累积和</td></tr><tr><td align="left">cumprod</td><td align="left">所有元素的累积值</td></tr></tbody></table><h3 id="用于布尔型数组的方法"><a href="#用于布尔型数组的方法" class="headerlink" title="用于布尔型数组的方法"></a>用于布尔型数组的方法</h3><ul><li><code>any</code>：用于测试数组中是否存在一个或多个 True；</li><li><code>all</code>：检查数组中所有值是否都是 True。</li></ul><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p>多维数组可以通过<code>sort()</code>在任何一个轴向上进行排序，只需将轴编号传入即可：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: arr</span><br><span class="line">Out[<span class="number">4</span>]:</span><br><span class="line">array([[-<span class="number">1.40298298</span>, -<span class="number">0.94302353</span>, -<span class="number">1.2592719</span> ,  <span class="number">1.15911978</span>],</span><br><span class="line">       [-<span class="number">0.53758748</span>,  <span class="number">1.44386652</span>, -<span class="number">1.8916146</span> ,  <span class="number">0.66364269</span>],</span><br><span class="line">       [-<span class="number">0.29874721</span>, -<span class="number">0.10316202</span>,  <span class="number">0.21261665</span>, -<span class="number">2.47289258</span>],</span><br><span class="line">       [-<span class="number">0.34280455</span>,  <span class="number">0.43060591</span>, -<span class="number">0.89851967</span>, -<span class="number">0.73198573</span>],</span><br><span class="line">       [-<span class="number">0.9066756</span> ,  <span class="number">1.28177992</span>,  <span class="number">0.35708335</span>,  <span class="number">0.2792639</span> ]])</span><br><span class="line">       </span><br><span class="line">In [<span class="number">7</span>]: arr.sort(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: arr</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">array([[-<span class="number">1.40298298</span>, -<span class="number">1.2592719</span> , -<span class="number">0.94302353</span>,  <span class="number">1.15911978</span>],</span><br><span class="line">       [-<span class="number">1.8916146</span> , -<span class="number">0.53758748</span>,  <span class="number">0.66364269</span>,  <span class="number">1.44386652</span>],</span><br><span class="line">       [-<span class="number">2.47289258</span>, -<span class="number">0.29874721</span>, -<span class="number">0.10316202</span>,  <span class="number">0.21261665</span>],</span><br><span class="line">       [-<span class="number">0.89851967</span>, -<span class="number">0.73198573</span>, -<span class="number">0.34280455</span>,  <span class="number">0.43060591</span>],</span><br><span class="line">       [-<span class="number">0.9066756</span> ,  <span class="number">0.2792639</span> ,  <span class="number">0.35708335</span>,  <span class="number">1.28177992</span>]])</span><br></pre></td></tr></table></figure><p>顶级方法<code>np.sort</code>返回的是数组的已排序副本，而就地排序则会修改数组本身。</p><h3 id="唯一化以及其他的集合逻辑"><a href="#唯一化以及其他的集合逻辑" class="headerlink" title="唯一化以及其他的集合逻辑"></a>唯一化以及其他的集合逻辑</h3><table><thead><tr><th align="left">方法</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">unique(x)</td><td align="left">计算 x 中的唯一元素，并返回有序结果</td></tr><tr><td align="left">intersect1d(x, y)</td><td align="left">计算 x 和 y 中的公共元素，并返回有序结果</td></tr><tr><td align="left">union1d(x, y)</td><td align="left">计算 x 和 y 的并集，并返回有序结果</td></tr><tr><td align="left">in1d(x, y)</td><td align="left">得到一个表示”x 的元素是否包含于 y“的布尔型数组</td></tr><tr><td align="left">setdiff1d(x, y)</td><td align="left">集合的差，即元素在 x 中且不在 y 中</td></tr><tr><td align="left">setxor1d(x, y)</td><td align="left">集合的异或，即存在于一个数组中但不同时存在于两个数组中的元素</td></tr></tbody></table><h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h3><p>NumPy 提供了一个用于矩阵乘法的<code>dot</code>函数（既是一个数组方法也是 numpy 命名空间中的一个函数）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: x = np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>],[<span class="number">4.</span>,<span class="number">5.</span>,<span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: y = np.array([[<span class="number">6.</span>, <span class="number">23.</span>], [-<span class="number">1</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: x.dot(y)</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">array([[  <span class="number">28.</span>,   <span class="number">64.</span>],</span><br><span class="line">       [  <span class="number">67.</span>,  <span class="number">181.</span>]])</span><br></pre></td></tr></table></figure><p>numpy.linalg 中有一组标准的矩阵分解运算以及诸如求逆和行列式之类的东西。</p><h4 id="常用的-numpy-linalg-函数"><a href="#常用的-numpy-linalg-函数" class="headerlink" title="常用的 numpy.linalg 函数"></a>常用的 numpy.linalg 函数</h4><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">diag</td><td align="left">以一维数组的形式返回方阵的对角线（或非对角线）元素，或将一维数组转换为方阵（非对角线元素为 0）</td></tr><tr><td align="left">dot</td><td align="left">矩阵乘法</td></tr><tr><td align="left">trace</td><td align="left">计算对角线元素的和</td></tr><tr><td align="left">det</td><td align="left">计算矩阵行列式</td></tr><tr><td align="left">eig</td><td align="left">计算方阵的本征值和本征向量</td></tr><tr><td align="left">inv</td><td align="left">计算方阵的逆</td></tr><tr><td align="left">svd</td><td align="left">计算奇异值分解（SVD）</td></tr><tr><td align="left">solve</td><td align="left">解线性方程组 Ax = b，其中 A 为一个方阵</td></tr><tr><td align="left">lstsq</td><td align="left">计算 Ax = b 的最小二乘解</td></tr></tbody></table><h3 id="随机数生成"><a href="#随机数生成" class="headerlink" title="随机数生成"></a>随机数生成</h3><p>numpy.random 模块对 Python 内置的<code>random</code>进行了补充，增加了一些用于高效生成多种概率分布的样本值的函数。例如用<code>normal</code>来得到一个标准正态分布的多维数组：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">16</span>]: samples = np.random.normal(size=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: samples</span><br><span class="line">Out[<span class="number">17</span>]:</span><br><span class="line">array([[ <span class="number">0.58779653</span>,  <span class="number">0.69406318</span>,  <span class="number">0.28244585</span>,  <span class="number">0.25620597</span>],</span><br><span class="line">       [ <span class="number">0.95854832</span>,  <span class="number">0.55800493</span>,  <span class="number">0.17702977</span>, -<span class="number">0.06442143</span>],</span><br><span class="line">       [-<span class="number">1.60361823</span>, -<span class="number">0.00796763</span>, -<span class="number">0.48375863</span>, -<span class="number">1.59447872</span>],</span><br><span class="line">       [ <span class="number">0.80866648</span>, -<span class="number">1.75706417</span>,  <span class="number">0.03793649</span>,  <span class="number">0.18152227</span>]])</span><br></pre></td></tr></table></figure><h4 id="部分-numpy-random-函数"><a href="#部分-numpy-random-函数" class="headerlink" title="部分 numpy.random 函数"></a>部分 numpy.random 函数</h4><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">seed</td><td align="left">确定随机数生成器的种子</td></tr><tr><td align="left">permutation</td><td align="left">返回一个序列的随机排列或返回一个随机排列的范围</td></tr><tr><td align="left">shuffle</td><td align="left">对一个序列就地随机排列</td></tr><tr><td align="left">rand</td><td align="left">产生均匀分布的样本值</td></tr><tr><td align="left">randint</td><td align="left">从给定的上下限范围内随机选取整数</td></tr><tr><td align="left">randn</td><td align="left">产生正态分布（平均值为 0，标准差为 1）的样本值</td></tr><tr><td align="left">binomial</td><td align="left">产生二项分布的样本值</td></tr><tr><td align="left">normal</td><td align="left">产生正态（高斯）分布的样本值</td></tr><tr><td align="left">beta</td><td align="left">产生 Beta 分布的样本值</td></tr><tr><td align="left">chisquare</td><td align="left">产生卡方分布的样本值</td></tr><tr><td align="left">gamma</td><td align="left">产生 Gamma 分布的样本值</td></tr><tr><td align="left">uniform</td><td align="left">产生在 [0,1) 中均匀分布的样本值</td></tr></tbody></table><h2 id="pandas-入门"><a href="#pandas-入门" class="headerlink" title="pandas 入门"></a>pandas 入门</h2><p>pandas 基于 NumPy 构建，含有使数据分析工作变得更快更简单的高级数据结构和操作工具。</p><p>pandas 引入约定：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> Series, DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h3 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h3><p>Series 是一种<strong>类似于一维数组</strong>的对象，它由一组数据（各种 NumPy 数据类型）以及一组与之相关的数据标签（即索引）组成：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: obj = Series([<span class="number">4</span>, <span class="number">7</span>, -<span class="number">5</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: obj</span><br><span class="line">Out[<span class="number">4</span>]:</span><br><span class="line"><span class="number">0</span>    <span class="number">4</span></span><br><span class="line"><span class="number">1</span>    <span class="number">7</span></span><br><span class="line"><span class="number">2</span>   -<span class="number">5</span></span><br><span class="line"><span class="number">3</span>    <span class="number">3</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure><p>Series 的字符串表现形式为：索引在左边，值在右边。可以通过 Series 的 values 和 index 属性获取其数组表示形式和索引对象：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">5</span>]: obj.values</span><br><span class="line">Out[<span class="number">5</span>]: array([ <span class="number">4</span>,  <span class="number">7</span>, -<span class="number">5</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: obj.index</span><br><span class="line">Out[<span class="number">6</span>]: RangeIndex(start=<span class="number">0</span>, stop=<span class="number">4</span>, step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>在创建 Series 时可以指定各个数据点的索引：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: obj2 = Series([<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], index=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: obj2</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">d    <span class="number">4</span></span><br><span class="line">a    <span class="number">2</span></span><br><span class="line">b    <span class="number">3</span></span><br><span class="line">c    <span class="number">5</span></span><br><span class="line">dtype: int64</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: obj2.index</span><br><span class="line">Out[<span class="number">9</span>]: Index([<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br></pre></td></tr></table></figure><p>与普通 NumPy 数组相比，可以通过索引来选取 Series 中的单个或一组值：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: obj2[<span class="string">&#x27;a&#x27;</span>]</span><br><span class="line">Out[<span class="number">10</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: obj2[<span class="string">&#x27;d&#x27;</span>] = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: obj2[[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>]]</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">d    <span class="number">6</span></span><br><span class="line">a    <span class="number">2</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure><p>NumPy 数组运算（根据布尔型数组进行过滤、标量乘法、应用数学函数等）都会保留索引和值之间的链接。</p><p>还可以将 Series 看成一个<strong>定长的有序字典</strong>，因为它是索引值到数据值的一个映射。它可以用在许多原本需要字典参数的函数中。</p><p>如果数据被存放在一个 Python 字典中，也可以直接通过这个字典来创建 Series。可以只传入一个字典（结果 Series 中的索引（有序排列）就是原字典的键），也可以同时传入一个 index 参数，字典中与索引相匹配的值被放到相应的位置上，<strong>找不到的值则为 NaN</strong>（数据缺失）。pandas 的<code>isnull</code>和<code>notnull</code>函数可用于检测缺失数据。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: sdata = &#123;<span class="string">&#x27;hello&#x27;</span>: <span class="number">100</span>, <span class="string">&#x27;python&#x27;</span>: <span class="number">200</span>, <span class="string">&#x27;pandas&#x27;</span>: <span class="number">300</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: states = [<span class="string">&#x27;hello&#x27;</span>, <span class="string">&#x27;pandas&#x27;</span>, <span class="string">&#x27;numpy&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: obj3 = Series(sdata, index=states)</span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: obj3</span><br><span class="line">Out[<span class="number">16</span>]:</span><br><span class="line">hello     <span class="number">100.0</span></span><br><span class="line">pandas    <span class="number">300.0</span></span><br><span class="line">numpy       NaN</span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: pd.isnull(obj3)</span><br><span class="line">Out[<span class="number">17</span>]:</span><br><span class="line">hello     <span class="literal">False</span></span><br><span class="line">pandas    <span class="literal">False</span></span><br><span class="line">numpy      <span class="literal">True</span></span><br><span class="line">dtype: <span class="built_in">bool</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: obj3.isnull()</span><br><span class="line">Out[<span class="number">18</span>]:</span><br><span class="line">hello     <span class="literal">False</span></span><br><span class="line">pandas    <span class="literal">False</span></span><br><span class="line">numpy      <span class="literal">True</span></span><br><span class="line">dtype: <span class="built_in">bool</span></span><br></pre></td></tr></table></figure><p>Series 很重要的一个功能是：它在算数运算中会<strong>自动对齐不同索引的数据</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">19</span>]: obj4 = Series(sdata)</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: obj4</span><br><span class="line">Out[<span class="number">20</span>]:</span><br><span class="line">hello     <span class="number">100</span></span><br><span class="line">pandas    <span class="number">300</span></span><br><span class="line">python    <span class="number">200</span></span><br><span class="line">dtype: int64</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: obj3 + obj4</span><br><span class="line">Out[<span class="number">21</span>]:</span><br><span class="line">hello     <span class="number">200.0</span></span><br><span class="line">numpy       NaN</span><br><span class="line">pandas    <span class="number">600.0</span></span><br><span class="line">python      NaN</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>Series 对象本身及其索引都带有一个 name 属性，该属性跟 pandas 其他的关键功能关系非常密切：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">23</span>]: obj4.name = <span class="string">&#x27;dict&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: obj4.index.name = <span class="string">&#x27;words&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: obj4</span><br><span class="line">Out[<span class="number">25</span>]:</span><br><span class="line">words</span><br><span class="line">hello     <span class="number">100</span></span><br><span class="line">pandas    <span class="number">300</span></span><br><span class="line">python    <span class="number">200</span></span><br><span class="line">Name: <span class="built_in">dict</span>, dtype: int64</span><br></pre></td></tr></table></figure><p>Series 的索引可以通过赋值的方式就地修改：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">26</span>]: obj4.index = [<span class="string">&#x27;bye&#x27;</span>, <span class="string">&#x27;little&#x27;</span>, <span class="string">&#x27;girl&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: obj4</span><br><span class="line">Out[<span class="number">27</span>]:</span><br><span class="line">bye       <span class="number">100</span></span><br><span class="line">little    <span class="number">300</span></span><br><span class="line">girl      <span class="number">200</span></span><br><span class="line">Name: <span class="built_in">dict</span>, dtype: int64</span><br></pre></td></tr></table></figure><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>DataFrame 是一个<strong>表格型</strong>的数据结构，它含有一组有序的列，每列可以是不同的值类型（数值、字符串、布尔型等）。DataFrame 既有行索引也有列索引，可以被看作由 Series 组成的字典（共用同一个索引）。</p><p>DataFrame 是以二维结构保存数据的，但可以被表示为<strong>层次化索引的表格型结构</strong>，这是 pandas 中许多高级数据处理功能的关键要素。</p><p>构建 DataFrame 的办法有很多，最常用的一种是直接传入一个由等长列表或 NumPy 数组组成的字典，DataFrame 会自动加上索引，且全部列会被有序排列：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: data = &#123;<span class="string">&#x27;state&#x27;</span>: [<span class="string">&#x27;well&#x27;</span>, <span class="string">&#x27;good&#x27;</span>, <span class="string">&#x27;soso&#x27;</span>, <span class="string">&#x27;bad&#x27;</span>], <span class="string">&#x27;year&#x27;</span>: [<span class="number">2000</span>, <span class="number">2001</span>, <span class="number">2002</span>, <span class="number">2003</span>], <span class="string">&#x27;nums&#x27;</span>:</span><br><span class="line">   ...: [<span class="number">1.1</span>, <span class="number">2.3</span>, <span class="number">4.2</span>, <span class="number">5.4</span>]&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: frame = DataFrame(data)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: frame</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">   nums state  year</span><br><span class="line"><span class="number">0</span>   <span class="number">1.1</span>  well  <span class="number">2000</span></span><br><span class="line"><span class="number">1</span>   <span class="number">2.3</span>  good  <span class="number">2001</span></span><br><span class="line"><span class="number">2</span>   <span class="number">4.2</span>  soso  <span class="number">2002</span></span><br><span class="line"><span class="number">3</span>   <span class="number">5.4</span>   bad  <span class="number">2003</span></span><br></pre></td></tr></table></figure><p>如果指定了列序列，则 DataFrame 的列就会按照指定顺序进行排列。如果传入的列找不到，会和 Series 一样产生 NaN 值：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: frames2 = DataFrame(data, columns=[<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;nums&#x27;</span>, <span class="string">&#x27;debt&#x27;</span>], index=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;</span></span><br><span class="line"><span class="string">    ...: three&#x27;</span>, <span class="string">&#x27;four&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: frames2</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">       year state  nums debt</span><br><span class="line">one    <span class="number">2000</span>  well   <span class="number">1.1</span>  NaN</span><br><span class="line">two    <span class="number">2001</span>  good   <span class="number">2.3</span>  NaN</span><br><span class="line">three  <span class="number">2002</span>  soso   <span class="number">4.2</span>  NaN</span><br><span class="line">four   <span class="number">2003</span>   bad   <span class="number">5.4</span>  NaN</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: frames2.columns</span><br><span class="line">Out[<span class="number">12</span>]: Index([<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;nums&#x27;</span>, <span class="string">&#x27;debt&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br></pre></td></tr></table></figure><p>通过类似字典标记的方式或属性的方式，可以将 DataFrame 的列获取为一个Series：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: frames2[<span class="string">&#x27;state&#x27;</span>] <span class="comment"># = frames2.state</span></span><br><span class="line">Out[<span class="number">13</span>]:</span><br><span class="line">one      well</span><br><span class="line">two      good</span><br><span class="line">three    soso</span><br><span class="line">four      bad</span><br><span class="line">Name: state, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure><p>行也可以通过位置或名称的方式进行获取，比如用索引字段<code>ix</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">15</span>]: frames2.ix[<span class="string">&#x27;one&#x27;</span>]</span><br><span class="line">Out[<span class="number">15</span>]:</span><br><span class="line">year     <span class="number">2000</span></span><br><span class="line">state    well</span><br><span class="line">nums      <span class="number">1.1</span></span><br><span class="line">debt      NaN</span><br><span class="line">Name: one, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure><p>列可以通过<strong>赋值</strong>的方式进行修改。将列表或数组赋值给某个列时，其长度必须跟 DataFrame 的长度相匹配。如果赋值的是一个 Series，就会精确匹配 DataFrame 的索引，所有的空位都将被填上缺失值。为不存在的列赋值会创建出一个新列。关键字 del 用于删除列：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">16</span>]: frames2[<span class="string">&#x27;eastern&#x27;</span>] = frames2.state == <span class="string">&#x27;Ohio&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: frames2</span><br><span class="line">Out[<span class="number">17</span>]:</span><br><span class="line">       year state  nums debt eastern</span><br><span class="line">one    <span class="number">2000</span>  well   <span class="number">1.1</span>  NaN   <span class="literal">False</span></span><br><span class="line">two    <span class="number">2001</span>  good   <span class="number">2.3</span>  NaN   <span class="literal">False</span></span><br><span class="line">three  <span class="number">2002</span>  soso   <span class="number">4.2</span>  NaN   <span class="literal">False</span></span><br><span class="line">four   <span class="number">2003</span>   bad   <span class="number">5.4</span>  NaN   <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="keyword">del</span> frames2[<span class="string">&#x27;eastern&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: frames2.columns</span><br><span class="line">Out[<span class="number">19</span>]: Index([<span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;nums&#x27;</span>, <span class="string">&#x27;debt&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>，通过索引方式返回的列只是相应数据的<strong>视图</strong>而非副本。因此，对返回的 Series 所做的任何就地修改全都会反映到源 DataFrame 上。通过 Series 的 copy 方法即可显式地复制列。</p><p>另一种常见的数据类型是嵌套字典。将其传给 DataFrame，会被解释为：外层字典的键作为列，内层键则作为行索引：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">20</span>]: pop = &#123;<span class="string">&#x27;niboer&#x27;</span>: &#123;<span class="number">2024</span>: <span class="number">2.4</span>, <span class="number">2027</span>: <span class="number">4.3</span>&#125;, <span class="string">&#x27;huacun&#x27;</span>: &#123;<span class="number">2024</span>: <span class="number">1.4</span>, <span class="number">2025</span>:<span class="number">3.2</span>, <span class="number">2027</span>: <span class="number">6.4</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: frames = DataFrame(pop)</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: frames</span><br><span class="line">Out[<span class="number">22</span>]:</span><br><span class="line">      huacun  niboer</span><br><span class="line"><span class="number">2024</span>     <span class="number">1.4</span>     <span class="number">2.4</span></span><br><span class="line"><span class="number">2025</span>     <span class="number">3.2</span>     NaN</span><br><span class="line"><span class="number">2027</span>     <span class="number">6.4</span>     <span class="number">4.3</span></span><br></pre></td></tr></table></figure><p>也可以对该结果进行转置：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">23</span>]: frames.T</span><br><span class="line">Out[<span class="number">23</span>]:</span><br><span class="line">        <span class="number">2024</span>  <span class="number">2025</span>  <span class="number">2027</span></span><br><span class="line">huacun   <span class="number">1.4</span>   <span class="number">3.2</span>   <span class="number">6.4</span></span><br><span class="line">niboer   <span class="number">2.4</span>   NaN   <span class="number">4.3</span></span><br></pre></td></tr></table></figure><p>如果 DataFrame 各列的数据类型不同，则值数组的数据类型就会选用能兼容所有列的数据类型。</p><h3 id="索引对象"><a href="#索引对象" class="headerlink" title="索引对象"></a>索引对象</h3><p>pandas 的索引对象负责管理轴标签和其他元数据（比如轴名称等）。构建 Series 或 DataFrame 时，所用到的任何数组或其他序列的标签都会被转换成一个 Index 对象：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">24</span>]: obj = Series(<span class="built_in">range</span>(<span class="number">3</span>), index=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: index = obj.index</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: index</span><br><span class="line">Out[<span class="number">26</span>]: Index([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: index[<span class="number">1</span>:]</span><br><span class="line">Out[<span class="number">27</span>]: Index([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Index 对象是不可修改的。这种设计使 Index 对象在多个数据结构之间安全共享：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">31</span>]: index = pd.Index(np.arange(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: obj2 = Series([<span class="number">1.5</span>, -<span class="number">2.5</span>, <span class="number">0</span>], index=index)</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: obj2.index <span class="keyword">is</span> index</span><br><span class="line">Out[<span class="number">33</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>Index 的方法和属性见原书 p126（pdf p137）。</p><h3 id="基本功能"><a href="#基本功能" class="headerlink" title="基本功能"></a>基本功能</h3><h4 id="重新索引（reindex）"><a href="#重新索引（reindex）" class="headerlink" title="重新索引（reindex）"></a>重新索引（<code>reindex</code>）</h4><p>调用某个 **Series 的<code>reindex</code>**方法将会根据新索引进行重排。如果某个索引值当前不存在，就引入缺失值：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: obj = Series([<span class="number">4.5</span>, <span class="number">7.2</span>, -<span class="number">5.3</span>, <span class="number">3.6</span>], index=[<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: obj = Series([<span class="number">4.5</span>, <span class="number">7.2</span>, -<span class="number">5.3</span>, <span class="number">3.6</span>], index=[<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: obj</span><br><span class="line">Out[<span class="number">5</span>]:</span><br><span class="line">d    <span class="number">4.5</span></span><br><span class="line">b    <span class="number">7.2</span></span><br><span class="line">a   -<span class="number">5.3</span></span><br><span class="line">c    <span class="number">3.6</span></span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: obj2 = obj.reindex([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: obj2</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">a   -<span class="number">5.3</span></span><br><span class="line">b    <span class="number">7.2</span></span><br><span class="line">c    <span class="number">3.6</span></span><br><span class="line">d    <span class="number">4.5</span></span><br><span class="line">e    NaN</span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: obj.reindex([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>], fill_value=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">a   -<span class="number">5.3</span></span><br><span class="line">b    <span class="number">7.2</span></span><br><span class="line">c    <span class="number">3.6</span></span><br><span class="line">d    <span class="number">4.5</span></span><br><span class="line">e    <span class="number">0.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>对于时间序列这样的有序数据，重新索引时可能需要做一些插值处理。<code>method</code>选项即可到达此目的，例如使用<code>ffill</code>可以实现前向值填充（<code>bfill</code>实现向后插值）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: obj3 = Series([<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;purple&#x27;</span>, <span class="string">&#x27;yellow&#x27;</span>], index=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: obj3.reindex(<span class="built_in">range</span>(<span class="number">6</span>), method=<span class="string">&#x27;ffill&#x27;</span>)</span><br><span class="line">Out[<span class="number">10</span>]:</span><br><span class="line"><span class="number">0</span>      blue</span><br><span class="line"><span class="number">1</span>      blue</span><br><span class="line"><span class="number">2</span>    purple</span><br><span class="line"><span class="number">3</span>    purple</span><br><span class="line"><span class="number">4</span>    yellow</span><br><span class="line"><span class="number">5</span>    yellow</span><br><span class="line">dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure><p>对于 <strong>DataFrame</strong>，<code>reindex</code>可以修改（行）索引、列，或两个都修改。如果仅传入一个序列，则会重新索引行。使用<code>columns</code>关键字即可重新索引列。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: frame = DataFrame(np.arange(<span class="number">9</span>).reshape((<span class="number">3</span>,<span class="number">3</span>)), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>],columns=[<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Texa</span></span><br><span class="line"><span class="string">    ...: s&#x27;</span>, <span class="string">&#x27;California&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: frame</span><br><span class="line">Out[<span class="number">14</span>]:</span><br><span class="line">   Ohio  Texas  California</span><br><span class="line">a     <span class="number">0</span>      <span class="number">1</span>           <span class="number">2</span></span><br><span class="line">c     <span class="number">3</span>      <span class="number">4</span>           <span class="number">5</span></span><br><span class="line">d     <span class="number">6</span>      <span class="number">7</span>           <span class="number">8</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: frame.reindex(columns=[<span class="string">&#x27;Texas&#x27;</span>, <span class="string">&#x27;California&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>])</span><br><span class="line">Out[<span class="number">18</span>]:</span><br><span class="line">   Texas  California  Ohio</span><br><span class="line">a      <span class="number">1</span>           <span class="number">2</span>     <span class="number">0</span></span><br><span class="line">c      <span class="number">4</span>           <span class="number">5</span>     <span class="number">3</span></span><br><span class="line">d      <span class="number">7</span>           <span class="number">8</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure><p>也可以同时对行和列进行重新索引，而插值则只能按行应用（即轴 0）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">19</span>]: frame.reindex(index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>], method=<span class="string">&#x27;ffill&#x27;</span>, columns=[<span class="string">&#x27;Texas&#x27;</span>, <span class="string">&#x27;California&#x27;</span>, <span class="string">&#x27;</span></span><br><span class="line"><span class="string">    ...: Ohio&#x27;</span>])</span><br><span class="line">Out[<span class="number">19</span>]:</span><br><span class="line">   Texas  California  Ohio</span><br><span class="line">a      <span class="number">1</span>           <span class="number">2</span>     <span class="number">0</span></span><br><span class="line">b      <span class="number">1</span>           <span class="number">2</span>     <span class="number">0</span></span><br><span class="line">c      <span class="number">4</span>           <span class="number">5</span>     <span class="number">3</span></span><br><span class="line">d      <span class="number">7</span>           <span class="number">8</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure><p>利用<code>ix</code>的标签索引功能，重新索引任务可以变得更简洁。</p><p><code>reindex</code>函数的各参数及说明见原书 p129（pdf p140）。</p><h4 id="丢弃指定轴上的项"><a href="#丢弃指定轴上的项" class="headerlink" title="丢弃指定轴上的项"></a>丢弃指定轴上的项</h4><p>对于 Series，<code>drop</code>方法返回一个在指定轴上删除了指定值的新对象：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: obj = Series(np.arange(<span class="number">5.</span>), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: new_obj = obj.drop(<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: new_obj</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">a    <span class="number">0.0</span></span><br><span class="line">b    <span class="number">1.0</span></span><br><span class="line">d    <span class="number">3.0</span></span><br><span class="line">e    <span class="number">4.0</span></span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: obj.drop([<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">a    <span class="number">0.0</span></span><br><span class="line">b    <span class="number">1.0</span></span><br><span class="line">e    <span class="number">4.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>对于 DataFrame，可以删除任意轴上的索引值：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: frames</span><br><span class="line">Out[<span class="number">10</span>]:</span><br><span class="line">      huacun  niboer</span><br><span class="line"><span class="number">2024</span>     <span class="number">1.4</span>     <span class="number">2.4</span></span><br><span class="line"><span class="number">2025</span>     <span class="number">3.2</span>     NaN</span><br><span class="line"><span class="number">2027</span>     <span class="number">6.4</span>     <span class="number">4.3</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: frames.drop([<span class="string">&#x27;huacun&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">13</span>]:</span><br><span class="line">      niboer</span><br><span class="line"><span class="number">2024</span>     <span class="number">2.4</span></span><br><span class="line"><span class="number">2025</span>     NaN</span><br><span class="line"><span class="number">2027</span>     <span class="number">4.3</span></span><br></pre></td></tr></table></figure><h4 id="索引、选取和过滤"><a href="#索引、选取和过滤" class="headerlink" title="索引、选取和过滤"></a>索引、选取和过滤</h4><p>Series 索引的工作方式类似于 NumPy 数组的索引，只不过不只是整数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: obj = Series(np.arange(<span class="number">4.</span>), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: obj[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: obj[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: obj[<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">c    <span class="number">2.0</span></span><br><span class="line">d    <span class="number">3.0</span></span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: obj[[<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]]</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">b    <span class="number">1.0</span></span><br><span class="line">a    <span class="number">0.0</span></span><br><span class="line">c    <span class="number">2.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>利用标签的切片运算与普通的 Python 切片运算不同，其末端是<strong>包含的</strong>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: obj[<span class="string">&#x27;b&#x27;</span>:<span class="string">&#x27;c&#x27;</span>]</span><br><span class="line">Out[<span class="number">9</span>]:</span><br><span class="line">b    <span class="number">1.0</span></span><br><span class="line">c    <span class="number">2.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>而对于 DataFrame，进行索引是在获取一个或多个<strong>列</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: data = DataFrame(np.arange(<span class="number">16</span>).reshape((<span class="number">4</span>, <span class="number">4</span>)), index=[<span class="string">&#x27;Obio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>, <span class="string">&#x27;Utah&#x27;</span>, <span class="string">&#x27;New Yor</span></span><br><span class="line"><span class="string">    ...: k&#x27;</span>], columns=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>, <span class="string">&#x27;four&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: data[<span class="string">&#x27;two&#x27;</span>]</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">Obio         <span class="number">1</span></span><br><span class="line">Colorado     <span class="number">5</span></span><br><span class="line">Utah         <span class="number">9</span></span><br><span class="line">New York    <span class="number">13</span></span><br><span class="line">Name: two, dtype: int64</span><br></pre></td></tr></table></figure><p>想选取<strong>行</strong>有以下两种方式：</p><ul><li>通过切片或布尔型数组选取行：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: data[:<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">          one  two  three  four</span><br><span class="line">Obio        <span class="number">0</span>    <span class="number">1</span>      <span class="number">2</span>     <span class="number">3</span></span><br><span class="line">Colorado    <span class="number">4</span>    <span class="number">5</span>      <span class="number">6</span>     <span class="number">7</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: data[data[<span class="string">&#x27;three&#x27;</span>] &gt; <span class="number">5</span>]</span><br><span class="line">Out[<span class="number">13</span>]:</span><br><span class="line">          one  two  three  four</span><br><span class="line">Colorado    <span class="number">4</span>    <span class="number">5</span>      <span class="number">6</span>     <span class="number">7</span></span><br><span class="line">Utah        <span class="number">8</span>    <span class="number">9</span>     <span class="number">10</span>    <span class="number">11</span></span><br><span class="line">New York   <span class="number">12</span>   <span class="number">13</span>     <span class="number">14</span>    <span class="number">15</span></span><br></pre></td></tr></table></figure><ul><li>通过布尔型 DataFrame 进行索引：</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: data &lt; <span class="number">5</span></span><br><span class="line">Out[<span class="number">14</span>]:</span><br><span class="line">            one    two  three   four</span><br><span class="line">Obio       <span class="literal">True</span>   <span class="literal">True</span>   <span class="literal">True</span>   <span class="literal">True</span></span><br><span class="line">Colorado   <span class="literal">True</span>  <span class="literal">False</span>  <span class="literal">False</span>  <span class="literal">False</span></span><br><span class="line">Utah      <span class="literal">False</span>  <span class="literal">False</span>  <span class="literal">False</span>  <span class="literal">False</span></span><br><span class="line">New York  <span class="literal">False</span>  <span class="literal">False</span>  <span class="literal">False</span>  <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: data[data &lt; <span class="number">5</span>]</span><br><span class="line">Out[<span class="number">15</span>]:</span><br><span class="line">          one  two  three  four</span><br><span class="line">Obio      <span class="number">0.0</span>  <span class="number">1.0</span>    <span class="number">2.0</span>   <span class="number">3.0</span></span><br><span class="line">Colorado  <span class="number">4.0</span>  NaN    NaN   NaN</span><br><span class="line">Utah      NaN  NaN    NaN   NaN</span><br><span class="line">New York  NaN  NaN    NaN   NaN</span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: data[data &lt; <span class="number">5</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: data</span><br><span class="line">Out[<span class="number">17</span>]:</span><br><span class="line">          one  two  three  four</span><br><span class="line">Obio        <span class="number">0</span>    <span class="number">0</span>      <span class="number">0</span>     <span class="number">0</span></span><br><span class="line">Colorado    <span class="number">0</span>    <span class="number">5</span>      <span class="number">6</span>     <span class="number">7</span></span><br><span class="line">Utah        <span class="number">8</span>    <span class="number">9</span>     <span class="number">10</span>    <span class="number">11</span></span><br><span class="line">New York   <span class="number">12</span>   <span class="number">13</span>     <span class="number">14</span>    <span class="number">15</span></span><br></pre></td></tr></table></figure><p>为了在 DataFrame 的行上进行标签索引，可以使用专门的索引字段<code>ix</code>，以通过 NumPy 式的标记法以及轴标签从 DataFrame 中选取行和列的子集。这也是一种重新索引的简单手段。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">18</span>]: data.ix[<span class="string">&#x27;Colorado&#x27;</span>, [<span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]]</span><br><span class="line">Out[<span class="number">18</span>]:</span><br><span class="line">two      <span class="number">5</span></span><br><span class="line">three    <span class="number">6</span></span><br><span class="line">Name: Colorado, dtype: int64</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">20</span>]: data.ix[[<span class="string">&#x27;Colorado&#x27;</span>, <span class="string">&#x27;Utah&#x27;</span>], [<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">Out[<span class="number">20</span>]:</span><br><span class="line">          four  one  two</span><br><span class="line">Colorado     <span class="number">7</span>    <span class="number">0</span>    <span class="number">5</span></span><br><span class="line">Utah        <span class="number">11</span>    <span class="number">8</span>    <span class="number">9</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: data.ix[<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">21</span>]:</span><br><span class="line">one       <span class="number">8</span></span><br><span class="line">two       <span class="number">9</span></span><br><span class="line">three    <span class="number">10</span></span><br><span class="line">four     <span class="number">11</span></span><br><span class="line">Name: Utah, dtype: int64</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">22</span>]: data.ix[:<span class="string">&#x27;Utah&#x27;</span>, <span class="string">&#x27;two&#x27;</span>]</span><br><span class="line">Out[<span class="number">22</span>]:</span><br><span class="line">Obio        <span class="number">0</span></span><br><span class="line">Colorado    <span class="number">5</span></span><br><span class="line">Utah        <span class="number">9</span></span><br><span class="line">Name: two, dtype: int64</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">23</span>]: data.ix[data.three &gt; <span class="number">5</span>, :<span class="number">3</span>]</span><br><span class="line">Out[<span class="number">23</span>]:</span><br><span class="line">          one  two  three</span><br><span class="line">Colorado    <span class="number">0</span>    <span class="number">5</span>      <span class="number">6</span></span><br><span class="line">Utah        <span class="number">8</span>    <span class="number">9</span>     <span class="number">10</span></span><br><span class="line">New York   <span class="number">12</span>   <span class="number">13</span>     <span class="number">14</span></span><br></pre></td></tr></table></figure><p>DataFrame 的索引选项见原书 p132（pdf p143）。</p><h4 id="算术运算和数据对齐"><a href="#算术运算和数据对齐" class="headerlink" title="算术运算和数据对齐"></a>算术运算和数据对齐</h4><p>pandas 最重要的一个功能是<strong>对不同索引的对象进行算数运算</strong>。在将对象相加时，如果存在不同的索引对，则结果的索引就是该索引对的并集。自动的数据对齐操作在不重叠的索引处引入了 NaN 值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">24</span>]: s1 = Series([<span class="number">7.3</span>, -<span class="number">2.5</span>, <span class="number">3.4</span>, <span class="number">1.5</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: s2 = Series([-<span class="number">2.1</span>, <span class="number">3.6</span>, -<span class="number">1.5</span>, <span class="number">4</span>, <span class="number">3.1</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: s1+s2</span><br><span class="line">Out[<span class="number">26</span>]:</span><br><span class="line">a    <span class="number">5.2</span></span><br><span class="line">c    <span class="number">1.1</span></span><br><span class="line">d    NaN</span><br><span class="line">e    <span class="number">0.0</span></span><br><span class="line">f    NaN</span><br><span class="line">g    NaN</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>对于 DataFrame，对齐操作会同时发生在行和列上。</p><h5 id="在算术方法中填充值"><a href="#在算术方法中填充值" class="headerlink" title="在算术方法中填充值"></a>在算术方法中填充值</h5><p>在使用<code>add</code>或者<code>reindex</code>等方法时，可以通过<code>fill_value</code>参数指定一个填充值.</p><h5 id="DataFrame-和-Series-之间的运算"><a href="#DataFrame-和-Series-之间的运算" class="headerlink" title="DataFrame 和 Series 之间的运算"></a>DataFrame 和 Series 之间的运算</h5><p>默认情况下，DataFrame 和 Series 之间的算术运算会将 Series 的索引匹配到 DataFrame 的<strong>列</strong>，然后沿着行一直向下<strong>广播</strong>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">27</span>]: frame = DataFrame(np.arange(<span class="number">12.</span>).reshape((<span class="number">4</span>,<span class="number">3</span>)), columns=<span class="built_in">list</span>(<span class="string">&#x27;bde&#x27;</span>), index=[<span class="string">&#x27;Obio&#x27;</span>, <span class="string">&#x27;Colorado&#x27;</span>, <span class="string">&#x27;Utah&#x27;</span>, <span class="string">&#x27;New York&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: series = frame.ix[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: frame</span><br><span class="line">Out[<span class="number">29</span>]:</span><br><span class="line">            b     d     e</span><br><span class="line">Obio      <span class="number">0.0</span>   <span class="number">1.0</span>   <span class="number">2.0</span></span><br><span class="line">Colorado  <span class="number">3.0</span>   <span class="number">4.0</span>   <span class="number">5.0</span></span><br><span class="line">Utah      <span class="number">6.0</span>   <span class="number">7.0</span>   <span class="number">8.0</span></span><br><span class="line">New York  <span class="number">9.0</span>  <span class="number">10.0</span>  <span class="number">11.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: series</span><br><span class="line">Out[<span class="number">30</span>]:</span><br><span class="line">b    <span class="number">0.0</span></span><br><span class="line">d    <span class="number">1.0</span></span><br><span class="line">e    <span class="number">2.0</span></span><br><span class="line">Name: Obio, dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">31</span>]: frame - series</span><br><span class="line">Out[<span class="number">31</span>]:</span><br><span class="line">            b    d    e</span><br><span class="line">Obio      <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">0.0</span></span><br><span class="line">Colorado  <span class="number">3.0</span>  <span class="number">3.0</span>  <span class="number">3.0</span></span><br><span class="line">Utah      <span class="number">6.0</span>  <span class="number">6.0</span>  <span class="number">6.0</span></span><br><span class="line">New York  <span class="number">9.0</span>  <span class="number">9.0</span>  <span class="number">9.0</span></span><br></pre></td></tr></table></figure><p>如果某个索引值在 DataFrame 的列或 Series 的索引中找不到，则参与运算的两个对象就会被重新索引以形成并集。</p><p>如果希望匹配行且在列上广播，则必须使用算数运算方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">32</span>]: series3 = frame[<span class="string">&#x27;d&#x27;</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: series3</span><br><span class="line">Out[<span class="number">33</span>]:</span><br><span class="line">Obio         <span class="number">1.0</span></span><br><span class="line">Colorado     <span class="number">4.0</span></span><br><span class="line">Utah         <span class="number">7.0</span></span><br><span class="line">New York    <span class="number">10.0</span></span><br><span class="line">Name: d, dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: frame.sub(series3, axis=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">35</span>]:</span><br><span class="line">            b    d    e</span><br><span class="line">Obio     -<span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span></span><br><span class="line">Colorado -<span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span></span><br><span class="line">Utah     -<span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span></span><br><span class="line">New York -<span class="number">1.0</span>  <span class="number">0.0</span>  <span class="number">1.0</span></span><br></pre></td></tr></table></figure><p>传入的轴号就是希望匹配的轴。</p><h4 id="函数应用和映射"><a href="#函数应用和映射" class="headerlink" title="函数应用和映射"></a>函数应用和映射</h4><p>NumPy 的<code>ufuncs</code>（元素级数组方法）也可以用于操作 pandas 对象：<code>np.abs(frame)</code>。</p><p>另一个常见的操作是，将函数应用到由各列或行所形成的一维数组上，DataFrame 的<code>apply</code>方法即可实现此功能。通过<code>applymap</code>也可以使用元素级的 Python 函数（Series 有一个用于应用元素级函数的<code>map</code>方法）。</p><h4 id="排序和排名"><a href="#排序和排名" class="headerlink" title="排序和排名"></a>排序和排名</h4><p>要对行或列索引进行排序（字典顺序），可使用<code>sort_index</code>方法来返回一个已排序的新对象：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: <span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame, Series</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: obj = Series(<span class="built_in">range</span>(<span class="number">4</span>), index=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: obj</span><br><span class="line">Out[<span class="number">38</span>]:</span><br><span class="line">d    <span class="number">0</span></span><br><span class="line">a    <span class="number">1</span></span><br><span class="line">b    <span class="number">2</span></span><br><span class="line">c    <span class="number">3</span></span><br><span class="line">dtype: int64</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: obj.sort_index()</span><br><span class="line">Out[<span class="number">39</span>]:</span><br><span class="line">a    <span class="number">1</span></span><br><span class="line">b    <span class="number">2</span></span><br><span class="line">c    <span class="number">3</span></span><br><span class="line">d    <span class="number">0</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure><!-- pdf p150 --><h3 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h3><p>pandas 使用浮点值 NaN 表示浮点和非浮点数组中的缺失数据，可以通过<code>np.nan</code>或者<code>None</code>得到。</p><h4 id="滤除缺失数据"><a href="#滤除缺失数据" class="headerlink" title="滤除缺失数据"></a>滤除缺失数据</h4><p>对于一个 Series，<code>dropna</code>函数返回一个仅含非空数据和索引值的 Series。</p><p>对于 DataFrame，<code>dropna</code>函数默认丢弃任何含有缺失值的行。传入<code>how=&#39;all&#39;</code>将只丢弃全为 NaN 的行。丢弃列则传入<code>axis=1</code>即可。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame, Series</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="keyword">from</span> numpy <span class="keyword">import</span> nan <span class="keyword">as</span> NA</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: data = Series([<span class="number">1</span>, NA, <span class="number">3.5</span>, NA, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: data.dropna()</span><br><span class="line">Out[<span class="number">4</span>]:</span><br><span class="line"><span class="number">0</span>    <span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">3.5</span></span><br><span class="line"><span class="number">4</span>    <span class="number">7.0</span></span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: data = DataFrame([[<span class="number">1.</span>, <span class="number">6.5</span>, <span class="number">3.</span>], [<span class="number">1.</span>, NA, NA], [NA, NA, NA], [NA, <span class="number">6.5</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: data</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">     <span class="number">0</span>    <span class="number">1</span>    <span class="number">2</span></span><br><span class="line"><span class="number">0</span>  <span class="number">1.0</span>  <span class="number">6.5</span>  <span class="number">3.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.0</span>  NaN  NaN</span><br><span class="line"><span class="number">2</span>  NaN  NaN  NaN</span><br><span class="line"><span class="number">3</span>  NaN  <span class="number">6.5</span>  <span class="number">3.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: data.dropna(axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">Empty DataFrame</span><br><span class="line">Columns: []</span><br><span class="line">Index: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: data.dropna()</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">     <span class="number">0</span>    <span class="number">1</span>    <span class="number">2</span></span><br><span class="line"><span class="number">0</span>  <span class="number">1.0</span>  <span class="number">6.5</span>  <span class="number">3.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: data.dropna(how=<span class="string">&#x27;all&#x27;</span>)</span><br><span class="line">Out[<span class="number">9</span>]:</span><br><span class="line">     <span class="number">0</span>    <span class="number">1</span>    <span class="number">2</span></span><br><span class="line"><span class="number">0</span>  <span class="number">1.0</span>  <span class="number">6.5</span>  <span class="number">3.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.0</span>  NaN  NaN</span><br><span class="line"><span class="number">3</span>  NaN  <span class="number">6.5</span>  <span class="number">3.0</span></span><br></pre></td></tr></table></figure><h4 id="填充缺失数据"><a href="#填充缺失数据" class="headerlink" title="填充缺失数据"></a>填充缺失数据</h4><p>通过一个常数调用<code>fillna</code>将缺失值替换为常数值。而通过一个字典调用<code>fillna</code>可以实现对不同的列填充不同的值。<code>fillna</code>默认返回新对象。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: df = DataFrame(np.random.randn(<span class="number">7</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: df.loc[:<span class="number">4</span>, <span class="number">1</span>] = np.nan</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: df.loc[:<span class="number">2</span>, <span class="number">2</span>] = np.nan</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: df</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">          <span class="number">0</span>         <span class="number">1</span>         <span class="number">2</span></span><br><span class="line"><span class="number">0</span> -<span class="number">0.048462</span>       NaN       NaN</span><br><span class="line"><span class="number">1</span> -<span class="number">1.114710</span>       NaN       NaN</span><br><span class="line"><span class="number">2</span>  <span class="number">1.290745</span>       NaN       NaN</span><br><span class="line"><span class="number">3</span> -<span class="number">0.129481</span>       NaN -<span class="number">2.151261</span></span><br><span class="line"><span class="number">4</span> -<span class="number">1.030350</span>       NaN -<span class="number">1.330047</span></span><br><span class="line"><span class="number">5</span> -<span class="number">2.431158</span> -<span class="number">0.340187</span> -<span class="number">0.246772</span></span><br><span class="line"><span class="number">6</span> -<span class="number">1.478923</span>  <span class="number">0.232547</span>  <span class="number">1.959353</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: df.fillna(<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">          <span class="number">0</span>         <span class="number">1</span>         <span class="number">2</span></span><br><span class="line"><span class="number">0</span> -<span class="number">0.048462</span>  <span class="number">0.000000</span>  <span class="number">0.000000</span></span><br><span class="line"><span class="number">1</span> -<span class="number">1.114710</span>  <span class="number">0.000000</span>  <span class="number">0.000000</span></span><br><span class="line"><span class="number">2</span>  <span class="number">1.290745</span>  <span class="number">0.000000</span>  <span class="number">0.000000</span></span><br><span class="line"><span class="number">3</span> -<span class="number">0.129481</span>  <span class="number">0.000000</span> -<span class="number">2.151261</span></span><br><span class="line"><span class="number">4</span> -<span class="number">1.030350</span>  <span class="number">0.000000</span> -<span class="number">1.330047</span></span><br><span class="line"><span class="number">5</span> -<span class="number">2.431158</span> -<span class="number">0.340187</span> -<span class="number">0.246772</span></span><br><span class="line"><span class="number">6</span> -<span class="number">1.478923</span>  <span class="number">0.232547</span>  <span class="number">1.959353</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: df.fillna(&#123;<span class="number">1</span>: <span class="number">0.5</span>, <span class="number">3</span>: -<span class="number">1</span>&#125;)</span><br><span class="line">Out[<span class="number">9</span>]:</span><br><span class="line">          <span class="number">0</span>         <span class="number">1</span>         <span class="number">2</span></span><br><span class="line"><span class="number">0</span> -<span class="number">0.048462</span>  <span class="number">0.500000</span>       NaN</span><br><span class="line"><span class="number">1</span> -<span class="number">1.114710</span>  <span class="number">0.500000</span>       NaN</span><br><span class="line"><span class="number">2</span>  <span class="number">1.290745</span>  <span class="number">0.500000</span>       NaN</span><br><span class="line"><span class="number">3</span> -<span class="number">0.129481</span>  <span class="number">0.500000</span> -<span class="number">2.151261</span></span><br><span class="line"><span class="number">4</span> -<span class="number">1.030350</span>  <span class="number">0.500000</span> -<span class="number">1.330047</span></span><br><span class="line"><span class="number">5</span> -<span class="number">2.431158</span> -<span class="number">0.340187</span> -<span class="number">0.246772</span></span><br><span class="line"><span class="number">6</span> -<span class="number">1.478923</span>  <span class="number">0.232547</span>  <span class="number">1.959353</span></span><br></pre></td></tr></table></figure><h2 id="数据加载、存储与文件格式"><a href="#数据加载、存储与文件格式" class="headerlink" title="数据加载、存储与文件格式"></a>数据加载、存储与文件格式</h2><p>NumPy 提供了一个低级但异常高效的二进制数据加载和存储机制，包括对内存映射数组的支持等。</p><p>本章着重介绍 pandas 的输入输出对象。输入输出通常划分为几个大类：</p><ul><li>读取文本文件和其他更高效的磁盘存储格式；</li><li>加载数据库中的数据；</li><li>利用 Web API 操作网络资源。</li></ul><h3 id="读写文本格式的数据"><a href="#读写文本格式的数据" class="headerlink" title="读写文本格式的数据"></a>读写文本格式的数据</h3><p>pandas 提供了一些用于将表格型数据读取为 DataFrame 对象的函数，其中<code>read_csv</code>和<code>read_table</code>可能使用较多。</p><table><thead><tr><th align="left">函数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">read_csv</td><td align="left">从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为逗号</td></tr><tr><td align="left">read_table</td><td align="left">从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为制表符（<code>\t</code>)，<strong>可以用</strong><code>sep</code><strong>参数指定分隔符</strong></td></tr><tr><td align="left">read_fwf</td><td align="left">读取定宽列格式数据（即没有分隔符）</td></tr><tr><td align="left">read_clipboard</td><td align="left">读取剪贴板中的数据，可以看作是 read_table 的剪贴板版。在将网页转换为表格时很有用</td></tr></tbody></table><p>在将文本数据转换为 DataFrame 时，用到了如下技术：</p><ul><li>索引：将一个或多个列当作返回的 DataFrame 处理，以及是否从文件、用户获取列名。</li><li><strong>类型推断</strong>和数据转换：包括用户定义值的转换、缺失值标记列表等。</li><li>日期解析：包括组合功能，比如将分散在多个列的日期时间信息组合成结果中的单个列。</li><li>迭代：支持对大文件进行逐块迭代。</li><li>不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西。</li></ul><p>读入一个以逗号分隔的 csv 文本文件：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">5</span>]: data = pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: data</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">   team  score</span><br><span class="line"><span class="number">0</span>     <span class="number">1</span>   <span class="number">45.6</span></span><br><span class="line"><span class="number">1</span>     <span class="number">2</span>   <span class="number">65.4</span></span><br><span class="line"><span class="number">2</span>     <span class="number">3</span>   <span class="number">67.7</span></span><br><span class="line"><span class="number">3</span>     <span class="number">4</span>   <span class="number">76.7</span></span><br><span class="line"><span class="number">4</span>     <span class="number">5</span>   <span class="number">56.3</span></span><br><span class="line"><span class="number">5</span>     <span class="number">6</span>   <span class="number">87.5</span></span><br><span class="line"><span class="number">6</span>     <span class="number">7</span>   <span class="number">45.6</span></span><br><span class="line"><span class="number">7</span>     <span class="number">8</span>   <span class="number">77.0</span></span><br><span class="line"><span class="number">8</span>     <span class="number">9</span>   <span class="number">54.6</span></span><br></pre></td></tr></table></figure><p>对于没有标题行的文件，可以让 pandas 为其分配默认的列名：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">Out[<span class="number">9</span>]:</span><br><span class="line">   <span class="number">0</span>     <span class="number">1</span></span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">45.6</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">65.4</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">67.7</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">76.7</span></span><br><span class="line"><span class="number">4</span>  <span class="number">5</span>  <span class="number">56.3</span></span><br><span class="line"><span class="number">5</span>  <span class="number">6</span>  <span class="number">87.5</span></span><br><span class="line"><span class="number">6</span>  <span class="number">7</span>  <span class="number">45.6</span></span><br><span class="line"><span class="number">7</span>  <span class="number">8</span>  <span class="number">77.0</span></span><br><span class="line"><span class="number">8</span>  <span class="number">9</span>  <span class="number">54.6</span></span><br></pre></td></tr></table></figure><p>也可以自定义列名：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>, names=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>])</span><br><span class="line">Out[<span class="number">10</span>]:</span><br><span class="line">   a     b</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">45.6</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">65.4</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">67.7</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">76.7</span></span><br><span class="line"><span class="number">4</span>  <span class="number">5</span>  <span class="number">56.3</span></span><br><span class="line"><span class="number">5</span>  <span class="number">6</span>  <span class="number">87.5</span></span><br><span class="line"><span class="number">6</span>  <span class="number">7</span>  <span class="number">45.6</span></span><br><span class="line"><span class="number">7</span>  <span class="number">8</span>  <span class="number">77.0</span></span><br><span class="line"><span class="number">8</span>  <span class="number">9</span>  <span class="number">54.6</span></span><br></pre></td></tr></table></figure><p>可以通过<code>index_col</code>参数来将某列指定为 DataFrame 的索引：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>, names=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], index_col=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">      a</span><br><span class="line">b</span><br><span class="line"><span class="number">45.6</span>  <span class="number">1</span></span><br><span class="line"><span class="number">65.4</span>  <span class="number">2</span></span><br><span class="line"><span class="number">67.7</span>  <span class="number">3</span></span><br><span class="line"><span class="number">76.7</span>  <span class="number">4</span></span><br><span class="line"><span class="number">56.3</span>  <span class="number">5</span></span><br><span class="line"><span class="number">87.5</span>  <span class="number">6</span></span><br><span class="line"><span class="number">45.6</span>  <span class="number">7</span></span><br><span class="line"><span class="number">77.0</span>  <span class="number">8</span></span><br><span class="line"><span class="number">54.6</span>  <span class="number">9</span></span><br></pre></td></tr></table></figure><p>如果希望将多个列做成一个<strong>层次化索引</strong>，只需通过<code>index_col</code>参数传入由列编号或列名组成的列表即可。</p><p>对于用<strong>不固定的分隔符</strong>分隔字段的文本，可以通过<code>sep</code>参数传入一个正则表达式来作为<code>read_table</code>的分隔符。</p><p>可以通过<code>skiprows</code>跳过文件的某几行：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>, names=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], skiprows=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">   a     b</span><br><span class="line"><span class="number">0</span>  <span class="number">2</span>  <span class="number">65.4</span></span><br><span class="line"><span class="number">1</span>  <span class="number">5</span>  <span class="number">56.3</span></span><br><span class="line"><span class="number">2</span>  <span class="number">6</span>  <span class="number">87.5</span></span><br><span class="line"><span class="number">3</span>  <span class="number">7</span>  <span class="number">45.6</span></span><br><span class="line"><span class="number">4</span>  <span class="number">8</span>  <span class="number">77.0</span></span><br><span class="line"><span class="number">5</span>  <span class="number">9</span>  <span class="number">54.6</span></span><br></pre></td></tr></table></figure><p>更多的<code>read_csv</code>/<code>read_table</code>函数的参数选项见原书 p167（pdf p178）。</p><h4 id="逐块读取文本文件"><a href="#逐块读取文本文件" class="headerlink" title="逐块读取文本文件"></a>逐块读取文本文件</h4><p>在处理很大的文件时，或找出大文件中的参数集以便后续处理时，可以<strong>只读取几行</strong>（避免读取整个文件），通过<code>nrows</code>进行指定即可：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>, names=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], nrows=<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">13</span>]:</span><br><span class="line">   a     b</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">45.6</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">65.4</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">67.7</span></span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">76.7</span></span><br></pre></td></tr></table></figure><p>也可以通过设置<code>chunksize</code>（行数）来<strong>逐块读取</strong>文件：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: chunker = pd.read_csv(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>, names=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], chunksize=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: chunker</span><br><span class="line">Out[<span class="number">23</span>]: &lt;pandas.io.parsers.TextFileReader at <span class="number">0x1134a7470</span>&gt;</span><br></pre></td></tr></table></figure><p><code>read_csv</code>所返回的这个 TextParser 对象使你可以根据 chunksize 对文件进行逐块迭代。比如将值计数聚合到 “a” 列中：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">18</span>]: tot = Series([])</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: <span class="keyword">for</span> piece <span class="keyword">in</span> chunker:</span><br><span class="line">    ...:     tot = tot.add(piece[<span class="string">&#x27;a&#x27;</span>].value_counts(), fill_value=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">In [<span class="number">20</span>]: tot = tot.order(ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>于是有：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">22</span>]: tot[:<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">22</span>]:</span><br><span class="line"><span class="number">1</span>    <span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">1.0</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><p>TextParser 还有一个<code>get_chunk</code>方法，可以读取任意大小的块。</p><h4 id="将数据写出到文本格式"><a href="#将数据写出到文本格式" class="headerlink" title="将数据写出到文本格式"></a>将数据写出到文本格式</h4><p>利用 DataFrame（或 Series）的<code>to_csv</code>方法，可以将数据写到一个以逗号分隔的文件中。也可以通过<code>sep</code>参数指定使用其他分隔符，或者输出到<code>sys.stdout</code>来打印文本结果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.to_csv(sys.stdout, sep=<span class="string">&#x27;|&#x27;</span>)</span><br></pre></td></tr></table></figure><p>缺失值在输出结果中会被表示为空字符串，可以通过<code>na_rep</code>参数指定其为别的标记值。</p><p>如果没有设置其他选项，则会写出行和列的标签，也可以将它们禁用：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.to_csv(sys.stdout, index=<span class="literal">False</span>, header=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>此外，也可以只按指定的顺序写出一部分的列：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.to_csv(sys.stdout, index=<span class="literal">False</span>, cols=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br></pre></td></tr></table></figure><h4 id="手工处理分隔符格式"><a href="#手工处理分隔符格式" class="headerlink" title="手工处理分隔符格式"></a>手工处理分隔符格式</h4><p>对于含有畸形行的文件需要做一些额外处理。对于任何<strong>单字符分隔符文件</strong>，可以直接使用 python 内置的 csv 模块，将人意已打开的文件或文件型对象传给<code>csv.reader</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">26</span>]: <span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: f = <span class="built_in">open</span>(<span class="string">&#x27;/Users/huang/Desktop/score.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: reader = csv.reader(f)</span><br></pre></td></tr></table></figure><p>对这个 reader 进行迭代将会为每行产生一个列表（并移除了所有的引号）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">29</span>]: <span class="keyword">for</span> line <span class="keyword">in</span> reader:</span><br><span class="line">    ...:     <span class="built_in">print</span>(line)</span><br><span class="line">    ...:</span><br><span class="line">[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;45.6&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;65.4&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;67.7&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;76.7&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;56.3&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;87.5&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;45.6&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;8&#x27;</span>, <span class="string">&#x27;77&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;9&#x27;</span>, <span class="string">&#x27;54.6&#x27;</span>]</span><br></pre></td></tr></table></figure><h4 id="JSON-数据"><a href="#JSON-数据" class="headerlink" title="JSON 数据"></a>JSON 数据</h4><p>json 库是构建于 Python 标准库的。通过<code>json.loads</code>即可将 JSON 字符串转换为 Python 形式，而<code>json.dumps</code>则将 Python 对象转换为 JSON 格式（对象中所有的键都必须是字符串）。</p><h4 id="XML-和-HTML：Web-信息收集"><a href="#XML-和-HTML：Web-信息收集" class="headerlink" title="XML 和 HTML：Web 信息收集"></a>XML 和 HTML：Web 信息收集</h4><p>相关内容参见原书 p174（psf p185）。</p><h3 id="二进制数据格式"><a href="#二进制数据格式" class="headerlink" title="二进制数据格式"></a>二进制数据格式</h3><p>二进制格式的 I/O 高速且高效，但不宜人来阅读。相关内容参见原书 p179（psf p190）。</p><h3 id="使用-HTML-和-Web-API"><a href="#使用-HTML-和-Web-API" class="headerlink" title="使用 HTML 和 Web API"></a>使用 HTML 和 Web API</h3><p>通过 requests 包可以方位公共 API。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: <span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: url = <span class="string">&#x27;https://www.easy-mock.com/mock/5a1fb5a85bc39d4cf3b7a778/robot/ask&#x27;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: resp = requests.get(url)</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: resp</span><br><span class="line">Out[<span class="number">39</span>]: &lt;Response [<span class="number">200</span>]&gt;</span><br></pre></td></tr></table></figure><p>Response 对象的 text 属性含有 GET 请求的内容。许多 Web API 返回的都是 JSON 字符串，必须将其加载到一个 Python 对象中：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">40</span>]: data = json.loads(resp.text)</span><br><span class="line"></span><br><span class="line">In [<span class="number">41</span>]: data.keys()</span><br><span class="line">Out[<span class="number">41</span>]: dict_keys([<span class="string">&#x27;answer&#x27;</span>, <span class="string">&#x27;id&#x27;</span>])</span><br></pre></td></tr></table></figure><h3 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h3><p>对于基于 SQL 的关系型数据库，以 SQLite 为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import sqlite3</span><br><span class="line">con = sqlite3.connect(&#x27;:memory:&#x27;)</span><br><span class="line">con.execute(query)</span><br><span class="line">con.commit()</span><br></pre></td></tr></table></figure><p>pandas 有一个可以简化数据规整操作过程<code>read_frame</code>函数（位于 pandas.io.sql 模块），只需要传入 select 语句和连接对象即可：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  pandas.io.sql <span class="keyword">as</span> sql</span><br><span class="line"></span><br><span class="line">sql.read_frame(<span class="string">&#x27;select * from test&#x27;</span>, con)</span><br></pre></td></tr></table></figure><h4 id="存取-MongoDB-中的数据"><a href="#存取-MongoDB-中的数据" class="headerlink" title="存取 MongoDB 中的数据"></a>存取 MongoDB 中的数据</h4><p>先在电脑上启动一个 MongoDB 实例，然后用 pymongo（MongoDB 的官方驱动器）通过默认端口进行连接：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">con = pymongo.Connection(<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">27017</span>)</span><br></pre></td></tr></table></figure><p>存储在 MongoDB 中的文档被组织在数据库的集合（collection）中。MongoDB 服务器的每个运行实例可以有多个数据库，而每个数据库又可以有多个集合。想保存之前通过 APi 获取的数据，首先访问响应集合：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">answers = con.db.answers</span><br></pre></td></tr></table></figure><p>然后可以通过<code>answers.save</code>将 Python 字典逐个存入 MongoDb 的相应集合中。</p><p>可以通过下列代码对集合进行查询：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cursor = answers.find(&#123;<span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;提问已收到，正在寻找回答...&#x27;</span>, <span class="string">&#x27;id&#x27;</span>: <span class="number">1111</span>&#125;)</span><br></pre></td></tr></table></figure><p>返回的游标是一个迭代器，可以为每个文档产生一个字典。可以将其转换为一个 DataFrame，并只获取部分字段：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = DataFrame(<span class="built_in">list</span>(cursor), columns=<span class="string">&#x27;answer&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="数据规整化：清理、转换、合并、重塑"><a href="#数据规整化：清理、转换、合并、重塑" class="headerlink" title="数据规整化：清理、转换、合并、重塑"></a>数据规整化：清理、转换、合并、重塑</h2><p>pandas 和 Python 标准库提供了一组高级的、灵活的、高效的核心函数和算法，以轻松地将数据规整化为正确的形式。</p><h3 id="合并数据集"><a href="#合并数据集" class="headerlink" title="合并数据集"></a>合并数据集</h3><ul><li><code>pandas.merge</code>可根据一个或多个键将不同 DataFrame 中的行连接起来（相当于 SQL 中的连接操作）。</li><li><code>pandas.concat</code>可以沿一条轴将多个对象堆叠到一起。</li><li>实例方法<code>combine_first</code>可以将重复数据编连在一起，用一个对象中的值填充另一个对象中的缺失值。</li></ul><h4 id="数据库风格的-DataFrame-合并"><a href="#数据库风格的-DataFrame-合并" class="headerlink" title="数据库风格的 DataFrame 合并"></a>数据库风格的 DataFrame 合并</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> pandas <span class="keyword">import</span> Series, DataFrame</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: df1 = DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], <span class="string">&#x27;data1&#x27;</span>: <span class="built_in">range</span>(<span class="number">7</span>)&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: df2 = DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;d&#x27;</span>], <span class="string">&#x27;data2&#x27;</span>: <span class="built_in">range</span>(<span class="number">3</span>)&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: df1</span><br><span class="line">Out[<span class="number">4</span>]:</span><br><span class="line">   data1 key</span><br><span class="line"><span class="number">0</span>      <span class="number">0</span>   b</span><br><span class="line"><span class="number">1</span>      <span class="number">1</span>   b</span><br><span class="line"><span class="number">2</span>      <span class="number">2</span>   a</span><br><span class="line"><span class="number">3</span>      <span class="number">3</span>   c</span><br><span class="line"><span class="number">4</span>      <span class="number">4</span>   a</span><br><span class="line"><span class="number">5</span>      <span class="number">5</span>   a</span><br><span class="line"><span class="number">6</span>      <span class="number">6</span>   b</span><br></pre></td></tr></table></figure><p>如果没有指定用哪个列进行连接，<code>merge</code>就会将重叠列的列名当作键。不过最好用<code>on</code>选项显式指定（如果两个对象的列名不同，也可以用<code>left_on</code>和<code>right_on</code>分别进行指定）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: pd.merge(df1, df2, on=<span class="string">&#x27;key&#x27;</span>)</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">   data1 key  data2</span><br><span class="line"><span class="number">0</span>      <span class="number">0</span>   b      <span class="number">1</span></span><br><span class="line"><span class="number">1</span>      <span class="number">1</span>   b      <span class="number">1</span></span><br><span class="line"><span class="number">2</span>      <span class="number">6</span>   b      <span class="number">1</span></span><br><span class="line"><span class="number">3</span>      <span class="number">2</span>   a      <span class="number">0</span></span><br><span class="line"><span class="number">4</span>      <span class="number">4</span>   a      <span class="number">0</span></span><br><span class="line"><span class="number">5</span>      <span class="number">5</span>   a      <span class="number">0</span></span><br></pre></td></tr></table></figure><p>默认情况下，<code>merge</code>做的是**”inner”<strong>连接，结果中的键是</strong>交集<strong>。其他方式还有”left”、”right”以及”outer”。</strong>外连接求取的是键的并集**，组合了左连接和右连接的效果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: pd.merge(df1, df2, how=<span class="string">&#x27;outer&#x27;</span>)</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">   data1 key  data2</span><br><span class="line"><span class="number">0</span>    <span class="number">0.0</span>   b    <span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">1.0</span>   b    <span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">6.0</span>   b    <span class="number">1.0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">2.0</span>   a    <span class="number">0.0</span></span><br><span class="line"><span class="number">4</span>    <span class="number">4.0</span>   a    <span class="number">0.0</span></span><br><span class="line"><span class="number">5</span>    <span class="number">5.0</span>   a    <span class="number">0.0</span></span><br><span class="line"><span class="number">6</span>    <span class="number">3.0</span>   c    NaN</span><br><span class="line"><span class="number">7</span>    NaN   d    <span class="number">2.0</span></span><br></pre></td></tr></table></figure><p>以上是<strong>多对一</strong>的合并。多对多的合并操作非常简单，无需额外的操作：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: df1 = DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], <span class="string">&#x27;data1&#x27;</span>: <span class="built_in">range</span>(<span class="number">6</span>)&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: df2 = DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;d&#x27;</span>], <span class="string">&#x27;data1&#x27;</span>: <span class="built_in">range</span>(<span class="number">5</span>)&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: pd.merge(df1, df2, on=<span class="string">&#x27;key&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">    data1_x key  data1_y</span><br><span class="line"><span class="number">0</span>         <span class="number">0</span>   b      <span class="number">1.0</span></span><br><span class="line"><span class="number">1</span>         <span class="number">0</span>   b      <span class="number">3.0</span></span><br><span class="line"><span class="number">2</span>         <span class="number">1</span>   b      <span class="number">1.0</span></span><br><span class="line"><span class="number">3</span>         <span class="number">1</span>   b      <span class="number">3.0</span></span><br><span class="line"><span class="number">4</span>         <span class="number">2</span>   a      <span class="number">0.0</span></span><br><span class="line"><span class="number">5</span>         <span class="number">2</span>   a      <span class="number">2.0</span></span><br><span class="line"><span class="number">6</span>         <span class="number">3</span>   c      NaN</span><br><span class="line"><span class="number">7</span>         <span class="number">4</span>   a      <span class="number">0.0</span></span><br><span class="line"><span class="number">8</span>         <span class="number">4</span>   a      <span class="number">2.0</span></span><br><span class="line"><span class="number">9</span>         <span class="number">5</span>   b      <span class="number">1.0</span></span><br><span class="line"><span class="number">10</span>        <span class="number">5</span>   b      <span class="number">3.0</span></span><br></pre></td></tr></table></figure><p><strong>多对多连接产生的是行的笛卡尔积</strong>。由于左边的 DataFrame 有 3 个 “b” 行，右边有 2 个，所以最终结果中就有 6 个 “b” 行。连接方式只影响出现在结果中的键：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: pd.merge(df1, df2, on=<span class="string">&#x27;key&#x27;</span>, how=<span class="string">&#x27;inner&#x27;</span>)</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">   data1_x key  data1_y</span><br><span class="line"><span class="number">0</span>        <span class="number">0</span>   b        <span class="number">1</span></span><br><span class="line"><span class="number">1</span>        <span class="number">0</span>   b        <span class="number">3</span></span><br><span class="line"><span class="number">2</span>        <span class="number">1</span>   b        <span class="number">1</span></span><br><span class="line"><span class="number">3</span>        <span class="number">1</span>   b        <span class="number">3</span></span><br><span class="line"><span class="number">4</span>        <span class="number">5</span>   b        <span class="number">1</span></span><br><span class="line"><span class="number">5</span>        <span class="number">5</span>   b        <span class="number">3</span></span><br><span class="line"><span class="number">6</span>        <span class="number">2</span>   a        <span class="number">0</span></span><br><span class="line"><span class="number">7</span>        <span class="number">2</span>   a        <span class="number">2</span></span><br><span class="line"><span class="number">8</span>        <span class="number">4</span>   a        <span class="number">0</span></span><br><span class="line"><span class="number">9</span>        <span class="number">4</span>   a        <span class="number">2</span></span><br></pre></td></tr></table></figure><p>要根据多个键进行合并，传入一个由列名组成的列表即可：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: left = DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span>: [<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>],</span><br><span class="line">    ...:  <span class="string">&#x27;key2&#x27;</span>: [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;one&#x27;</span>], </span><br><span class="line">    ...:  <span class="string">&#x27;lval&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: right = DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span>: [<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>], </span><br><span class="line">    ...:  <span class="string">&#x27;key2&#x27;</span>: [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>], </span><br><span class="line">    ...:  <span class="string">&#x27;lval&#x27;</span>: [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: pd.merge(left, right, on=[<span class="string">&#x27;key1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span>], how=<span class="string">&#x27;outer&#x27;</span>)</span><br><span class="line">Out[<span class="number">19</span>]:</span><br><span class="line">  key1 key2  lval_x  lval_y</span><br><span class="line"><span class="number">0</span>  foo  one     <span class="number">1.0</span>     <span class="number">4.0</span></span><br><span class="line"><span class="number">1</span>  foo  one     <span class="number">1.0</span>     <span class="number">5.0</span></span><br><span class="line"><span class="number">2</span>  foo  two     <span class="number">2.0</span>     NaN</span><br><span class="line"><span class="number">3</span>  bar  one     <span class="number">3.0</span>     <span class="number">6.0</span></span><br><span class="line"><span class="number">4</span>  bar  two     NaN     <span class="number">7.0</span></span><br></pre></td></tr></table></figure><p><strong>在进行列-列连接时，DataFrame 对象中的索引会被丢弃。</strong></p><p><code>merge</code>的<code>suffixes</code>选项可用于指定<strong>附加到</strong>左右两个 DataFrame 对象的重叠列名上的字符串：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">20</span>]: pd.merge(left, right, on=<span class="string">&#x27;key1&#x27;</span>)</span><br><span class="line">Out[<span class="number">20</span>]:</span><br><span class="line">  key1 key2_x  lval_x key2_y  lval_y</span><br><span class="line"><span class="number">0</span>  foo    one       <span class="number">1</span>    one       <span class="number">4</span></span><br><span class="line"><span class="number">1</span>  foo    one       <span class="number">1</span>    one       <span class="number">5</span></span><br><span class="line"><span class="number">2</span>  foo    two       <span class="number">2</span>    one       <span class="number">4</span></span><br><span class="line"><span class="number">3</span>  foo    two       <span class="number">2</span>    one       <span class="number">5</span></span><br><span class="line"><span class="number">4</span>  bar    one       <span class="number">3</span>    one       <span class="number">6</span></span><br><span class="line"><span class="number">5</span>  bar    one       <span class="number">3</span>    two       <span class="number">7</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: pd.merge(left, right, on=<span class="string">&#x27;key1&#x27;</span>, suffixes=(<span class="string">&#x27;_left&#x27;</span>, <span class="string">&#x27;_right&#x27;</span>))</span><br><span class="line">Out[<span class="number">21</span>]:</span><br><span class="line">  key1 key2_left  lval_left key2_right  lval_right</span><br><span class="line"><span class="number">0</span>  foo       one          <span class="number">1</span>        one           <span class="number">4</span></span><br><span class="line"><span class="number">1</span>  foo       one          <span class="number">1</span>        one           <span class="number">5</span></span><br><span class="line"><span class="number">2</span>  foo       two          <span class="number">2</span>        one           <span class="number">4</span></span><br><span class="line"><span class="number">3</span>  foo       two          <span class="number">2</span>        one           <span class="number">5</span></span><br><span class="line"><span class="number">4</span>  bar       one          <span class="number">3</span>        one           <span class="number">6</span></span><br><span class="line"><span class="number">5</span>  bar       one          <span class="number">3</span>        two           <span class="number">7</span></span><br></pre></td></tr></table></figure><p><code>merge</code>的<code>copy</code>选项默认为 True，以根据连接键对合并后的数据进行排序。有时在处理大数据集时，禁用该选项可获得更好的性能。</p><p>在 DataFrame 中的<strong>连接键位于其索引中</strong>时，可以传入<code>left_index=True</code>或<code>right_index=True</code>以说明：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: left1 = DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>], <span class="string">&#x27;value&#x27;</span>: <span class="built_in">range</span>(<span class="number">6</span>)&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: right1 = DataFrame(&#123;<span class="string">&#x27;group_val&#x27;</span>: [<span class="number">3.5</span>, <span class="number">7</span>]&#125;, index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: right1</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">   group_val</span><br><span class="line">a        <span class="number">3.5</span></span><br><span class="line">b        <span class="number">7.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: pd.merge(left1, right1, left_on=<span class="string">&#x27;key&#x27;</span>, right_index=<span class="literal">True</span>)</span><br><span class="line">Out[<span class="number">7</span>]:</span><br><span class="line">  key  value  group_val</span><br><span class="line"><span class="number">0</span>   a      <span class="number">0</span>        <span class="number">3.5</span></span><br><span class="line"><span class="number">2</span>   a      <span class="number">2</span>        <span class="number">3.5</span></span><br><span class="line"><span class="number">3</span>   a      <span class="number">3</span>        <span class="number">3.5</span></span><br><span class="line"><span class="number">1</span>   b      <span class="number">1</span>        <span class="number">7.0</span></span><br><span class="line"><span class="number">4</span>   b      <span class="number">4</span>        <span class="number">7.0</span></span><br></pre></td></tr></table></figure><h4 id="轴向连接"><a href="#轴向连接" class="headerlink" title="轴向连接"></a>轴向连接</h4><p>另一种数据合并预算也被称作连接（concatenation）、绑定（binding）或堆叠（stacking）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: s1 = Series([<span class="number">0</span>, <span class="number">1</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: s2 = Series([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], index=[<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: s3 = Series([<span class="number">5</span>, <span class="number">6</span>], index=[<span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: pd.concat([s1, s2, s3])</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">a    <span class="number">0</span></span><br><span class="line">b    <span class="number">1</span></span><br><span class="line">c    <span class="number">2</span></span><br><span class="line">d    <span class="number">3</span></span><br><span class="line">e    <span class="number">4</span></span><br><span class="line">f    <span class="number">5</span></span><br><span class="line">g    <span class="number">6</span></span><br><span class="line">dtype: int64</span><br></pre></td></tr></table></figure><p>默认情况下，<code>concat</code>在<code>axis=0</code>上工作，最终产生一个新的 Series（<strong>纵向叠加</strong>）。如果传入<code>axis=1</code>（列），则结果就会变成一个 DataFrame（<strong>横向叠加</strong>）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: pd.concat([s1, s2, s3], axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">     <span class="number">0</span>    <span class="number">1</span>    <span class="number">2</span></span><br><span class="line">a  <span class="number">0.0</span>  NaN  NaN</span><br><span class="line">b  <span class="number">1.0</span>  NaN  NaN</span><br><span class="line">c  NaN  <span class="number">2.0</span>  NaN</span><br><span class="line">d  NaN  <span class="number">3.0</span>  NaN</span><br><span class="line">e  NaN  <span class="number">4.0</span>  NaN</span><br><span class="line">f  NaN  NaN  <span class="number">5.0</span></span><br><span class="line">g  NaN  NaN  <span class="number">6.0</span></span><br></pre></td></tr></table></figure><!--p207：剩下的内容基本就属于 Cookbook 了，暂时不打算看了--><h2 id="绘图与可视化"><a href="#绘图与可视化" class="headerlink" title="绘图与可视化"></a>绘图与可视化</h2><p>Python 有许多可视化工具，例如 Chaco、mayavi 等。而作为 Python 领域中使用最广泛的绘图工具，matplotlib 是一个用于创建出版质量图表的桌面绘图包。</p><p>有多种方法使用 matplotlib，最常用的方式是 Pylab 模式的 IPython（<code>ipython --pylab</code>），这样会将 IPython 配置为使用用户所指定（一般为默认）的 matplotlib GUI 后端。</p><p>可以绘制一张简单的图标来测试一切是否准备就绪：<code>plot(np.arange(10))</code>。如果没有问题就会弹出新窗口，可以用鼠标或者输入<code>close()</code>将其关闭。</p><p>matplotlib API 函数都位于 matplotlib.pyplot 模块中，通常的引入约定是：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>pandas 的绘图函数能够处理许多普通的绘图任务（书上介绍的相关函数可能比较老旧，需要学习官方文档），但如果需要自定义一些高级功能的话就必须学习 matplotlib API。推荐查阅 matplotlib 的示例库和文档进行学习。</p><h3 id="Figure-和-Subplot"><a href="#Figure-和-Subplot" class="headerlink" title="Figure 和 Subplot"></a>Figure 和 Subplot</h3><p><strong>matplotlib 的图像都位于 Figure 对象中</strong>。可以用<code>plt.figure()</code>创建一个新的 Figure：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: fig = plt.figure()</span><br></pre></td></tr></table></figure><p>会弹出一个新窗口，但是不能通过空 Figure 绘图，必须用<code>add_subplot</code>创建一个或多个 subplot：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: ax1 = fig.add_subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>上面这一行代码的意思是，图像有 2 * 2 个 subplot，且当前选中的是 4 个 subplot 中的第一个（编号从 1 开始）。可以更改第三个参数来创建剩余的 subplot。</p><p>如果直接发出绘图命令（<code>plt.plot()</code>），matplotlib 会在最后一个用过的 subplot（如果没有则创建一个）上进行绘制。</p><p>由<code>fig.add_subplot</code>所返回的对象是 AxesSubplot 对象，直接调用它们的实例方法就可以在其他空着的 subplot 里画图：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">33</span>]: ax1.hist(randn(<span class="number">100</span>), bins=<span class="number">20</span>, color=<span class="string">&#x27;k&#x27;</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: ax3.scatter(np.arange(<span class="number">30</span>), np.arange(<span class="number">30</span>) + <span class="number">3</span> * randn(<span class="number">30</span>))</span><br></pre></td></tr></table></figure><p>由于根据特定布局创建 Figure 和 subplot 是一件非常常见的任务，因此可以用更方便的<code>plt.subplots</code>创建一个 Figure，并返回一个含有已创建的 subplot 对象的 NumPy 数组：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: axes</span><br></pre></td></tr></table></figure><p>之后可以通过和二维数组一样的索引方式来指定 subplot，例如<code>axes[0, 1]</code>。</p><p>关于 pyplot.subplots 的选项以及调整 subplot 周围的间距的更多内容参见原书 p235（pdf p246）。</p><h3 id="颜色、标记和线型"><a href="#颜色、标记和线型" class="headerlink" title="颜色、标记和线型"></a>颜色、标记和线型</h3><p>matplot 的 plot 函数接受一组 X 和 Y 坐标，还可以接受一个表示颜色和线型的字符串缩写。例如根据 x 和 y 绘制绿色虚线：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax.plot(x, y, <span class="string">&#x27;g--&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这种写法会更明确：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax.plot(x, y, linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br></pre></td></tr></table></figure><p>常见的颜色都有一个缩写词，也可以通过指定 RGB 值的形式。</p><p>可以通过指定<code>marker=&#39;o&#39;</code>来给实际数据点加上标记（marker）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(randn(<span class="number">30</span>).cumsum(), color=<span class="string">&#x27;k&#x27;</span>, linestyle=<span class="string">&#x27;dashed&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure><p>标记也可以放到格式字符串中，但标记类型和线型必须放在颜色后面：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(randn(<span class="number">30</span>).cumsum(), <span class="string">&#x27;ko--&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在线型图中，非实际数据点默认是按线性方式插值的。可以通过<code>drawstyle</code>选项修改：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">47</span>]: fig.add_subplot(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).plot(data, <span class="string">&#x27;k-&#x27;</span>, label=<span class="string">&#x27;Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: plt.plot(data, <span class="string">&#x27;k-&#x27;</span>, drawstyle=<span class="string">&#x27;steps-post&#x27;</span>, label=<span class="string">&#x27;steps-post&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">49</span>]: plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="刻度、标签和图例"><a href="#刻度、标签和图例" class="headerlink" title="刻度、标签和图例"></a>刻度、标签和图例</h3><p>对于大多数的图表装饰项，其主要实现方式有二：</p><ul><li>使用过程型的 pyplot 接口；</li><li>使用更为面向对象的原生 matplotlib API。</li></ul><h4 id="设置标题、轴标签、刻度以及刻度标签"><a href="#设置标题、轴标签、刻度以及刻度标签" class="headerlink" title="设置标题、轴标签、刻度以及刻度标签"></a>设置标题、轴标签、刻度以及刻度标签</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">55</span>]: fig = plt.figure()</span><br><span class="line"></span><br><span class="line">In [<span class="number">56</span>]: ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: ax.plot(randn(<span class="number">1000</span>).cumsum())</span><br></pre></td></tr></table></figure><p>修改 X 轴的刻度时，<code>set_xticks</code>可以确定将刻度放在数据范围中的哪些位置：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">58</span>]: ticks = ax.set_xticks([<span class="number">0</span>, <span class="number">250</span>, <span class="number">500</span>, <span class="number">750</span>, <span class="number">1000</span>])</span><br></pre></td></tr></table></figure><p>默认情况下，这些位置也就是刻度标签，但可以通过<code>set_xticklabels</code>将任何其他的值用作标签：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">59</span>]: labels = ax.set_xticklabels([<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>, <span class="string">&#x27;four&#x27;</span>, <span class="string">&#x27;five&#x27;</span>], rotation=<span class="number">30</span>, fontsize=<span class="string">&#x27;small&#x27;</span>)</span><br></pre></td></tr></table></figure><p>再用<code>set_xlabel</code>为 X 轴设置一个名称，并用<code>set_title</code>设置一个标题：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">61</span>]: ax.set_title(<span class="string">&#x27;My first matplotlib plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: ax.set_xlabels(<span class="string">&#x27;Stages&#x27;</span>)</span><br></pre></td></tr></table></figure><p>结果如图：</p><p><img src="https://user-images.githubusercontent.com/18595460/34092191-3794f9a6-e3fd-11e7-824b-7da5a960fb03.png" alt="figure_1"></p><h4 id="添加图例"><a href="#添加图例" class="headerlink" title="添加图例"></a>添加图例</h4><p>图例（legend）是另一种用于标识图表元素的重要工具。添加图例最简单的是在添加 subplot 的时候传入 label 参数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">67</span>]: ax.plot(randn(<span class="number">1000</span>).cumsum(), <span class="string">&#x27;k&#x27;</span>, label=<span class="string">&#x27;one&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: ax.plot(randn(<span class="number">1000</span>).cumsum(), <span class="string">&#x27;k--&#x27;</span>, label=<span class="string">&#x27;two&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">69</span>]: ax.plot(randn(<span class="number">1000</span>).cumsum(), <span class="string">&#x27;k.&#x27;</span>, label=<span class="string">&#x27;three&#x27;</span>)</span><br></pre></td></tr></table></figure><p>如果有不想添加图例的元素，不传入<code>label</code>或传入<code>label=&#39;_nolegend_&#39;</code>即可。</p><p>之后，可以调用<code>ax.legend()</code>和<code>plt.legend()</code>来自动创建图例：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="注解以及在-Subplot-上绘图"><a href="#注解以及在-Subplot-上绘图" class="headerlink" title="注解以及在 Subplot 上绘图"></a>注解以及在 Subplot 上绘图</h4><p>可以绘制一些自定义的注解（例如文本、箭头或其他图形等）。</p><p>文本：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">71</span>]: ax.text(<span class="number">10</span>, <span class="number">10</span>, <span class="string">&#x27;Hello world!&#x27;</span>, family=<span class="string">&#x27;monospace&#x27;</span>, fontsize=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>其他参见官方文档。</p><h3 id="将图表保存到文件"><a href="#将图表保存到文件" class="headerlink" title="将图表保存到文件"></a>将图表保存到文件</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">72</span>]: plt.savefig(<span class="string">&#x27;./Desktop/figpath.svg&#x27;</span>)</span><br></pre></td></tr></table></figure><p>文件类型通过文件扩展名推断。</p><p>可以通过<code>dpi</code>控制每英寸点数分辨率，以及通过<code>bbox_inches</code>剪除当前图表周围的空白部分：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: plt.savefig(<span class="string">&#x27;./Desktop/figpath.png&#x27;</span>, dpi=<span class="number">400</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br></pre></td></tr></table></figure><p>更多 Figure.savefig 的选项见原书 p244（pdf p255）。</p><h3 id="matplotlib-配置"><a href="#matplotlib-配置" class="headerlink" title="matplotlib 配置"></a>matplotlib 配置</h3><p>几乎所有默认行为都能通过一组全局参数进行自定义。操作 matplotlib 配置系统的方式主要有两种，第一种是 Python 编程方式，即利用<code>rc</code>方法。</p><p><code>rc</code>的第一个参数是希望自定义的对象。最简单的办法是将这些选项写成一个字典：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">76</span>]: font_options = &#123;<span class="string">&#x27;family&#x27;</span>: <span class="string">&#x27;monospace&#x27;</span>,</span><br><span class="line">    ...:                 <span class="string">&#x27;weight&#x27;</span>: <span class="string">&#x27;bold&#x27;</span>,</span><br><span class="line">    ...:                 <span class="string">&#x27;size&#x27;</span>  : <span class="string">&#x27;small&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: plt.rc(<span class="string">&#x27;font&#x27;</span>, **font_options)</span><br></pre></td></tr></table></figure><p>要了解全部的自定义选项，请查阅 matplotlib 的配置文件 matplotlibrc（位于 matplotlib/mpl-data 目录）。如果对该文件进行了自定义，并将其放在你自己的 .matplotlibrc 目录中，则每次使用 matplotlib 时就会加载该文件。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;《利用Python进行数据分析》是一本非常不错的 Cookbook。早在去年年底的时候我就一边看一边记录一遍敲代码，阅读完比较重要的章节，整理了这篇笔记。有时候有些 API 忘记了，就翻看查找，比直接看 pdf 要方便。现在为了查找更方便，将这篇笔记迁移到博客上来。&lt;/p&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="读书笔记" scheme="https://kyonhuang.top/blog/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="数据科学" scheme="https://kyonhuang.top/blog/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/"/>
    
    <category term="pandas" scheme="https://kyonhuang.top/blog/tags/pandas/"/>
    
    <category term="NumPy" scheme="https://kyonhuang.top/blog/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】Deep Learning for Sentiment Analysis：A Survey</title>
    <link href="https://kyonhuang.top/blog/1801-07883-notes/"/>
    <id>https://kyonhuang.top/blog/1801-07883-notes/</id>
    <published>2018-08-09T01:32:21.000Z</published>
    <updated>2019-09-06T08:34:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文链接：<a href="https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf">Deep Learning for Sentiment Analysis: A Survey</a></p><p>作者：Lei Zhang, Shuai Wang, Bing Liu</p><p>简介：该文首先对深度学习的技术进行了概述，然后对基于深度学习的情感分析研究进行了全面的综述。</p><p>选读原因：之前想把手头情感分析的工作再进一步推动一下，因此选择在 arXiv 上找一篇相关综述进行阅读。同样，也是对自然语言处理中的一些概念和技术的了解和复习。这篇综述相对较新，质量很高，而且作者之一刘兵老师是情感分析领域最有影响力的学者之一。当然，综述只是帮助对某一研究领域建立一个初步的印象，要想真正开始深入还是得读海量的 paper + 选择复现。这篇综述中对相关工作建立表格详细介绍，因此原文仍值得一看。</p><h2 id="研究概况"><a href="#研究概况" class="headerlink" title="研究概况"></a>研究概况</h2><h3 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h3><p>情感分析（Sentiment analysis）或意见挖掘（Opinion mining）是对人群对于产品、服务、组织、个体、问题、事件、话题等实体的意见、情感、情绪、评价、态度的计算研究。鉴于网络社交媒体的风行，我们拥有了海量的情绪化数据。情感分析已经成为自然语言处理中最有吸引力的研究领域之一，它与管理科学、社会科学领域有交集。</p><p>早期的情感分析技术包括监督方法（支持向量机、最大熵、朴素贝叶斯等监督机器学习方法）和无监督方法（包括利用情感词典、语法分析和句法模式的各种方法），而深度学习走红后，其应用到情感分析的出色效果催生大量基于深度学习的情感分析研究。</p><span id="more"></span><h2 id="具体内容"><a href="#具体内容" class="headerlink" title="具体内容"></a>具体内容</h2><h3 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h3><p>这里简单概括一下文中对各种深度学习技术的介绍，同时也是对我知道的点做一个复习。DNN、CNN、RNN、Attention 就不介绍了，如果想了解欢迎看我的<a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/">吴恩达《深度学习》系列课程笔记</a>。</p><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>NLP 中的许多深度学习模型需要词嵌入作为输入特征。词嵌入将词转换为包含实数的向量，并做从高维稀疏向量空间（通常是 One-hot 编码得到的词向量）到低维密集向量空间的处理。嵌入向量的每个维度都代表一个单词的潜在特征，因此这些向量可以编码语言的规律和模式。</p><p>Word2Vec 和 Glove 是两种常用于获得词嵌入的方法。前者是一种神经网络预测模型，可以从文本中学习词嵌入；后者是在一个全局单词共现矩阵的非零项上进行训练。</p><h4 id="自动编码器"><a href="#自动编码器" class="headerlink" title="自动编码器"></a>自动编码器</h4><p><strong>自动编码器（Autoencoder）</strong>是一种主要用于数据降维或者特征抽取的的<strong>自监督</strong>学习算法。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/Autoencoder.png" alt="Autoencoder"></p><p>一个三层的神经网络，其中隐藏层被看作一个编码器和解码器，而输出层的目标值和输入层输入值一致。这样，隐含层是尽量保证输出数据等于输入数据的，使得隐含层能够抓住输入数据的特点，使其特征保持不变。设编码函数为<code>h()</code>、解码函数为<code>g()</code>，则目标是最小化损失函数<code>loss(x, g(h(x)))</code>，而最后获得的隐藏层激活值即是自动编码器捕捉到的训练数据中最显著的特征，权重矩阵<code>W</code>则可作为神经网络训练的初始值。</p><p>自动编码器的一种扩展是<strong>去噪自动编码器（Denoising Autoencoder，DAE）</strong>。其目标是最小化<code>loss(x, g(h(~x)))</code>，其中<code>~x</code>是被某种噪声损坏的<code>x</code>，使得隐藏层学习更健壮的特征。</p><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p><strong>LSTM（Long Short Term Memory，长短期记忆）</strong>是一种特殊的 RNN，用于缓解 RNN 梯度爆炸和梯度消失的问题。</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/LSTM-network.png" alt="LSTM-network"></p><p>在 LSTM 的一个可供重复使用的结构中，有四个层相互作用，并有 hidden state 和 cell state 两个 states。</p><p>在时间步 $t$，LSTM 首先决定是否将 cell state 中的信息丢弃。这个决定由使用 sigmoid 函数的层 $f_t$（称为“<strong>遗忘门</strong>”）。函数使用 $h_{t-1}$（前一个隐藏层的输出）和 $x_t$（当前输入），输出一个 [0, 1] 的值，1 表示完全保留，0 表示完全丢弃。</p><p>$$f_t = \sigma(W^fx_t + U^fh_{t-1})$$</p><p>之后，LSTM 决定存储在 cell state 中的新的信息。第一步，使用 sigmoid 函数的“<strong>输入门</strong>” $i_t$ 决定 LSTM 更新的值；第二步，一个使用 tanh 函数的层创建一个候选值向量 $\tilde{C_t}$ 来加到 cell state 上。</p><p>$$i_t = \sigma(W^ix_t + U^ih_{t-1})$$</p><p>$$\tilde{C_t} = tanh(W^nx_t + U^nh_{t-1})$$</p><p>然后用以下公式更新 $C_t$。其中遗忘门 $f_t$ 可以控制经过的梯度，以缓解梯度消失或梯度爆炸的问题。</p><p>$$C_t = f_t * C_{t-1} + i_t * \tilde{C_t}$$</p><p>最后，LSTM 基于 cell state 决定输出。这里用到一个使用 sigmoid 函数的“<strong>输出门</strong>”。</p><p>$$o_t = \sigma(W^o x_t + U^o h_{t-1})$$</p><p>$$h_t = o_t * tanh(C_t)$$</p><p>LSTM 常用于序列数据，但也可用于树结构数据。<strong>树结构 LSTM（Tree-structured LSTM）</strong>在表示句子含义上比普通的 LSTM 表现更好。[1]</p><h4 id="递归神经网络（RecNN）"><a href="#递归神经网络（RecNN）" class="headerlink" title="递归神经网络（RecNN）"></a>递归神经网络（RecNN）</h4><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/Recursive-NN.png" alt="RecNN"></p><p><strong>递归神经网络（RecNN）</strong>通常用于从数据中学习有向无环图结构（例如树结构）。给定句子的结构表示形式（例如解析树），RecNN 以自下而上的方式递归地生成父表示形式（一般为向量），通过组合 tokens 来生成短语表示形式，最终生成整个句子。然后可以使用句子级表示形式对给定的输入句子进行最终分类（例如情感分类）。</p><h3 id="情感分析任务"><a href="#情感分析任务" class="headerlink" title="情感分析任务"></a>情感分析任务</h3><p>情感分析可以根据分析对象的粒度分为三个 level：</p><ul><li><strong>Document level</strong>：对文章做总体的情感分类。</li><li><strong>Sentence level</strong>：一般先分类为有情感和无情感的（这个过程被称作 subjectivity classification），然后再对有情感的句子进行分类。</li><li><strong>Aspect level</strong>：任务是提取人们对实体和实体的各个方面/特征所表达的意见。因此，它包含了方面抽取、情感抽取和情感分类。</li></ul><h4 id="Document-level"><a href="#Document-level" class="headerlink" title="Document level"></a>Document level</h4><p>如果是多分类，例如 1-5 星，可以考虑用回归实现。</p><p>比较传统的方法是基于词袋模型，对每篇文档定长的数值化特征向量，长度为词典中的词量，值可以是词频或者 TF-IDF。词袋模型的缺点是文本中单词顺序和上下文所蕴含的信息被完全丢弃，n-gram 可以略微减少其影响，但是增加了数据稀疏和高维等问题。另外，词袋模型无法表示语义。</p><p>由此引入词嵌入和神经网络。神经网络可能只用于提取文本特征/表示，这些特征用于非神经网络的分类器中，以获得一个结合二者优点的分类器。</p><p>鉴于文章常有较长的依赖关系，注意力机制在文章级别的情感分类中也经常使用。</p><h4 id="Sentence-level"><a href="#Sentence-level" class="headerlink" title="Sentence level"></a>Sentence level</h4><p>由于句子相较文章来说要短得多，信息量更小，因此一些语义和句法的信息会对 Sentence level 的情感分析有帮助。早期研究中，会用包括解析树、情感词典、词性标注等来提供这些信息。但现在，CNN 和 RNN 使得不再需要用解析树从句子中提取特征，取而代之的是已经将语义、句法信息编码在内的词嵌入作为输入。同时 CNN 和 RNN 也会学习词与词之间的关系信息。</p><h4 id="Aspect-level"><a href="#Aspect-level" class="headerlink" title="Aspect level"></a>Aspect level</h4><p>在 Aspect level 的情感分析中，我们不只关注情感本身，还关注它所联系的目标对象（target）。一个对象通常是一个实体或者实体的某一方面，它们有时也被称为 aspect。</p><p>例如，在 “the screen is very clear but the battery life is too short.” 这一句话中，如果 target aspect 是 “screen”，那么情感是正面的；但如果 target aspect 是 “battery life”，那么情感是负面的。</p><p>Aspect level 的情感分析难度较大，因为需要捕捉目标对象和上下文（context）的语义联系。</p><p>三个重要任务：</p><ol><li>生成目标对象上下文的表示：可以使用之前提到的文本表示方法；</li><li>生成目标对象的表示：可以类似词嵌入，学习一个文本嵌入；</li><li>识别对于特定目标对象的重要情感上下文：目前常用注意力机制处理，但是还没有统治性的方法出现。</li></ol><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>还有一些其他的研究内容，包括：</p><ul><li>Aspect extraction and categorization：实体对象的抽取与分类；</li><li>Opinion expression extraction：意见表达词的抽取；</li><li>Sentiment composition：直译为情感成分，认为意见表达的情感取向是由其构成成分的意义和语法结构决定的；</li><li>Opinion holder extraction：抽取意见的持有者；</li><li>Temporal opinion mining：情感或意见是有时效性的，因此预测未来的情感或意见也是一个可研究的问题；</li><li>Sarcasm analysis：分析是否为讽刺；</li><li>Emotion analysis：我将这里的 Emotion 翻译成情绪，比情感更主观；</li><li>Multimodal data for sentiment analysis：多模态数据，例如包含文本、视觉和声学信息的数据，被用来帮助情感分析；</li><li>Resource-poor language and multilingual sentiment analysis：资源贫乏语言和多语言的情感分析；</li><li>Sentiment intersubjectivity：情感主体间性，研究语言的表层形式与相应的抽象概念之间的差距；</li><li>Lexicon Expansion：词汇扩展；</li><li>Financial Volatility Prediction：根据情感分析进行金融波动预测；</li><li>Opinion Recommendation：意见推荐，将用户写好的关于商品的意见选择推荐给其他用户；</li><li>Stance Detection：立场检测，分析例如政治 twitter 等数据隐含的政治立场。</li></ul><p>由此可见，情感分析仍然是一个值得深挖的研究方向，并且在深度学习的结合下，有更多的研究和应用出现。</p><h2 id="文献"><a href="#文献" class="headerlink" title="文献"></a>文献</h2><p>[1] Tai K.S, Socher R, Manning C. D. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2015), 2015.</p><script type="text/x-mathjax-config"> MathJax.Hub.Config({   tex2jax: {inlineMath: [ ['$', '$'] ],         displayMath: [ ['$$', '$$']]} });</script><script src="https://cdn.bootcss.com/mathjax/2.7.4/latest.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;原文链接：&lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf&quot;&gt;Deep Learning for Sentiment Analysis: A Survey&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Lei Zhang, Shuai Wang, Bing Liu&lt;/p&gt;
&lt;p&gt;简介：该文首先对深度学习的技术进行了概述，然后对基于深度学习的情感分析研究进行了全面的综述。&lt;/p&gt;
&lt;p&gt;选读原因：之前想把手头情感分析的工作再进一步推动一下，因此选择在 arXiv 上找一篇相关综述进行阅读。同样，也是对自然语言处理中的一些概念和技术的了解和复习。这篇综述相对较新，质量很高，而且作者之一刘兵老师是情感分析领域最有影响力的学者之一。当然，综述只是帮助对某一研究领域建立一个初步的印象，要想真正开始深入还是得读海量的 paper + 选择复现。这篇综述中对相关工作建立表格详细介绍，因此原文仍值得一看。&lt;/p&gt;
&lt;h2 id=&quot;研究概况&quot;&gt;&lt;a href=&quot;#研究概况&quot; class=&quot;headerlink&quot; title=&quot;研究概况&quot;&gt;&lt;/a&gt;研究概况&lt;/h2&gt;&lt;h3 id=&quot;研究背景&quot;&gt;&lt;a href=&quot;#研究背景&quot; class=&quot;headerlink&quot; title=&quot;研究背景&quot;&gt;&lt;/a&gt;研究背景&lt;/h3&gt;&lt;p&gt;情感分析（Sentiment analysis）或意见挖掘（Opinion mining）是对人群对于产品、服务、组织、个体、问题、事件、话题等实体的意见、情感、情绪、评价、态度的计算研究。鉴于网络社交媒体的风行，我们拥有了海量的情绪化数据。情感分析已经成为自然语言处理中最有吸引力的研究领域之一，它与管理科学、社会科学领域有交集。&lt;/p&gt;
&lt;p&gt;早期的情感分析技术包括监督方法（支持向量机、最大熵、朴素贝叶斯等监督机器学习方法）和无监督方法（包括利用情感词典、语法分析和句法模式的各种方法），而深度学习走红后，其应用到情感分析的出色效果催生大量基于深度学习的情感分析研究。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="深度学习" scheme="https://kyonhuang.top/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="自然语言处理" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    <category term="综述" scheme="https://kyonhuang.top/blog/tags/%E7%BB%BC%E8%BF%B0/"/>
    
    <category term="情感分析" scheme="https://kyonhuang.top/blog/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    
    <category term="意见挖掘" scheme="https://kyonhuang.top/blog/tags/%E6%84%8F%E8%A7%81%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>数据库系统复习笔记</title>
    <link href="https://kyonhuang.top/blog/database-systems-notes/"/>
    <id>https://kyonhuang.top/blog/database-systems-notes/</id>
    <published>2018-07-02T11:22:42.000Z</published>
    <updated>2018-07-02T11:24:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>简单地复习一下数据库系统。重在思想，基本不会涉及到 SQL 语句，有需要可以在文末的参考资料中找到相应资料。</p><h2 id="数据库简介"><a href="#数据库简介" class="headerlink" title="数据库简介"></a>数据库简介</h2><h3 id="基于文件系统的局限根源"><a href="#基于文件系统的局限根源" class="headerlink" title="基于文件系统的局限根源"></a>基于文件系统的局限根源</h3><p><strong>基于文件的系统</strong>存在的局限性（分离孤立、冗余、依赖性、文件格式不相容）可以归结为两个原因：</p><ul><li>数据的定义被嵌入到应用程序中，而不是分开和独立地存储；</li><li>除了应用程序规定之外的那些数据访问和操作无法得到控制。</li></ul><p>因此，引入数据库和数据库管理系统。</p><h3 id="数据库与数据库管理系统"><a href="#数据库与数据库管理系统" class="headerlink" title="数据库与数据库管理系统"></a>数据库与数据库管理系统</h3><ul><li><strong>数据库</strong>：为满足某个组织机构的信息要求而设计的一个逻辑相关数据及其描述的共享集。<ul><li>数据库是“逻辑相关的”，当分析一个组织需要的信息时，总是试图找出实体、属性和联系；</li><li><strong>实体</strong>是组织中一个独立的、将要在数据库中体现出来的对象（人、地点、东西、概念、事件）；</li><li><strong>属性</strong>描述我们想要记录的对象的某一方面的特性；</li><li><strong>联系</strong>描述实体之间的关联。</li></ul></li><li><strong>数据库管理系统（DBMS）</strong>：一个支持用户对数据库进行定义、创建、维护及控制访问的软件系统。<ul><li>允许用户定义数据库，通常是通过<strong>数据定义语言（DDL）</strong>；</li><li>允许用户在数据库中对数据进行增删改查，通常通过<strong>数据操作语言（DML）</strong>，最常见的查询语言是<strong>结构化查询语言（SQL）</strong>。</li><li>提供数据库的受控访问。</li></ul></li></ul><p>DBMS 的功能：</p><ul><li>数据存储、检索和更新；</li><li>提供用户可访问的目录；</li><li>提供事务支持；</li><li>并发控制的服务；</li><li>恢复服务、授权服务、完整性服务；</li><li>提高数据独立性的服务；</li><li>支持数据通信。</li></ul><p>DBMS 的优点：</p><ul><li>受控的数据冗余；</li><li>数据一致性；</li><li>相同数据量表示更多信息；</li><li>数据共享；</li><li>增强的数据完整性；</li><li>增强的安全性；</li><li>强制执行标准；</li><li>经济合算的规模；</li><li>平衡各种需求冲突；</li><li>增强的数据可访问性和响应性；</li><li>提高的生产率；</li><li>通过数据的独立性增强可维护性；</li><li>提高的并发性；</li><li>增强的备份和恢复服务。</li></ul><p>DBMS 的缺点：</p><ul><li>复杂性高；</li><li>规模大；</li><li>DBMS 的费用高；</li><li>需要附加的硬件费用；</li><li>转化费用大；</li><li>性能相对较低；</li><li>故障带来的影响较大。</li></ul><span id="more"></span><h2 id="数据库环境"><a href="#数据库环境" class="headerlink" title="数据库环境"></a>数据库环境</h2><h3 id="ANSI-SPARC-三层体系结构"><a href="#ANSI-SPARC-三层体系结构" class="headerlink" title="ANSI-SPARC 三层体系结构"></a>ANSI-SPARC 三层体系结构</h3><p><strong>三层体系结构</strong>包括<strong>外部层</strong>、<strong>概念层</strong>和<strong>内部层</strong>：</p><ul><li><strong>外部层</strong>：数据库的用户视图。这一层描述与每一个用户相关的数据库部分。</li><li><strong>概念层</strong>：数据库的整体视图。这一层描述了哪些数据被存储在数据库中，以及这些数据库之间的联系。</li><li><strong>内部层</strong>：数据库在计算机上的物理表示。这一层描述数据是如何存储在数据库中的。</li></ul><p>内部层之下的是<strong>物理层</strong>，物理层可能在 DBMS 的指导下受操作系统的控制。</p><p>三层体系结构的目的是将用户的数据库视图与数据库的物理描述分离开，以保证<strong>数据独立性</strong>。这意味着对较低层的修改不会对较高层造成影响。有两种类型的数据独立性：<strong>逻辑数据独立性</strong>和<strong>物理数据独立性</strong>：</p><ul><li><strong>逻辑数据独立性</strong>：指外部模式不受概念模式变化（例如添加或删除实体、属性或者联系）的影响；</li><li><strong>物理数据独立性</strong>：指概念模式不受内部模式变化（例如使用不同的文件组织方式或者存储结构、使用不同的存储设备、修改索引或散列算法）的影响。</li></ul><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><p><strong>数据模型</strong>是一组集成的概念，用于描述和操纵组织机构内的数据、数据间联系以及对数据的约束。其包含下列三个组件：</p><ul><li><strong>结构部分</strong>，由一组创建数据库的规则组成。</li><li><strong>操纵部分</strong>，定义允许对数据进行的操作的种类；</li><li><strong>一组完整性约束</strong>，确保数据的准确性。</li></ul><p>数据模型的分类：</p><ul><li>基于对象的数据模型：用到实体、属性和联系等概念<ul><li>实体-联系（ER）模型</li><li>语义模型</li><li>函数模型</li><li>面向对象模型</li></ul></li><li>基于记录的数据模型：数据库由不同类型的固定格式记录组成<ul><li>关系数据模型</li><li>网状数据模型</li><li>层次数据模型</li></ul></li><li>物理数据模型：描述数据如何存储在计算机中</li></ul><h2 id="关系模型"><a href="#关系模型" class="headerlink" title="关系模型"></a>关系模型</h2><h3 id="基本术语"><a href="#基本术语" class="headerlink" title="基本术语"></a>基本术语</h3><h4 id="关系数据结构"><a href="#关系数据结构" class="headerlink" title="关系数据结构"></a>关系数据结构</h4><ul><li><strong>关系</strong>：由行和列组成的表；</li><li><strong>属性</strong>：关系中命名的列；</li><li><strong>域</strong>：一个或多个属性的取值集合；</li><li><strong>元组</strong>：关系中的每一行；</li><li><strong>维数</strong>：关系所包含的属性的个数；</li><li><strong>基数</strong>：关系所包含的元组的个数；</li><li><strong>关系模式</strong>：用一组属性和域名对定义的具名的关系。</li></ul><h4 id="关系关键字"><a href="#关系关键字" class="headerlink" title="关系关键字"></a>关系关键字</h4><ul><li><strong>超关键字（superkey）</strong>：一个属性或属性集合，它能唯一地标识出关系中的每个元组；</li><li><strong>候选关键字（candidate key）</strong>：最小的超关键字；</li><li><strong>主关键字（primary key）</strong>：被选用于唯一标识关系中各元组的候选关键字。没有被选为主关键字的候选关键字称为<strong>可替换关键字（alternate key）</strong>；</li><li><strong>外部关键字（foreign key）</strong>：当一个关系中的某个属性或属性集合与另一个关系（也可能就是自己）的候选关键字匹配时，就称这个属性或属性集合为外部关键字。</li></ul><h3 id="完整性约束"><a href="#完整性约束" class="headerlink" title="完整性约束"></a>完整性约束</h3><p><strong>域约束</strong>：限定关系中各个属性的取值集合。</p><p><strong>实体完整性</strong>：在基本关系中，主关键字的属性不能为空；</p><p><strong>引用完整性</strong>：如果在关系中存在某个外部关键字，则它的值与主关系中某个元组的候选关键字取值相等（或者全为空）。</p><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><p>在关系模型中，“<strong>视图</strong>”指<strong>虚关系</strong>或称<strong>导出关系</strong>，即无需单独存在，必要时可从一或多个基本关系中<strong>动态地</strong>将其导出。</p><p>需要视图机制的原因有：</p><ul><li>通过对特定用户隐藏部分数据库信息，提供了一个强大而灵活的安全机制；</li><li>允许用户根据自己的需求自定义访问数据的方法；</li><li>可以简化对基本关系的复杂操作。</li></ul><p>不是所有的视图都可更新。</p><h2 id="事务管理"><a href="#事务管理" class="headerlink" title="事务管理"></a>事务管理</h2><p><strong>事务</strong>指由一个或多个 SQL 语句组成的<strong>逻辑操作单位</strong>，可视为一组操作，通过<code>COMMIT</code>语句提交，通过<code>ROLLBACK</code>语句回退。其满足 ACID 特性：</p><ul><li><strong>原子性（Atomicity）</strong>：事务被视为不可分割的最小单元，要么全部执行，要么都不执行。</li><li><strong>一致性（Consistency）</strong>：数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。</li><li><strong>隔离性（Isolation）</strong>：事务的执行相互独立。未完成事务的中间结果对其它事务是不可见的。由并发控制子系统负责保证事务的隔离性。</li><li><strong>持久性（Durability）</strong>：成功完成（提交）的事务的结果要永久地记录在数据库中，不能因以后的故障而丢失。可以通过数据库备份和数据恢复来保证事务的持久性。</li></ul><!--## 实体-联系建模（E-R）--><!--12、13章--><h2 id="规范化"><a href="#规范化" class="headerlink" title="规范化"></a>规范化</h2><h3 id="规范化的目的"><a href="#规范化的目的" class="headerlink" title="规范化的目的"></a>规范化的目的</h3><p><strong>规范化</strong>是一种能够生成一组既具有所期望的特性又能满足企业数据需求的关系的技术，也是一种<strong>依靠关键字和属性之间的函数依赖对关系进行验证</strong>的形式化技术。</p><h3 id="数据冗余与更新异常"><a href="#数据冗余与更新异常" class="headerlink" title="数据冗余与更新异常"></a>数据冗余与更新异常</h3><p>存在数据冗余的关系可能产生<strong>更新异常</strong>问题，更新异常可分为<strong>插入异常</strong>、<strong>删除异常</strong>和<strong>修改异常</strong>。</p><h3 id="函数依赖"><a href="#函数依赖" class="headerlink" title="函数依赖"></a>函数依赖</h3><p>规范化的一个重要的概念是<strong>函数依赖</strong>，函数依赖描述了属性之间的联系。例如，加入 A 和 B 是关系 R 的属性（或属性组），如果 A 的每个值都仅与 B 中的一个值对应，那么 B 就函数依赖于 A（记为 A -&gt; B）。函数依赖的<strong>决定方</strong>是位于箭头左端的属性或属性组。</p><p><strong>完全函数依赖</strong>：假设 A 和 B 是某一关系的属性（组），若 B 函数依赖于 A，但不函数依赖于 A 的任一真子集，则称 B 完全函数依赖于 A。</p><p><strong>传递依赖</strong>：假设 A、B、C 是某一关系的属性，若 A -&gt; B，B -&gt; C，则称 C 通过 B 传递依赖于 A（假设 A 并不函数依赖于 B 或 C）。</p><p>在规范化时使用的函数依赖具有以下特性：依赖左右两边的属性（组）之间具有一对一的联系，且恒成立，并且右边完全函数依赖于左边。</p><p>确定关系函数依赖集的<strong>主要目的</strong>是确定该关系必须满足的完整性约束集，从而确定候选关键字和主关键字。</p><h3 id="规范化过程"><a href="#规范化过程" class="headerlink" title="规范化过程"></a>规范化过程</h3><p><strong>非范式（UNF）</strong>是一个包含了一个或多个重复组的表</p><p><strong>第一范式（1NF）</strong>是指每一行和每一列相交的位置有且仅有一个值的关系。</p><p><strong>第二范式（2NF）</strong>是指已经是第一范式且每个非关键字属性都完全函数依赖于主关键字的关系。</p><p><strong>第三范式（3NF）</strong>是满足第一范式和第二范式的要求且所有非关键字属性都不传递依赖于主关键字的关系。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>主要参考机械工业出版社《数据库系统–设计、实现与管理（基础篇）》。</p><p>其他参考资料：</p><ul><li><a href="https://github.com/CyC2018/Interview-Notebook/blob/master/notes/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86.md">Interview-Notebook/数据库系统原理.md at master · CyC2018/Interview-Notebook</a></li><li><a href="https://github.com/CyC2018/Interview-Notebook/blob/master/notes/SQL.md">Interview-Notebook/SQL.md at master · CyC2018/Interview-Notebook</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;简单地复习一下数据库系统。重在思想，基本不会涉及到 SQL 语句，有需要可以在文末的参考资料中找到相应资料。&lt;/p&gt;
&lt;h2 id=&quot;数据库简介&quot;&gt;&lt;a href=&quot;#数据库简介&quot; class=&quot;headerlink&quot; title=&quot;数据库简介&quot;&gt;&lt;/a&gt;数据库简介&lt;/h2&gt;&lt;h3 id=&quot;基于文件系统的局限根源&quot;&gt;&lt;a href=&quot;#基于文件系统的局限根源&quot; class=&quot;headerlink&quot; title=&quot;基于文件系统的局限根源&quot;&gt;&lt;/a&gt;基于文件系统的局限根源&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;基于文件的系统&lt;/strong&gt;存在的局限性（分离孤立、冗余、依赖性、文件格式不相容）可以归结为两个原因：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据的定义被嵌入到应用程序中，而不是分开和独立地存储；&lt;/li&gt;
&lt;li&gt;除了应用程序规定之外的那些数据访问和操作无法得到控制。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，引入数据库和数据库管理系统。&lt;/p&gt;
&lt;h3 id=&quot;数据库与数据库管理系统&quot;&gt;&lt;a href=&quot;#数据库与数据库管理系统&quot; class=&quot;headerlink&quot; title=&quot;数据库与数据库管理系统&quot;&gt;&lt;/a&gt;数据库与数据库管理系统&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据库&lt;/strong&gt;：为满足某个组织机构的信息要求而设计的一个逻辑相关数据及其描述的共享集。&lt;ul&gt;
&lt;li&gt;数据库是“逻辑相关的”，当分析一个组织需要的信息时，总是试图找出实体、属性和联系；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实体&lt;/strong&gt;是组织中一个独立的、将要在数据库中体现出来的对象（人、地点、东西、概念、事件）；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;属性&lt;/strong&gt;描述我们想要记录的对象的某一方面的特性；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;联系&lt;/strong&gt;描述实体之间的关联。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据库管理系统（DBMS）&lt;/strong&gt;：一个支持用户对数据库进行定义、创建、维护及控制访问的软件系统。&lt;ul&gt;
&lt;li&gt;允许用户定义数据库，通常是通过&lt;strong&gt;数据定义语言（DDL）&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;允许用户在数据库中对数据进行增删改查，通常通过&lt;strong&gt;数据操作语言（DML）&lt;/strong&gt;，最常见的查询语言是&lt;strong&gt;结构化查询语言（SQL）&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;提供数据库的受控访问。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DBMS 的功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据存储、检索和更新；&lt;/li&gt;
&lt;li&gt;提供用户可访问的目录；&lt;/li&gt;
&lt;li&gt;提供事务支持；&lt;/li&gt;
&lt;li&gt;并发控制的服务；&lt;/li&gt;
&lt;li&gt;恢复服务、授权服务、完整性服务；&lt;/li&gt;
&lt;li&gt;提高数据独立性的服务；&lt;/li&gt;
&lt;li&gt;支持数据通信。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DBMS 的优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;受控的数据冗余；&lt;/li&gt;
&lt;li&gt;数据一致性；&lt;/li&gt;
&lt;li&gt;相同数据量表示更多信息；&lt;/li&gt;
&lt;li&gt;数据共享；&lt;/li&gt;
&lt;li&gt;增强的数据完整性；&lt;/li&gt;
&lt;li&gt;增强的安全性；&lt;/li&gt;
&lt;li&gt;强制执行标准；&lt;/li&gt;
&lt;li&gt;经济合算的规模；&lt;/li&gt;
&lt;li&gt;平衡各种需求冲突；&lt;/li&gt;
&lt;li&gt;增强的数据可访问性和响应性；&lt;/li&gt;
&lt;li&gt;提高的生产率；&lt;/li&gt;
&lt;li&gt;通过数据的独立性增强可维护性；&lt;/li&gt;
&lt;li&gt;提高的并发性；&lt;/li&gt;
&lt;li&gt;增强的备份和恢复服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DBMS 的缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复杂性高；&lt;/li&gt;
&lt;li&gt;规模大；&lt;/li&gt;
&lt;li&gt;DBMS 的费用高；&lt;/li&gt;
&lt;li&gt;需要附加的硬件费用；&lt;/li&gt;
&lt;li&gt;转化费用大；&lt;/li&gt;
&lt;li&gt;性能相对较低；&lt;/li&gt;
&lt;li&gt;故障带来的影响较大。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="学科补完计划" scheme="https://kyonhuang.top/blog/categories/%E5%AD%A6%E7%A7%91%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"/>
    
    
    <category term="学科复习笔记" scheme="https://kyonhuang.top/blog/tags/%E5%AD%A6%E7%A7%91%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="数据库系统" scheme="https://kyonhuang.top/blog/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>中文短文本情感分析 web 应用开发记录（一）</title>
    <link href="https://kyonhuang.top/blog/sentiment-analysis-webapp-record1/"/>
    <id>https://kyonhuang.top/blog/sentiment-analysis-webapp-record1/</id>
    <published>2018-06-21T14:22:05.000Z</published>
    <updated>2018-07-17T07:53:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>“中文短文本情感分析 web 应用”是我带领的小组在《软件工程》这门课的大作业选题。按照这门课大作业的传统风格，就是做一个比较寻常的网站/APP，不过我这次提议另辟蹊径，一是我个人对情感分析这个方面比较感兴趣，想要借此机会进行了解和实践；二是组内成员都志在继续深造，做一个带有科研意味的项目或许能为简历添色。</p><p>先简单介绍一下我们的项目。该 web 应用可分析用户输入的中文短文本中蕴含的情感，并输出数值化结果。前端使用 Vue.js 开发，后台使用 Flask 开发。算法部分目前包含自主实现的词袋模型和引入的 SnowNLP。项目地址如下（目前服务器关了，想要尝试效果可以将项目 clone 到本地，照着 README 运行）：</p><ul><li><a href="http://kyonhuang.top/sentiment-analysis-webapp/">在线使用地址</a></li><li><a href="https://github.com/bighuang624/sentiment-analysis-webapp">项目地址</a></li></ul><p>截止这篇博文写作时，这个项目算是成型，也在 Github 上获得了 7 个 star。但是也遇到了一些问题，当然最大的问题就是分析效果不好。效果不好的原因包括很多，后文会一一介绍，当然其中一个是我们目前的算法部分还没有涉及到深度学习，不过我正在争取在近期用 LSTM 等实现效果较好的模型。</p><p>这篇博文首先会简单介绍情感分析的概念、技术和难点。之后针对我们的项目，详细介绍我们使用的算法、迄今为止遇到的问题和一些改进措施。</p><span id="more"></span><h2 id="什么是情感分析？"><a href="#什么是情感分析？" class="headerlink" title="什么是情感分析？"></a>什么是情感分析？</h2><p><strong>情感分析（Sentiment analysis）</strong>或意见挖掘（Opinion mining）是对人群对于产品、服务、组织、个体、问题、事件、话题等实体的意见、情感、情绪、评价、态度的计算研究。鉴于网络社交媒体的风行，我们拥有了海量的情绪化数据。情感分析已经成为自然语言处理中最有吸引力的研究领域之一，它与管理科学、社会科学领域有交集。情感分析在现实中的典型应用包括舆情分析、民意调查、产品意见调查、预测股票市场行情等等。</p><p>最典型的情感分析会分析一段文本对某个对象的情感是正面的还是负面。更细致的处理包括情感的分类（快乐、愤怒、恐惧、悲哀等）和情感的程度。意见挖掘可能还需要从文本中挖掘出对象的属性，再分析对应属性的情感。</p><p>早期的情感分析技术包括监督方法（支持向量机、最大熵、朴素贝叶斯等监督机器学习方法）和非监督方法（包括利用情感词汇、语法分析和句法模式的各种方法），而深度学习走红后，其应用到情感分析的出色效果催生大量基于深度学习的情感分析研究[1]。</p><h2 id="中文情感分析的难点"><a href="#中文情感分析的难点" class="headerlink" title="中文情感分析的难点"></a>中文情感分析的难点</h2><p>以下是我在查阅资料后总结的一些中文情感分析的难点：</p><ol><li>很多情感的表达是隐晦的，没有特别明显的代表性词语。例如“蓝屏”这个词一般不会出现在情感词典之中，但这个词明显表达了不满的情绪。因此需要另外根据具体领域构建针对性的情感词典。</li><li>同一个词在不同语境或领域里有不同的意思，可能表达不同的情感态度，例如“我家洗衣机声音很大”这些很可能是差评，而“我家音响声音很大”很可能就是好评。反讽的表达同理。</li><li>网络流行语等新词也会影响情感分析，比如“给力”、“不明觉厉”、“累觉不爱”等，这些词利用传统的分词一般都会被切开，而且会影响词性标注，如果想避免只能加入人工干预，修改分词的粒度和词性标注的结果。</li><li>以上三条在做英语情感分析时也会遇到。但是中文独有的问题是缺少高质量的有标注数据集。相比起来，英文情感分析有公认的数据集 <a href="http://ai.stanford.edu/~amaas/data/sentiment/">IMDB</a>。此外，可能还需要一些高质量的情感词典，英文有 SentiWordNet，但是中文的词典资源质量不高，不细致，另外缺乏主客观词典。当然，因为有监督的机器学习方法的风行，现在情感分析也不一定非要情感词典。</li></ol><p>因为我们的项目主要还是拿文本分类的方法进行处理，所以以上问题不是都遇到了，仅停留在了解层面，如有谬误欢迎指出。</p><h2 id="初步尝试"><a href="#初步尝试" class="headerlink" title="初步尝试"></a>初步尝试</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>首先谈谈数据。根据前文所述，中文的有标注数据集很难找。我前两周和做过类似工作的学长交流时，他的做法是自行标注。而我们选择的做法是自己用 Python 编写爬虫爬取各类含评论的网站，将评论正文作为数据，评论的星级作为标签：</p><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/douban-comment.png"></p><p>我们最终爬取了豆瓣的 146856 条电影评论和蜂窝网的 174055 条酒店评论作为我们的数据集。你可以在 <a href="https://github.com/bighuang624/sentiment-analysis-webapp/tree/master/training">sentiment-analysis-webapp/training/</a> 找到这两份数据以及训练模型的代码。</p><p>我们希望最终给出的结果也能反应情感的程度，而不是单纯的正负二元分类。因此，我们没有对标签做后续处理。</p><h3 id="算法部分实现过程"><a href="#算法部分实现过程" class="headerlink" title="算法部分实现过程"></a>算法部分实现过程</h3><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>既然是使用监督机器学习算法，那么自然需要特征。计算机是看不懂自然语言的，<strong>从文本中提取特征</strong>最终的结果一般是将文本转换为<strong>向量</strong>，而这一步不管是使用什么算法，都需要首先对文本进行分词。</p><p>成熟的中文分词工具还是比较多的，包括 <a href="https://github.com/FudanNLP/fnlp">FudanNLP</a>、结巴分词、THULAC、Stanford CoreNLP。其中 Stanford CoreNLP 是重量级工具，需要自备数据集、数据字典等，不太方便；另外三个工具经过简单的比较后，我们最终选用了结巴分词，因为确实很容易使用，分词效果也很好。</p><p>分词的相关代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chinese_word_cut</span>(<span class="params">mytext</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot; &quot;</span>.join(jieba.cut(mytext))</span><br><span class="line">    </span><br><span class="line">X[<span class="string">&#x27;cutted_comment&#x27;</span>]=X.comment.apply(chinese_word_cut)</span><br></pre></td></tr></table></figure><h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p>我们的算法都是使用 scikit-learn 实现的，其中从文本中提取特征的方法在<code>sklearn.feature_extraction.text</code>包中。</p><p>比较典型的自然语言向量化方法有词袋模型（带或不带 TF-IDF）、词嵌入等等，在这里可以简单的介绍一下。</p><h5 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h5><p><strong>词袋模型（bag of words）</strong>是信息检索领域常用的文档表示方法。其前提是不考虑词语的出现顺序，也不考虑词语和前后词语之间的连接。<strong>文档中每个单词的出现都是独立的，不依赖于其它单词是否出现</strong>。这样，每个句子被编码成一个 $R^{|V| \times 1}$向量，其中 $|V|$是语料库中所有单词的数量。所有单词组成一个词汇表，每个句子向量的第 $i$ 位即是词汇表中第 $i$ 个词在句子中出现的次数。</p><p>词袋模型可以以单个词语做单位，也可以以 n 个连续的单词做单位（被称为 n-gram）。词袋模型的优点是原理简单，容易实现；其缺点是文本中单词顺序和上下文所蕴含的信息被完全丢弃。</p><p>词袋模型的相关代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">max_df=<span class="number">0.8</span> <span class="comment">#在超过这一比例的文档中出现的关键词过于平凡，去除掉</span></span><br><span class="line">min_df=<span class="number">3</span> <span class="comment">#在低于这一数量的文档中出现的关键词过于独特，去除掉</span></span><br><span class="line"></span><br><span class="line">vect = CountVectorizer(max_df=max_df, min_df=min_df, token_pattern=<span class="string">u&#x27;(?u)\\b[^\\d\\W]\\w+\\b&#x27;</span>)</span><br><span class="line">term_matrix = pd.DataFrame(vect.fit_transform(X_train.cutted_comment).toarray(), columns=vect.get_feature_names())</span><br></pre></td></tr></table></figure><h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p>TF-IDF 是在词袋模型的基础上<strong>度量词语重要性</strong>的指标。其基于基本假设：<strong>对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取TF词频作为测度，就可以体现同类文本的特点</strong>。</p><p>根据以上思想，我们通过 TF-IDF 这个指标来寻找重要的词语。TF（Term Frequency）指<strong>词项频率</strong>，用于计算该词描述文档内容的能力；IDF（Inverse Document Frequency）指<strong>逆文档频率</strong>，用于计算该词区分文档的能力。TF-IDF 通过以下方法计算：假定文档集中有 $N$ 篇文档，$f_{ij}$ 为词项 $i$ 在文档 $j$ 中出现的频率（即次数），则词项 $i$ 在文档 $j$ 中的词项频率 $TF_{ij}$ 定义为</p><p>$$TF_{ij} = \frac{f_{ij}}{max_kf_{kj}}$$</p><p>这里有一个归一化的操作，通过除以同一文档中出现最多的词项（可能不考虑停用词的频率）的频率来计算。</p><p>假定词项 $i$ 在文档集的 $n_i$ 篇文档中出现，那么词项 $i$ 的 IDF 定义如下：</p><p>$$IDF_i = log_2{\frac{N}{n_i}}$$</p><p>于是词项 $i$ 在文档中的得分被定义为 $TF_{ij} \times IDF_i$。相关代码如下（其实就是将<code>sklearn.feature_extraction.text</code>提供的<code>CountVectorizer</code>换为<code>TfidfVectorizer</code>）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">max_df=<span class="number">0.8</span> <span class="comment">#在超过这一比例的文档中出现的关键词过于平凡，去除掉</span></span><br><span class="line">min_df=<span class="number">3</span> <span class="comment">#在低于这一数量的文档中出现的关键词过于独特，去除掉</span></span><br><span class="line"></span><br><span class="line">vect = TfidfVectorizer(max_df=max_df, min_df=min_df, token_pattern=<span class="string">u&#x27;(?u)\\b[^\\d\\W]\\w+\\b&#x27;</span>)</span><br><span class="line">term_matrix = pd.DataFrame(vect.fit_transform(X_train.cutted_comment).toarray(), columns=vect.get_feature_names())</span><br></pre></td></tr></table></figure><p>TF-IDF 在通用情况下效果一般不错，但是也存在一些问题：（1）单纯地认为文本频数小的单词就越重要，文本频数大的单词就越无用；（2）没有体现出单词的位置信息（例如在 Web 文档中处于网页不同位置或标签中的词应该有不同的权重）。</p><h5 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h5><p><strong>词嵌入（Word Embedding）</strong>是指把一个维数为所有词的数量的高维空间（one-hot 形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。因为我们暂时还没用到相关技术，所以不对学习词嵌入的方法进行介绍了（可能会在下一篇博文谈论）。想要了解更多可以看<a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Sequence_Models/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5">自然语言处理与词嵌入 - 吴恩达《深度学习》系列课程笔记</a>。</p><h4 id="停用词"><a href="#停用词" class="headerlink" title="停用词"></a>停用词</h4><p>谈一下停用词的问题。无论是处理中文还是英文，都需要处理停用词。<strong>停用词指那些用于辅助表达但本身不携带任何含义的词</strong>，比如英文中的定冠词“the”。这些词经常在文本中大量出现，如果将其和那些包含信息更丰富的词汇放在一起统计，就容易对我们把握文本的特征形成干扰。所以在文本分类中，一般要首先在文本数据中去掉这些停用词。</p><p>scikit-learn 中自带了英文停用词。如果想做中文停用词的处理，可以用一些机构开源的停用词表。哈工大、四川大学、百度都有类似的词表，可以在<a href="https://github.com/chdd/weibo/tree/master/stopwords">这里</a>找到。</p><p>不过一个值得讨论的问题是，我们做的是情感分析工作，而非单纯的文本分类。实际上，很多语气词和标点等都是停用词，但是却对句子的情感表达有着很大的影响。因此，一般在做情感分析工作时，倾向于不去处理停用词。我们会在后文的效果展示中讨论是否去除停用词对我们实际的模型效果的影响。</p><p>还是放一下处理停用词的相关代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_custom_stopwords</span>(<span class="params">stop_words_file</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(stop_words_file) <span class="keyword">as</span> f:</span><br><span class="line">        stopwords = f.read()</span><br><span class="line">    stopwords_list = stopwords.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    custom_stopwords_list = [i <span class="keyword">for</span> i <span class="keyword">in</span> stopwords_list]</span><br><span class="line">    <span class="keyword">return</span> custom_stopwords_list</span><br><span class="line">    </span><br><span class="line">stop_words_file = <span class="string">&quot;stopwords.dat&quot;</span> <span class="comment"># 停用词表文件名</span></span><br><span class="line">stopwords = get_custom_stopwords(stop_words_file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里增加了一个选项</span></span><br><span class="line">vect = CountVectorizer(max_df = max_df, </span><br><span class="line">                       min_df = min_df, </span><br><span class="line">                       token_pattern=<span class="string">u&#x27;(?u)\\b[^\\d\\W]\\w+\\b&#x27;</span>, </span><br><span class="line">                       stop_words=<span class="built_in">frozenset</span>(stopwords))</span><br></pre></td></tr></table></figure><h4 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h4><p>我们初期使用的分类算法是<a href="http://kyonhuang.top/Machine-learning-in-action-notes/#/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF">朴素贝叶斯</a>，相关代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">nb = MultinomialNB()</span><br><span class="line">nb.fit(X_train.cutted_comment, y_train)</span><br></pre></td></tr></table></figure><h4 id="管道和模型持久化"><a href="#管道和模型持久化" class="headerlink" title="管道和模型持久化"></a>管道和模型持久化</h4><p>scikit-learn 提供<strong>管道</strong>功能，可以将处理流程中的工作顺序连接，隐藏其中的功能，从外部一次调用，就能完成顺序定义的全部工作。这样做可以简化调用过程，减少出错几率。</p><p>同时，scikit-learn 也支持<strong>模型持久化</strong>，即将训练好的模型保存到硬盘，待后台程序启动时可以直接加载到内存。这样，就不必每次启动后台程序时都要花费大量时间和资源进行训练了。相关代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line">pipe = make_pipeline(vect, nb)</span><br><span class="line">pipe.fit(X_train.cutted_comment, y_train)</span><br><span class="line">y_pred=pipe.predict(X_test.cutted_comment)</span><br><span class="line">joblib.dump(pipe, <span class="string">&quot;hotel_comment.pkl&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>为了量化模型效果，需要模型实际预测输出和样本真是输出之间的差异（即<strong>误差</strong>）。因为过拟合的存在，训练误差不适合作为模型选择标准，因此需要从数据集中划分出与训练集互斥的测试集，以测试误差作为泛化误差的近似。</p><p>这里，我们选用<strong>交叉验证法（cross-validation）</strong>，这是一种十分常用的模型评估方法。<strong>k 折交叉验证（k-fold CV）</strong>指划分 k 个互斥子集，每次用 k-1 个子集作训练集，余下一个子集作测试集，进行 k 次训练与测试并取均值。其优点是结果准确，Kaggle 的参赛者大多会提交 CV 分最高的模型，而非在官方提供的测试集上得分最高的模型；缺点是计算量比较大。</p><p>常用的 k 值包括 5、10、20。这里我们统一用 5 折交叉验证来分析各种搭配得到的模型效果。其中所使用的停用词为<a href="https://github.com/dongxiexidian/Chinese/blob/master/stopwords.dat">这份</a>。</p><p>对于豆瓣数据集，各种模型的评分如下：</p><ul><li>CountVectorizer + 不去除停用词：0.505020743656528</li><li>CountVectorizer + 去除停用词：0.4937080095876466</li><li>TfidfVectorizer + 不去除停用词：0.49594151903667905</li></ul><p>而对于酒店数据集，各种模型的评分如下：</p><ul><li>CountVectorizer + 不去除停用词：0.5133750057452774</li><li>CountVectorizer + 去除停用词：0.5060018311093455</li><li>TfidfVectorizer + 不去除停用词：0.5297950445100629</li></ul><p>可以看到，去除停用词的效果普遍比不去除停用词要差。而词袋模型和 TF-IDF 的效果比较结果则与数据集有关，这也说明需要针对具体问题选择合适的学习算法。</p><h2 id="问题和改进措施"><a href="#问题和改进措施" class="headerlink" title="问题和改进措施"></a>问题和改进措施</h2><p>从之前 CV 的分数，可以判断我们模型的正确率大概在 50% 左右，相比起在 5 种给分中随机预测的 20% 看似要好很多，但实际表现为不管输入的是正面还是负面情感句子、程度如何，基本上都给 5 分，偶尔会给 4 分。</p><h3 id="样本类别不均衡"><a href="#样本类别不均衡" class="headerlink" title="样本类别不均衡"></a>样本类别不均衡</h3><p>从模型经常给出高分预测的表现，我们很容易做出样本中<strong>正负类别样本不均衡</strong>的判断。实际上，我们的酒店评论数据集中，有 148939 条标签值大于 3，而只有 6895 条标签值小于 3，比例达到 21.6 : 1。豆瓣数据集情况略好，但正负面评论数比例也达到了 5.69 : 1。数据集中样本类别不均衡导致训练出的模型偏向于给出正面预测，以减小损失函数。</p><p>处理样本不均衡的方式包括以下几种：</p><ul><li><p><strong>尝试其他评价指标</strong>：当样本类别不均衡时，准确度这个指标不但没有说服力，反而会对分类器的好坏产生误导。这时，<strong>精确率（Precision）</strong>、<strong>召回率（Recall）</strong>以及 <strong>F1 得分（F1 Score，精确率与召回率的调和平均）</strong>作为评价指标会更加有效。</p></li><li><p><strong>对数据集进行过采样和欠采样</strong>：</p><ul><li><strong>过采样（over-sampling）</strong>指采样个数大于该类样本个数，适用于小类的数据样本。具体表现为添加部分样本的副本；</li><li><strong>欠采样（under-sampling）</strong>指采样个数小于该类样本个数，适用于大类的数据样本。具体表现为删除部分样本。</li></ul></li><li><p><strong>数据增强</strong>：数据增强是 CV 中经常使用的方式，联系到 NLP 领域，可以将文本进行句子顺序打乱、句内词序打乱、同义词替换等操作来增加相应类别的样本量。</p></li></ul><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>使用 scikit-learn 提供的方法，我们来计算一下在原始酒店数据集上用 TfidfVectorizer + 不去除停用词训练出来的模型的精确率、召回率和 F1 得分：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">metrics.accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0.5307027623293653</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">metrics.confusion_matrix(y_test, y_pred)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[    0,     0,   151,   237,    27],</span></span><br><span class="line"><span class="string">       [    0,     0,   227,  1017,   106],</span></span><br><span class="line"><span class="string">       [    0,     0,   530,  3603,   507],</span></span><br><span class="line"><span class="string">       [    0,     1,   327, 12089,  6894],</span></span><br><span class="line"><span class="string">       [    0,     0,    58,  7266, 10474]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">metrics.precision_score(y_test, y_pred, average=<span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0.29816554278872476</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">metrics.recall_score(y_test, y_pred, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0.5307027623293653</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">metrics.f1_score(y_test, y_pred, average=<span class="string">&#x27;weighted&#x27;</span>)  </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">0.5048778200537771</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>可以看到精确率相对来说比较低。</p><!--1. 说明什么？2. 进一步处理数据--><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><p>解决了样本类别不均衡以及模型评估的问题，我们很容易想到，我们一开始采用的朴素贝叶斯是否是在这个问题上表现最好的机器学习模型？SVM、随机森林或者 Kaggle 前任大杀器 XGBoost 会不会有更出色的结果？</p><p>To be continued…</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>文本分类中常见的特征选择方法除开 TF-IDF 外，还包括信息增益、互信息、期望交叉熵、CHI 统计等[2]。根据清华大学刘知远老师在博士阶段的研究，TF-IDF 具有较强的普适性，能够满足绝大部分的中文场景下的需求。不过也有其他研究工作表明使用信息增益的性能表现也很优秀。由于我们之后会使用深度学习模型在做端到端的学习，并且特征选择不是目前的瓶颈，因此这里不进行深入研究。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>通过这个项目，我感觉我确实很喜欢情感分析这个研究方向。我也因此承担了绝大部分的算法研究开发工作，查阅了很多文献和网络资料，由此对情感分析研究现状乃至自然语言处理的一些基础知识有了比较深入的了解，可谓受益匪浅。</p><p>比较可惜的是，我们在课程作业结题时实现的版本还不尽人意，而且因为事务忙碌和自身安排原因，深度学习模型计划了很久都没有动手实现。写这篇博文一是对现有工作做一个总结，同时有些算法写的比较详细，巩固一下所学知识；二就是督促自己尽早实现基于深度学习的模型，届时也会出该系列的第二篇博文。也欢迎看到这里的读者在 Github 和这里继续关注我们的工作。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><h3 id="引用文献"><a href="#引用文献" class="headerlink" title="引用文献"></a>引用文献</h3><ul><li>[1] <a href="https://arxiv.org/abs/1801.07883">Deep Learning for Sentiment Analysis : A Survey</a>. Lei Zhang, Shuai Wang, Bing Liu.</li><li>[2] 徐泓洋, 杨国为. <a href="http://www.wanfangdata.com.cn/details/detail.do?_type=perio&id=gykzjsj201711038">中文文本特征选择方法研究综述</a>[J]. 工业控制计算机, 2017, 30(11):80-81.</li></ul><h3 id="参考书籍"><a href="#参考书籍" class="headerlink" title="参考书籍"></a>参考书籍</h3><ul><li>《互联网大规模数据挖掘与分布式处理》 1.3.1</li></ul><h3 id="网络资料"><a href="#网络资料" class="headerlink" title="网络资料"></a>网络资料</h3><ul><li><a href="https://www.zhihu.com/question/20700012">中文情感分析 (Sentiment Analysis) 的难点在哪？现在做得比较好的有哪几家？ - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/34482959">如何用Python和机器学习训练中文文本情感分类模型？</a></li><li><a href="https://blog.csdn.net/simona081/article/details/80275506">处理文本分类中样本不均衡的问题 - CSDN博客</a></li></ul><script type="text/x-mathjax-config">MathJax.Hub.Config({  tex2jax: {inlineMath: [ ['$', '$'] ],        displayMath: [ ['$$', '$$']]}});</script><script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=default"></script>]]></content>
    
    
    <summary type="html">&lt;p&gt;“中文短文本情感分析 web 应用”是我带领的小组在《软件工程》这门课的大作业选题。按照这门课大作业的传统风格，就是做一个比较寻常的网站/APP，不过我这次提议另辟蹊径，一是我个人对情感分析这个方面比较感兴趣，想要借此机会进行了解和实践；二是组内成员都志在继续深造，做一个带有科研意味的项目或许能为简历添色。&lt;/p&gt;
&lt;p&gt;先简单介绍一下我们的项目。该 web 应用可分析用户输入的中文短文本中蕴含的情感，并输出数值化结果。前端使用 Vue.js 开发，后台使用 Flask 开发。算法部分目前包含自主实现的词袋模型和引入的 SnowNLP。项目地址如下（目前服务器关了，想要尝试效果可以将项目 clone 到本地，照着 README 运行）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://kyonhuang.top/sentiment-analysis-webapp/&quot;&gt;在线使用地址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/bighuang624/sentiment-analysis-webapp&quot;&gt;项目地址&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;截止这篇博文写作时，这个项目算是成型，也在 Github 上获得了 7 个 star。但是也遇到了一些问题，当然最大的问题就是分析效果不好。效果不好的原因包括很多，后文会一一介绍，当然其中一个是我们目前的算法部分还没有涉及到深度学习，不过我正在争取在近期用 LSTM 等实现效果较好的模型。&lt;/p&gt;
&lt;p&gt;这篇博文首先会简单介绍情感分析的概念、技术和难点。之后针对我们的项目，详细介绍我们使用的算法、迄今为止遇到的问题和一些改进措施。&lt;/p&gt;</summary>
    
    
    
    <category term="开源小项目" scheme="https://kyonhuang.top/blog/categories/%E5%BC%80%E6%BA%90%E5%B0%8F%E9%A1%B9%E7%9B%AE/"/>
    
    
    <category term="机器学习" scheme="https://kyonhuang.top/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="自然语言处理" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    <category term="文本分类" scheme="https://kyonhuang.top/blog/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    <category term="情感分析" scheme="https://kyonhuang.top/blog/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    
    <category term="情感计算" scheme="https://kyonhuang.top/blog/tags/%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>面向对象程序设计复习笔记</title>
    <link href="https://kyonhuang.top/blog/OOP-notes/"/>
    <id>https://kyonhuang.top/blog/OOP-notes/</id>
    <published>2018-06-18T00:24:35.000Z</published>
    <updated>2018-06-17T11:46:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>作为一个软件工程的大三学生，如果要求我立刻回答“面向对象程序设计是什么？”、“面向对象的特性是什么？”等问题，我想我是难以做到的。这是因为我们专业的“面向对象程序设计”这一门课程后有一个“(Java)”，在上这门课的时候我们可能更多地关注 Java 语法，而忽略了面向对象程序设计思想的一些精髓。实际上这些精髓映射出的是软件工程发展过程的智慧成果，只是当时我们什么也不懂。而面向对象程序设计的思想及设计模式的韵味则需要更大的代码量才能一探究竟。总之，这篇复习笔记会比较简洁地重温一下这些精髓的概念，真正去理解面向对象的思想，才会发现这些概念的设计精妙之处。</p><h3 id="概念重温"><a href="#概念重温" class="headerlink" title="概念重温"></a>概念重温</h3><ul><li><strong>对象</strong>：一个自包含的实体，用一组可识别的特性和行为来标识。</li><li><strong>类</strong>：具有相同的属性和功能的对象的抽象的集合。</li><li><strong>实例化</strong>：创建对象的过程。</li><li><strong>方法重载</strong>：提供创建同名的多个方法的能力，但这些方法需使用不同的参数类型。好处是在不改变原方法的基础上，新增功能。</li></ul><h3 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h3><p>面向对象的程序是由对象组成的，每个对象包含对用户公开的特定功能部分和隐藏的实现部分。在 OOP 中，<strong>不必关心对象的具体实现，只要能够满足用户的需求即可</strong>。</p><p>传统的结构化程序设计通过设计一系列的<strong>过程</strong>（即算法）来求解问题，首先确定如何操作数据，然后再决定如何组织数据。而 OOP 则把数据放在第一位，然后再考虑操作数据的算法。</p><h3 id="面向对象的好处"><a href="#面向对象的好处" class="headerlink" title="面向对象的好处"></a>面向对象的好处</h3><p><strong>封装、继承、多态</strong>，<strong>面向对象的三大特性</strong>降低了程序的耦合度，从而存在以下优点：</p><ul><li>可维护</li><li>可复用</li><li>可扩展</li><li>灵活性好</li></ul><span id="more"></span><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p><strong>封装（encapsulation）</strong>指每个对象都包含它能进行操作所需要的所有信息，因此对象不必依赖其他对象来完成自己的操作。</p><p>实现封装的关键在于绝对不能让类中的方法直接地访问其他类的实例域，程序仅通过对象的方法与对象数据进行交互。</p><p>优点：</p><ol><li>良好的封装能减少耦合；</li><li>类内部的实现可以自由修改；</li><li>类具有清晰的对外接口。</li></ol><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p><strong>继承（inheritance）</strong>定义了类如何相互关联，共享特性。继承的工作方式是，定义父类和子类，其中子类不但继承父类所有特性，还可以定义新的特性。</p><p>在 Java 中，继承通过关键字<code>extends</code>实现。有些语言（如 C++）允许一个类有多个父类（称为多继承），而 Java 不支持多继承，而是选择用接口提供多继承的好处，并避免多继承的复杂性和低效性。</p><p>如果子类继承于父类：</p><ol><li>子类拥有父类非<code>private</code>的属性和功能；</li><li>子类具有自己的属性和功能；</li><li>子类还可以以自己的方式实现父类的功能（<strong>方法重写</strong>）。</li></ol><p>优点：</p><ol><li>使得所有子类公共部分都放在父类，使得代码得到共享，避免重复；</li><li>继承可使得修改或扩展继承而来的实现都较为容易。</li></ol><p>缺点：</p><ol><li>继承是一种类与类之间<strong>强耦合</strong>的关系。父类变，子类不得不变；</li><li>继承会破坏保障，父类实现细节暴露给子类。</li></ol><p>如果想要阻止某个类或方法被继承，可以使用<code>final</code>修饰符。</p><h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p><strong>多态</strong>表示不同的对象可以执行相同的动作，但要通过它们自己的实现代码来执行。</p><ol><li>子类以父类的身份出现；</li><li>子类在工作时以自己的方式来实现；</li><li>子类以父类的身份出现时，子类特有的属性和方法不可以使用。</li></ol><p>多态的原理是当方法被调用时，无论对象是否被转换为其父类，都只有位于对象继承链最末端的方法实现会被调用。</p><p>实现多态的技术称为<strong>动态绑定（dynamic binding）</strong>，是指在执行期间判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。</p><ul><li>静态绑定发生在编译时期，动态绑定发生在运行时；</li><li>使用<code>private</code>或<code>static</code>或<code>final</code>修饰的变量或者方法，使用静态绑定。而虚方法（可以被子类重写的方法）则会根据运行时的对象进行动态绑定；</li><li>静态绑定使用类信息来完成，而动态绑定则需要使用对象信息来完成；</li><li>重载（Overload）的方法使用静态绑定完成，而重写（Override）的方法则使用动态绑定完成。</li></ul><h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><p>Java 中，使用<code>abstract</code>可以声明一个<strong>抽象方法</strong>。为了提高程序的清晰度，包含一个或多个抽象方法的类本身必须被声明为<strong>抽象类</strong>。除了抽象方法之外，抽象类还可以包含具体数据和具体方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(String n)</span> </span>&#123;</span><br><span class="line">        name = n;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> String <span class="title">getDescription</span><span class="params">()</span></span>;  <span class="comment">// 无需实现</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>抽象方法充当着占位的角色，具体实现在子类中。若子类没有全部实现抽象方法，则子类也必须被声明为抽象类。</p><p>抽象类不能被实例化，但是可以定义一个抽象类的<strong>对象变量</strong>并引用非抽象子类的对象：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Person p = <span class="keyword">new</span> Student(<span class="string">&quot;Kyon Huang&quot;</span>, <span class="string">&quot;Software Engineering&quot;</span>);</span><br></pre></td></tr></table></figure><h3 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h3><p><strong>接口（interface）</strong>是对类的一组需求描述，主要用来描述类具有什么功能，而并不给出每个功能的具体实现。一个类可以<strong>实现（implement）</strong>一个或多个接口，并在需要接口的地方，随时使用实现了相应接口的对象。</p><p>例如，<code>Arrays</code>类中的<code>sort</code>方法承诺可以对对象数组进行排序，但前提是对象所属的类必须实现了<code>Comparable</code>接口。<code>Comparable</code>接口的代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Comparable</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 接口中所有方法自动为 public，不必提供关键字</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(T other)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了让类实现一个接口，通常需要下面两个步骤：</p><ol><li>将类声明为实现给定的接口（使用关键字<code>implements</code>）；</li><li>对接口中的所有方法进行定义。</li></ol><p>在上例中，让<code>Employee</code>类实现<code>Comparable</code>接口，则有：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Employee</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Employee other)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Double.compare(salary, other.salary);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接口不是类，不能使用<code>new</code>运算符实例化一个接口。但是可以声明接口的变量，并引用实现了接口的类对象。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>《Java 核心技术卷 I》</li><li>《大话设计模式》</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;作为一个软件工程的大三学生，如果要求我立刻回答“面向对象程序设计是什么？”、“面向对象的特性是什么？”等问题，我想我是难以做到的。这是因为我们专业的“面向对象程序设计”这一门课程后有一个“(Java)”，在上这门课的时候我们可能更多地关注 Java 语法，而忽略了面向对象程序设计思想的一些精髓。实际上这些精髓映射出的是软件工程发展过程的智慧成果，只是当时我们什么也不懂。而面向对象程序设计的思想及设计模式的韵味则需要更大的代码量才能一探究竟。总之，这篇复习笔记会比较简洁地重温一下这些精髓的概念，真正去理解面向对象的思想，才会发现这些概念的设计精妙之处。&lt;/p&gt;
&lt;h3 id=&quot;概念重温&quot;&gt;&lt;a href=&quot;#概念重温&quot; class=&quot;headerlink&quot; title=&quot;概念重温&quot;&gt;&lt;/a&gt;概念重温&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对象&lt;/strong&gt;：一个自包含的实体，用一组可识别的特性和行为来标识。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类&lt;/strong&gt;：具有相同的属性和功能的对象的抽象的集合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实例化&lt;/strong&gt;：创建对象的过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;方法重载&lt;/strong&gt;：提供创建同名的多个方法的能力，但这些方法需使用不同的参数类型。好处是在不改变原方法的基础上，新增功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;面向对象&quot;&gt;&lt;a href=&quot;#面向对象&quot; class=&quot;headerlink&quot; title=&quot;面向对象&quot;&gt;&lt;/a&gt;面向对象&lt;/h3&gt;&lt;p&gt;面向对象的程序是由对象组成的，每个对象包含对用户公开的特定功能部分和隐藏的实现部分。在 OOP 中，&lt;strong&gt;不必关心对象的具体实现，只要能够满足用户的需求即可&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;传统的结构化程序设计通过设计一系列的&lt;strong&gt;过程&lt;/strong&gt;（即算法）来求解问题，首先确定如何操作数据，然后再决定如何组织数据。而 OOP 则把数据放在第一位，然后再考虑操作数据的算法。&lt;/p&gt;
&lt;h3 id=&quot;面向对象的好处&quot;&gt;&lt;a href=&quot;#面向对象的好处&quot; class=&quot;headerlink&quot; title=&quot;面向对象的好处&quot;&gt;&lt;/a&gt;面向对象的好处&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;封装、继承、多态&lt;/strong&gt;，&lt;strong&gt;面向对象的三大特性&lt;/strong&gt;降低了程序的耦合度，从而存在以下优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可维护&lt;/li&gt;
&lt;li&gt;可复用&lt;/li&gt;
&lt;li&gt;可扩展&lt;/li&gt;
&lt;li&gt;灵活性好&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="学科补完计划" scheme="https://kyonhuang.top/blog/categories/%E5%AD%A6%E7%A7%91%E8%A1%A5%E5%AE%8C%E8%AE%A1%E5%88%92/"/>
    
    
    <category term="面向对象程序设计" scheme="https://kyonhuang.top/blog/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/"/>
    
    <category term="软件工程" scheme="https://kyonhuang.top/blog/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
    <category term="Java" scheme="https://kyonhuang.top/blog/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>【paper reading】基于 Web 的问答系统综述</title>
    <link href="https://kyonhuang.top/blog/survey-on-web-based-question-answering-notes/"/>
    <id>https://kyonhuang.top/blog/survey-on-web-based-question-answering-notes/</id>
    <published>2018-06-17T09:10:59.000Z</published>
    <updated>2019-09-06T08:34:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>原文链接：<a href="http://www.jsjkx.com/jsjkx/ch/reader/create_pdf.aspx?file_no=20170601&flag=&journal_id=jsjkx&year_id=2017">基于 Web 的问答系统综述</a></p><p>作者：李舟军，李水华</p><p>简介：详细介绍了基于 Web 的问答系统的研究背景、架构及其问题分析、信息检索、答案抽取这三大关键技术的研究进展，并分析了基于 Web 的问答系统所面临的问题。</p><p>选读原因：选了一篇中文的基于 Web 的问答系统综述，和手头上工作比较贴近，来对接下来的研究方向有个大致的了解和思考，因此记录也比较详细。</p><span id="more"></span><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p><strong>问答系统（Question Answering, QA）</strong>以自然语言为输入与输出，理解用户的查询意图后，通过一系列的检索、分析与处理，返回精确、简练的答案。</p><p>根据问答系统知识来源的不同，该文将问答系统分为 3 类：</p><ol><li><strong>基于知识库的问答系统（Qustion Answering over Knowledge Bases, KBQA）</strong>：主要以知识库作为问答系统的知识来源；</li><li><strong>基于社区的问答系统（Community-based Question Answering, CQA）</strong>：主要以问答社区（如知乎、百度知道等）作为问答系统的知识来源；</li><li><strong>基于 Web 的问答系统（Web-based Question Answering, WQA）</strong>：以开放的互联网上的 Web 文档作为问答系统的知识来源，从搜索引擎上返回的相关网页片段中抽取出用户所提问题的答案。</li></ol><p>其中，WQA 系统同时具有搜索引擎和问答系统的优点，与时俱进，不断更新。</p><h3 id="WQA-系统"><a href="#WQA-系统" class="headerlink" title="WQA 系统"></a>WQA 系统</h3><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/Architecture-of-question-answering-system.png" alt="Architecture-of-question-answering-system"></p><p>经典的 WQA 系统通常由以下 3 个模块构成（其他 QA 系统各模块大体一致，功能有所不同。上图来源[1]）：</p><ol><li><strong>问题分析模块</strong>：根据用户的查询意图生成相应的查询语句，可能包含对问题的分类、提取问题的关键词或者生成一些其他描述用户查询意图的中间数据；</li><li><strong>信息检索模块</strong>：将问题分析模块得到的查询语句或关键词提交给搜索引擎，并整理返回的搜索结果来得到可能包含正确答案的网页片段（性能瓶颈）；</li><li><strong>答案抽取模块</strong>：利用信息抽取技术，从网页片段中抽取答案，可能需要用到问题分析模块得到的问题类别、关键词等数据。</li></ol><h4 id="问题分析模块"><a href="#问题分析模块" class="headerlink" title="问题分析模块"></a>问题分析模块</h4><p>流程：</p><ol><li>分词</li><li>问题分类：问题的类别是反应用户提问意图的重要信息</li><li>问题重写：便于搜索引擎理解问题语义</li></ol><h5 id="问题分类"><a href="#问题分类" class="headerlink" title="问题分类"></a>问题分类</h5><p>问题分类的作用体现在：</p><ul><li><strong>能够有效减少候选答案空间，提高系统返回答案的准确性</strong>。例如，一个问句被分类为时间类，则在答案抽取阶段，系统把不含时间的候选句子过滤掉；</li><li><strong>决定答案喧杂策略，根据不同的问句类型调节对不同问题的答案选择策略</strong>。例如，对于“安徽省的简称是什么”，分析出其询问地点类别，抽取文档库中地点类的文档作为候选答案。</li></ul><p><img src="https://raw.githubusercontent.com/bighuang624/pic-repo/master/question-classifier.png" alt="问题分类体系"></p><p>表一为国际权威的 UIUC 问句分类体系，针对英文分类。表二是哈工大定义的中文问句分类体系。</p><p>很多 WQA 系统采用规则分类器对问题进行分类。最简单的规则可以是通过问题是否包含某个词来分类，而比较复杂的规则包括基于语义模式匹配的问题分类[2]。如果没有一条规则能够与问题匹配成功，则借助一些统计机器学习的方法分类，例如 SVM、RNN 等。</p><h5 id="关键词提取和扩展"><a href="#关键词提取和扩展" class="headerlink" title="关键词提取和扩展"></a>关键词提取和扩展</h5><p>关键词既可作为搜索引擎的输入，也可辅助答案的抽取过程。WQA 系统通常在分词、取出停用词之后进行关键词的提取，并将名词、动词、形容词等作为关键词。因此可以用一些简单规则提取，例如“所有带形容词的名词都是关键词”等。也可以分析问题的语法结构，抽取主语、宾语作为关键词[3]。</p><p>关键词的扩展主要用于解决关键词的同义词的匹配问题，通常需要一些同义词词库进行辅助。</p><h5 id="问题重写"><a href="#问题重写" class="headerlink" title="问题重写"></a>问题重写</h5><p>当问题本身（而非关键词）作为搜索引擎的输入时，可能较难领会语义，此时需要对问题进行重写。一些启发式的方法依靠简单的字符串操作（替换、拼接、删除）来实现问题重写，也有较为复杂的优化方法，例如定义一个针对问题的操作集合，然后利用概率模型选择最合适的操作来重写问题[4]。</p><h4 id="信息检索模块"><a href="#信息检索模块" class="headerlink" title="信息检索模块"></a>信息检索模块</h4><p>信息检索模块的实现可以调用搜索引擎提供的接口，也可以利用爬虫技术。搜索引擎所具有的高质量摘要技术能过滤原始网页的噪音数据，因此无需抓取解析原始网页。</p><h4 id="答案抽取模块"><a href="#答案抽取模块" class="headerlink" title="答案抽取模块"></a>答案抽取模块</h4><p>答案抽取模块是 WQA 系统中的重点和难点，通常包括两个步骤：</p><ol><li>候选答案抽取：从网页片段中抽取出候选答案；</li><li>候选答案排序：对候选答案进行排序，得到最佳答案。</li></ol><h5 id="候选答案抽取"><a href="#候选答案抽取" class="headerlink" title="候选答案抽取"></a>候选答案抽取</h5><p>抽取候选答案的几种典型方法：</p><ol><li>手工编辑或自动生成名词词典，将词典中的所有名词都作为候选答案。这种做法的候选答案集非常大，因此候选答案排序以及维护难度很大，难以更新以应对新的领域和新的概念；</li><li>利用<strong>命名实体识别（Named Entity Recognition, NER）</strong>工具，抽取命名实体作为候选答案。具体效果受问题分类算法和命名实体识别算法效果的影响；</li><li>根据手工编辑或自动生成的文本模式抽取候选答案。准确率较高，但匹配较为死板，无法适应新的数据。</li></ol><h5 id="候选答案排序"><a href="#候选答案排序" class="headerlink" title="候选答案排序"></a>候选答案排序</h5><p>候选答案排序及最佳答案选择的几种典型方法：</p><ol><li>采用<strong>向量空间模型（Vector Space Model, VSM）</strong>计算候选答案与问题的相似度，并以此进行排序[5]；</li><li>根据语法结构判断候选答案与问题的匹配度，并以此进行排序；</li><li>根据词汇特征、相似度特征、统计特征等多种特征进行综合排序。</li></ol><h3 id="WQA-面临的主要问题"><a href="#WQA-面临的主要问题" class="headerlink" title="WQA 面临的主要问题"></a>WQA 面临的主要问题</h3><ol><li>问题分类有待改善：问题分类的本质是短文本分类，受限于特征稀缺，分类器效果有待提升；</li><li>同义句子的理解需要解决：同义词的使用和句法结构的变化使得 WQA 系统难以准确抽取答案；</li><li>高质量的 QA 对难以获取：缺少相关数据；</li><li>利用跨语言语料能力较差：网页片段可能存在多种语言，WQA 系统难以利用多种语言的文本数据来回答某一种特定语言的问题；</li><li>通用型不足：回答通用领域问题的能力尚有待进一步增强；</li><li>处理复杂问题的能力不足：对于定义型、原因型、关系型、比较型、方法型等问题难以给出满意的回答。</li></ol><h3 id="WQA-的发展趋势"><a href="#WQA-的发展趋势" class="headerlink" title="WQA 的发展趋势"></a>WQA 的发展趋势</h3><ol><li>与其他问答系统的融合；</li><li>通过答案摘要生成答案；</li><li>自动生成高质量问答对数据；</li><li>提升 WQA 系统处理复杂问题的能力；</li><li>跨语言能力、跨领域能力的进一步增强；</li><li>与语音识别、语音生成等工具的进一步结合；</li><li>辅助机器人。</li></ol><h3 id="文献"><a href="#文献" class="headerlink" title="文献"></a>文献</h3><p>[1] 镇丽华, 王小林, 杨思春. 自动问答系统中问句分类研究综述[J]. 安徽工业大学学报(自科版), 2015, 32(1):48-54.</p><p>[2] LI X, HU D, LI H, et al. Automatic question answering from Web documents[J]. Wuhan University Journal of Natural Sciences, 2007, 12(5):875 880.</p><p>[3] LIU Z J, WANG X L, CHEN Q C, et al. A Chinese question answering system based on Web search [C]. International Conference on Machine Learning and Cybernetics, Lanzhou: IEEE, 2014:816-820.</p><p>[4] CHALI Y, HASAN S A, MOJAHID M. A reinforcement learning formulation to the complex question answering problem<br>[J]. Information Processing &amp; Management, 2015, 51(3):252 272.</p><p>[5] 余正涛，樊孝忠，郭剑毅，等. 基于潜在语义分析的汉语问答系统答案提取[J]. 计算机学报，2006，29(10):1889—1893.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;原文链接：&lt;a href=&quot;http://www.jsjkx.com/jsjkx/ch/reader/create_pdf.aspx?file_no=20170601&amp;flag=&amp;journal_id=jsjkx&amp;year_id=2017&quot;&gt;基于 Web 的问答系统综述&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：李舟军，李水华&lt;/p&gt;
&lt;p&gt;简介：详细介绍了基于 Web 的问答系统的研究背景、架构及其问题分析、信息检索、答案抽取这三大关键技术的研究进展，并分析了基于 Web 的问答系统所面临的问题。&lt;/p&gt;
&lt;p&gt;选读原因：选了一篇中文的基于 Web 的问答系统综述，和手头上工作比较贴近，来对接下来的研究方向有个大致的了解和思考，因此记录也比较详细。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读笔记" scheme="https://kyonhuang.top/blog/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="自然语言处理" scheme="https://kyonhuang.top/blog/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    <category term="问答系统" scheme="https://kyonhuang.top/blog/tags/%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="信息检索" scheme="https://kyonhuang.top/blog/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
    <category term="答案抽取" scheme="https://kyonhuang.top/blog/tags/%E7%AD%94%E6%A1%88%E6%8A%BD%E5%8F%96/"/>
    
    <category term="问题分析" scheme="https://kyonhuang.top/blog/tags/%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/"/>
    
    <category term="综述" scheme="https://kyonhuang.top/blog/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
</feed>
